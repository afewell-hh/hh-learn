id,name,path,difficulty,meta_description,full_content,display_order,estimated_minutes,social_image_url,prerequisites_json,tags
197361539224,Accessing the Hedgehog Virtual Lab with Amazon Web Services,accessing-the-hedgehog-virtual-lab-with-amazon-web-services,[object Object],Create and configure an AWS EC2 metal instance suitable for running the Hedgehog Virtual Lab so you can begin hands-on learning with Hedgehog Fabric.,"<p>Welcome! This guide will help you create an Amazon Web Services EC2 metal instance that&#39;s ready to run the Hedgehog Virtual Lab. We&#39;ll walk through each step to get you up and running quickly, and we&#39;ll make sure you understand what&#39;s happening along the way.</p>
<h2>What You&#39;ll Learn</h2>
<ul>
<li>Understand why AWS requires metal instances for VLAB</li>
<li>Create a properly configured EC2 metal instance</li>
<li>Connect to your instance and prepare for VLAB installation</li>
</ul>
<h2>Prerequisites</h2>
<ul>
<li>An AWS account with an active subscription</li>
<li>AWS CLI v2 installed on your local machine</li>
<li>Basic familiarity with command-line tools</li>
<li>IAM permissions to create EC2 instances, key pairs, and security groups</li>
</ul>
<p><strong>Need to install AWS CLI?</strong> Visit the <a href=""https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"">AWS CLI installation guide</a>.</p>
<h2>Understanding the Requirements</h2>
<p>The Hedgehog Virtual Lab creates a complete virtual network fabric using multiple virtual machines to simulate switches, servers, and a control node. While Hedgehog&#39;s control software is lightweight, the lab uses Broadcom SONiC Virtual Switch VMs, which require substantial resources.</p>
<h3>Why Metal Instances?</h3>
<p><strong>AWS does not support nested virtualization on standard EC2 instances.</strong> To run VLAB, which needs to create VMs inside VMs, you must use a <strong>bare metal instance</strong>. These instances give you direct access to the physical server&#39;s processor and memory, allowing you to run KVM/QEMU for nested virtualization.</p>
<h3>Recommended Instance Types</h3>
<p>AWS metal instances are generally larger than what you&#39;d find with nested virtualization support on other clouds. We recommend any x86/x64 metal instance with <strong>at least 32 vCPUs and 128GB RAM</strong>. Good options include:</p>
<ul>
<li><strong>c5n.metal</strong>: 72 vCPUs, 192 GB RAM (~$3.89/hour) - Good balance of compute and cost</li>
<li><strong>c6i.metal</strong>: 128 vCPUs, 256 GB RAM (~$5.44/hour) - More powerful, higher cost</li>
<li><strong>m5.metal</strong>: 96 vCPUs, 384 GB RAM - Memory-optimized alternative</li>
<li><strong>Any newer metal instance</strong> that meets the minimum specs</li>
</ul>
<p><strong>Important:</strong> Make sure to select an <strong>x86/x64 architecture</strong> instance, not ARM-based (Graviton) instances, as the VLAB software is designed for x86/x64 processors.</p>
<p><strong>Note:</strong> These specs exceed the official <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB documentation requirements</a>, but AWS doesn&#39;t offer smaller metal instances. The extra resources can be useful for larger topologies or running multiple experiments.</p>
<p><strong>Cost Warning:</strong> Metal instances are significantly more expensive than standard VMs. Be sure to stop or terminate your instance when not in use to avoid unexpected charges!</p>
<h2>Step 1: Set Up Your AWS Environment</h2>
<p>First, let&#39;s make sure you&#39;re authenticated and ready to create resources.</p>
<h3>Verify AWS CLI Installation</h3>
<pre><code class=""language-bash"">aws --version
</code></pre>
<p>You should see output showing AWS CLI version 2.x. If not, please install the CLI first.</p>
<h3>Configure AWS Credentials</h3>
<pre><code class=""language-bash"">aws configure
</code></pre>
<p>You&#39;ll be prompted for:</p>
<ul>
<li>AWS Access Key ID</li>
<li>AWS Secret Access Key</li>
<li>Default region (e.g., <code>us-west-2</code>)</li>
<li>Output format (use <code>json</code> or <code>table</code>)</li>
</ul>
<h3>Verify Your Identity</h3>
<pre><code class=""language-bash"">aws sts get-caller-identity
</code></pre>
<p>You should see your account ID, user ID, and ARN.</p>
<h2>Step 2: Create an SSH Key Pair</h2>
<p>You&#39;ll need an SSH key to connect to your instance.</p>
<h3>Create and Save the Key</h3>
<pre><code class=""language-bash"">aws ec2 create-key-pair \
  --key-name hedgehog-vlab-key \
  --query &#39;KeyMaterial&#39; \
  --output text &gt; ~/.ssh/hedgehog-vlab-key.pem

chmod 400 ~/.ssh/hedgehog-vlab-key.pem
</code></pre>
<p>This creates a new key pair in AWS and saves the private key locally. The <code>chmod 400</code> makes it readable only by you (required by SSH).</p>
<h2>Step 3: Launch Your Metal Instance</h2>
<p>Now we&#39;ll launch an EC2 metal instance configured for running the Hedgehog Virtual Lab.</p>
<h3>The Command</h3>
<pre><code class=""language-bash"">aws ec2 run-instances \
  --image-id resolve:ssm:/aws/service/canonical/ubuntu/server/24.04/stable/current/amd64/hvm/ebs-gp3/ami-id \
  --instance-type c5n.metal \
  --key-name hedgehog-vlab-key \
  --block-device-mappings &#39;DeviceName=/dev/sda1,Ebs={VolumeSize=200,VolumeType=gp3}&#39; \
  --tag-specifications &#39;ResourceType=instance,Tags=[{Key=Name,Value=hedgehog-vlab}]&#39;
</code></pre>
<h3>What This Command Does</h3>
<p>Let&#39;s break down the key parameters:</p>
<ul>
<li><strong><code>--image-id resolve:ssm:...</code></strong>: Automatically resolves to the latest Ubuntu 24.04 LTS AMI for your region (no need to look up AMI IDs!)</li>
<li><strong><code>--instance-type c5n.metal</code></strong>: Launches a bare metal instance with 72 vCPUs and 192GB RAM</li>
<li><strong><code>--key-name hedgehog-vlab-key</code></strong>: Uses the SSH key we just created</li>
<li><strong><code>--block-device-mappings</code></strong>: Creates a 200GB GP3 EBS volume (VLAB needs substantial disk space)</li>
<li><strong><code>--tag-specifications</code></strong>: Tags the instance with a friendly name for easy identification</li>
</ul>
<h3>Customize for Your Environment</h3>
<p>Before running the command, you may want to:</p>
<ul>
<li>Change <code>c5n.metal</code> to a different metal instance type if needed</li>
<li>Adjust the region (set with <code>aws configure</code> or add <code>--region us-west-2</code>)</li>
<li>Change <code>hedgehog-vlab</code> to a different name</li>
</ul>
<h3>Add a Security Group (Optional but Recommended)</h3>
<p>The command above uses your default VPC and security group. For better security, create a security group that allows SSH only from your IP:</p>
<pre><code class=""language-bash""># Get your public IP
MY_IP=$(curl -s https://checkip.amazonaws.com)

# Create a security group
aws ec2 create-security-group \
  --group-name hedgehog-vlab-sg \
  --description &quot;Security group for Hedgehog VLAB&quot;

# Allow SSH from your IP only
aws ec2 authorize-security-group-ingress \
  --group-name hedgehog-vlab-sg \
  --protocol tcp \
  --port 22 \
  --cidr ${MY_IP}/32
</code></pre>
<p>Then add <code>--security-groups hedgehog-vlab-sg</code> to the run-instances command.</p>
<h3>Run the Command</h3>
<p>Copy the command with your customizations and run it. The instance will take several minutes to launch. You&#39;ll see JSON output with instance details.</p>
<p><strong>Save the Instance ID</strong> from the output—you&#39;ll need it for management commands.</p>
<h3>Wait for the Instance to Be Ready</h3>
<p>Check the instance status:</p>
<pre><code class=""language-bash"">aws ec2 describe-instances \
  --filters &quot;Name=tag:Name,Values=hedgehog-vlab&quot; \
  --query &#39;Reservations[0].Instances[0].State.Name&#39; \
  --output text
</code></pre>
<p>Wait until it shows <code>running</code>.</p>
<h2>Step 4: Connect to Your Instance</h2>
<p>Once your instance is running, you can connect via SSH.</p>
<h3>Get the Public IP Address</h3>
<pre><code class=""language-bash"">aws ec2 describe-instances \
  --filters &quot;Name=tag:Name,Values=hedgehog-vlab&quot; \
  --query &#39;Reservations[0].Instances[0].PublicIpAddress&#39; \
  --output text
</code></pre>
<h3>Connect via SSH</h3>
<pre><code class=""language-bash"">ssh -i ~/.ssh/hedgehog-vlab-key.pem ubuntu@YOUR_INSTANCE_PUBLIC_IP
</code></pre>
<p>On first connection, you&#39;ll be asked to verify the host key fingerprint—type &quot;yes&quot; to continue.</p>
<p>You should see an Ubuntu welcome message and a command prompt.</p>
<h2>Next Steps: Installing the VLAB</h2>
<p>Congratulations! You now have a metal instance ready for the Hedgehog Virtual Lab. The installation process from here is identical regardless of whether you&#39;re running on AWS, Google Cloud, Azure, or bare metal.</p>
<h3>What Comes Next</h3>
<p>You&#39;ll need to:</p>
<ol>
<li>Install Docker, QEMU/KVM, and other prerequisites</li>
<li>Install the <code>hhfab</code> utility</li>
<li>Initialize and run the VLAB</li>
<li>Access the control node and switches</li>
</ol>
<p>All of these steps are covered in detail in the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB Overview documentation</a> starting at the <strong>&quot;Installing Prerequisites&quot;</strong> section.</p>
<p><strong>Look for our companion module:</strong> We&#39;re creating a separate &quot;Setting up the VLAB&quot; module that will walk through the installation steps in the same friendly format as this guide—stay tuned!</p>
<h2>Troubleshooting</h2>
<h3>&quot;aws: command not found&quot;</h3>
<p>Install the AWS CLI following the <a href=""https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"">official installation guide</a>.</p>
<h3>Authentication Issues</h3>
<ul>
<li>Make sure you&#39;ve run <code>aws configure</code> with valid credentials</li>
<li>Verify your credentials with <code>aws sts get-caller-identity</code></li>
<li>Check that your IAM user/role has EC2 permissions</li>
</ul>
<h3>Instance Launch Fails</h3>
<ul>
<li><strong>Quota/Limit issues</strong>: Metal instances often have lower service quotas. Request a quota increase in the AWS Service Quotas console for &quot;Running On-Demand All Standard instances&quot;</li>
<li><strong>Region availability</strong>: Not all regions have all metal instance types. Try a different region or instance type.</li>
<li><strong>AMI not found</strong>: The SSM parameter resolver requires AWS CLI v2. Upgrade if you&#39;re using v1.</li>
</ul>
<h3>Can&#39;t SSH to the Instance</h3>
<ul>
<li>Wait 2-3 minutes after the instance shows &quot;running&quot; for it to fully boot</li>
<li>Check that your security group allows SSH (port 22) from your IP</li>
<li>Verify the key file has correct permissions: <code>ls -l ~/.ssh/hedgehog-vlab-key.pem</code> should show <code>-r--------</code></li>
<li>Make sure you&#39;re using the correct username (<code>ubuntu</code> for Ubuntu AMIs)</li>
</ul>
<h3>Instance Launches But Immediately Stops</h3>
<ul>
<li>Check CloudWatch logs or instance status checks</li>
<li>Metal instances take longer to initialize than standard instances—wait a few minutes</li>
</ul>
<h2>Cost Considerations</h2>
<p><strong>Metal instances are expensive!</strong> The c5n.metal costs approximately $3.89/hour (~$2,800/month) if left running continuously. Here are some tips to manage costs:</p>
<ul>
<li><strong>Stop when not in use</strong>: <code>aws ec2 stop-instances --instance-ids YOUR_INSTANCE_ID</code></li>
<li><strong>Start when needed</strong>: <code>aws ec2 start-instances --instance-ids YOUR_INSTANCE_ID</code></li>
<li><strong>Terminate when completely done</strong>: <code>aws ec2 terminate-instances --instance-ids YOUR_INSTANCE_ID</code></li>
</ul>
<p><strong>Important:</strong> Stopped instances still incur EBS storage charges (about $20/month for 200GB). To completely stop charges, terminate the instance—but note that this deletes the instance and its data!</p>
<p><strong>Cost-Saving Tips:</strong></p>
<ul>
<li>Use the instance during business hours only</li>
<li>Consider Spot Instances for even lower costs (with interruption risk)</li>
<li>Use Reserved Instances if you plan to use VLAB long-term</li>
<li>Set up billing alerts to avoid surprises</li>
</ul>
<h2>Resources</h2>
<ul>
<li><a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">Hedgehog VLAB Documentation</a></li>
<li><a href=""https://docs.aws.amazon.com/cli/"">AWS CLI Documentation</a></li>
<li><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances"">EC2 Metal Instances Overview</a></li>
<li><a href=""https://calculator.aws/"">EC2 Pricing Calculator</a></li>
</ul>
<hr>
<p><strong>Ready to continue?</strong> Once you&#39;ve completed the VLAB installation steps from the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">official documentation</a>, you&#39;ll have a complete Hedgehog Fabric environment to explore and learn!</p>
",11,15,,"[""AWS account with an active subscription"",""AWS CLI v2 installed on your local machine"",""Basic familiarity with command-line tools"",""IAM permissions to create EC2 instances, key pairs, and security groups"",""Sufficient quota for metal instances in your region""]","hedgehog,virtual-lab,aws,onboarding,access"
197355232900,Accessing the Hedgehog Virtual Lab with Google Cloud,accessing-the-hedgehog-virtual-lab-with-google-cloud,[object Object],Create and configure a Google Cloud VM suitable for running the Hedgehog Virtual Lab so you can begin hands-on learning with Hedgehog Fabric.,"<p>Welcome! This guide will help you create a Google Cloud virtual machine that&#39;s ready to run the Hedgehog Virtual Lab. We&#39;ll walk through each step to get you up and running quickly, and we&#39;ll make sure you understand what&#39;s happening along the way.</p>
<h2>What You&#39;ll Learn</h2>
<ul>
<li>Understand the VM requirements for running Hedgehog VLAB</li>
<li>Create a properly configured Google Cloud VM with nested virtualization</li>
<li>Connect to your VM and prepare for VLAB installation</li>
</ul>
<h2>Prerequisites</h2>
<ul>
<li>A Google Cloud account with an active project</li>
<li>Google Cloud SDK (gcloud) installed on your local machine</li>
<li>Basic familiarity with command-line tools</li>
</ul>
<p><strong>Need to install gcloud?</strong> Visit the <a href=""https://cloud.google.com/sdk/docs/install"">Google Cloud SDK installation guide</a>.</p>
<h2>Understanding the Requirements</h2>
<p>The Hedgehog Virtual Lab creates a complete virtual network fabric using multiple virtual machines to simulate switches, servers, and a control node. While Hedgehog&#39;s control software is lightweight, the lab uses Broadcom SONiC Virtual Switch VMs, which require substantial resources.</p>
<h3>Recommended VM Size</h3>
<p>We recommend the <strong>n2-standard-32</strong> machine type for the default VLAB topology. This provides:</p>
<ul>
<li>32 vCPUs (16 physical cores with 2 threads each)</li>
<li>128 GB RAM</li>
<li>Nested virtualization support</li>
</ul>
<p><strong>Note:</strong> This is slightly below the official <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB documentation requirements</a> (which are based on conservative SONiC virtual switch recommendations), but works well in practice for learning and testing. If you plan to run larger topologies or need guaranteed performance, consider a larger instance.</p>
<p><strong>Important:</strong> We make no performance guarantees for the VLAB environment—different use cases may require different VM sizes. The VLAB is designed for learning and testing, not production workloads.</p>
<h2>Step 1: Set Up Your Google Cloud Environment</h2>
<p>First, let&#39;s make sure you&#39;re authenticated and ready to create resources.</p>
<h3>Verify gcloud Installation</h3>
<pre><code class=""language-bash"">gcloud --version
</code></pre>
<p>You should see output showing the Google Cloud SDK version. If not, please install the SDK first.</p>
<h3>Authenticate to Google Cloud</h3>
<pre><code class=""language-bash"">gcloud auth login
</code></pre>
<p>This will open your browser for authentication. Follow the prompts to sign in.</p>
<h3>Set Your Project and Preferred Zone</h3>
<p>Replace <code>YOUR_PROJECT_ID</code> with your actual GCP project ID, and choose a zone close to you:</p>
<pre><code class=""language-bash"">gcloud config set project YOUR_PROJECT_ID
gcloud config set compute/zone us-west1-c
</code></pre>
<p><strong>Tip:</strong> You can list available zones with <code>gcloud compute zones list</code>.</p>
<h2>Step 2: Create Your VLAB Virtual Machine</h2>
<p>Now we&#39;ll create a VM configured specifically for running the Hedgehog Virtual Lab. The command below looks long, but we&#39;ll break down what it does.</p>
<h3>The Command</h3>
<pre><code class=""language-bash"">gcloud compute instances create hedgehog-vlab \
  --project=YOUR_PROJECT_ID \
  --zone=us-west1-c \
  --machine-type=n2-standard-32 \
  --enable-nested-virtualization \
  --network-interface=network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=default \
  --maintenance-policy=MIGRATE \
  --provisioning-model=STANDARD \
  --create-disk=auto-delete=yes,boot=yes,image=projects/ubuntu-os-cloud/global/images/ubuntu-minimal-2404-noble-amd64-v20250930,mode=rw,size=200,type=pd-balanced \
  --threads-per-core=2 \
  --visible-core-count=16
</code></pre>
<h3>What This Command Does</h3>
<p>Let&#39;s break down the key flags:</p>
<ul>
<li><strong><code>--machine-type=n2-standard-32</code></strong>: Creates a VM with 32 vCPUs and 128GB RAM</li>
<li><strong><code>--enable-nested-virtualization</code></strong>: Critical! Allows the VM to run other VMs inside it (required for VLAB). This flag can <strong>only</strong> be set from the command line.</li>
<li><strong><code>--image=...ubuntu-minimal-2404...</code></strong>: Uses Ubuntu 24.04 LTS, which is tested and recommended for VLAB</li>
<li><strong><code>--create-disk=...size=200...</code></strong>: Creates a 200GB disk (VLAB needs substantial disk space for all the virtual switches and servers)</li>
<li><strong><code>--threads-per-core=2 --visible-core-count=16</code></strong>: Configures 16 physical cores with hyperthreading enabled</li>
</ul>
<p><strong>Prefer using the GUI?</strong> You can configure most of these settings in the Google Cloud Console, then click &quot;EQUIVALENT COMMAND LINE&quot; at the bottom of the VM creation form to get the gcloud command. Just make sure to add the <code>--enable-nested-virtualization</code> flag—it&#39;s only available via the command line!</p>
<h3>Customize for Your Environment</h3>
<p>Before running the command, replace:</p>
<ul>
<li><code>YOUR_PROJECT_ID</code> with your GCP project ID</li>
<li><code>us-west1-c</code> with your preferred zone (if different)</li>
<li><code>hedgehog-vlab</code> with a different name if you prefer</li>
</ul>
<h3>Run the Command</h3>
<p>Copy the command with your values and run it. It will take a couple of minutes to create and start your VM.</p>
<p>You should see output indicating the VM was created successfully and is now running.</p>
<h2>Step 3: Connect to Your VM</h2>
<p>Once your VM is running, connect via SSH:</p>
<pre><code class=""language-bash"">gcloud compute ssh hedgehog-vlab --zone=us-west1-c
</code></pre>
<p>This will open an SSH connection to your new VM. You should see an Ubuntu welcome message and a command prompt.</p>
<h2>Next Steps: Installing the VLAB</h2>
<p>Congratulations! You now have a VM ready for the Hedgehog Virtual Lab. The installation process from here is identical regardless of whether you&#39;re running on Google Cloud, AWS, Azure, or bare metal.</p>
<h3>What Comes Next</h3>
<p>You&#39;ll need to:</p>
<ol>
<li>Install Docker, QEMU/KVM, and other prerequisites</li>
<li>Install the <code>hhfab</code> utility</li>
<li>Initialize and run the VLAB</li>
<li>Access the control node and switches</li>
</ol>
<p>All of these steps are covered in detail in the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB Overview documentation</a> starting at the <strong>&quot;Installing Prerequisites&quot;</strong> section.</p>
<p><strong>Look for our companion module:</strong> We&#39;re creating a separate &quot;Setting up the VLAB&quot; module that will walk through the installation steps in the same friendly format as this guide—stay tuned!</p>
<h2>Troubleshooting</h2>
<h3>&quot;gcloud: command not found&quot;</h3>
<p>Install the Google Cloud SDK following the <a href=""https://cloud.google.com/sdk/docs/install"">official installation guide</a>.</p>
<h3>Authentication Issues</h3>
<p>Make sure you&#39;re logged in with <code>gcloud auth login</code> and your account has the necessary permissions to create compute instances in your project.</p>
<h3>VM Creation Fails</h3>
<ul>
<li><strong>Quota issues</strong>: Your project may not have enough quota for a 32-vCPU instance. Check your quotas in the GCP Console under IAM &amp; Admin → Quotas.</li>
<li><strong>Zone availability</strong>: Try a different zone if the machine type isn&#39;t available in your selected zone.</li>
</ul>
<h3>Can&#39;t SSH to the VM</h3>
<ul>
<li>Wait a minute or two after creation for SSH keys to propagate</li>
<li>Check that your firewall rules allow SSH (port 22)</li>
<li>Try <code>gcloud compute ssh hedgehog-vlab --zone=YOUR_ZONE --tunnel-through-iap</code> if direct SSH isn&#39;t working</li>
</ul>
<h2>Cost Considerations</h2>
<p>The n2-standard-32 instance is a substantial VM and will incur meaningful costs if left running. Here are some tips:</p>
<ul>
<li><strong>Stop when not in use</strong>: <code>gcloud compute instances stop hedgehog-vlab --zone=YOUR_ZONE</code></li>
<li><strong>Start when needed</strong>: <code>gcloud compute instances start hedgehog-vlab --zone=YOUR_ZONE</code></li>
<li><strong>Delete when done</strong>: <code>gcloud compute instances delete hedgehog-vlab --zone=YOUR_ZONE</code></li>
</ul>
<p>Even when stopped, you&#39;ll be charged for disk storage, but it&#39;s much less than the running VM cost.</p>
<h2>Resources</h2>
<ul>
<li><a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">Hedgehog VLAB Documentation</a></li>
<li><a href=""https://cloud.google.com/sdk/docs"">Google Cloud SDK Documentation</a></li>
<li><a href=""https://cloud.google.com/compute/docs/instances/nested-virtualization/overview"">GCP Nested Virtualization Guide</a></li>
<li><a href=""https://cloud.google.com/compute/vm-instance-pricing"">GCP VM Instance Pricing</a></li>
</ul>
<hr>
<p><strong>Ready to continue?</strong> Once you&#39;ve completed the VLAB installation steps from the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">official documentation</a>, you&#39;ll have a complete Hedgehog Fabric environment to explore and learn!</p>
",10,15,,"[""Google Cloud account with an active project"",""Google Cloud SDK (gcloud) installed on your local machine"",""Basic familiarity with command-line tools"",""Sufficient GCP quota for a 32-vCPU instance""]","hedgehog,virtual-lab,gcp,onboarding,access"
197355232901,Accessing the Hedgehog Virtual Lab with Microsoft Azure,accessing-the-hedgehog-virtual-lab-with-microsoft-azure,[object Object],Create and configure a Microsoft Azure VM suitable for running the Hedgehog Virtual Lab so you can begin hands-on learning with Hedgehog Fabric.,"<p>Welcome! This guide will help you create a Microsoft Azure virtual machine that&#39;s ready to run the Hedgehog Virtual Lab. We&#39;ll walk through each step to get you up and running quickly, and we&#39;ll make sure you understand what&#39;s happening along the way.</p>
<h2>What You&#39;ll Learn</h2>
<ul>
<li>Understand the VM requirements for running Hedgehog VLAB</li>
<li>Create a properly configured Azure VM with nested virtualization support</li>
<li>Connect to your VM and prepare for VLAB installation</li>
</ul>
<h2>Prerequisites</h2>
<ul>
<li>A Microsoft Azure account with an active subscription</li>
<li>Azure CLI (az) installed on your local machine</li>
<li>Basic familiarity with command-line tools</li>
</ul>
<p><strong>Need to install Azure CLI?</strong> Visit the <a href=""https://learn.microsoft.com/cli/azure/install-azure-cli"">Azure CLI installation guide</a>.</p>
<h2>Understanding the Requirements</h2>
<p>The Hedgehog Virtual Lab creates a complete virtual network fabric using multiple virtual machines to simulate switches, servers, and a control node. While Hedgehog&#39;s control software is lightweight, the lab uses Broadcom SONiC Virtual Switch VMs, which require substantial resources.</p>
<h3>Recommended VM Size</h3>
<p>We recommend the <strong>Standard_D32s_v3</strong> VM size for the default VLAB topology. This provides:</p>
<ul>
<li>32 vCPUs</li>
<li>128 GB RAM</li>
<li>Support for nested virtualization (critical for VLAB)</li>
<li>Premium SSD storage support</li>
</ul>
<p><strong>Note:</strong> This is slightly below the official <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB documentation requirements</a> (which are based on conservative SONiC virtual switch recommendations), but works well in practice for learning and testing. If you plan to run larger topologies or need guaranteed performance, consider a larger instance.</p>
<p><strong>Important:</strong> We make no performance guarantees for the VLAB environment—different use cases may require different VM sizes. The VLAB is designed for learning and testing, not production workloads.</p>
<h3>Why Standard_D32s_v3?</h3>
<p>Azure supports nested virtualization on specific VM series including Dv3, Dv4, Dv5, Ev3, Ev4, Ev5, and Fsv2 series. The D32s_v3 offers the right balance of compute, memory, and cost for VLAB workloads with:</p>
<ul>
<li>4 GiB memory per vCPU ratio</li>
<li>Intel Xeon processors with Hyper-Threading</li>
<li>Premium storage support for better disk performance</li>
</ul>
<h2>Step 1: Set Up Your Azure Environment</h2>
<p>First, let&#39;s make sure you&#39;re authenticated and ready to create resources.</p>
<h3>Verify Azure CLI Installation</h3>
<pre><code class=""language-bash"">az version
</code></pre>
<p>You should see output showing the Azure CLI version. If not, please install the CLI first.</p>
<h3>Sign in to Azure</h3>
<pre><code class=""language-bash"">az login
</code></pre>
<p>This will open your browser for authentication. Follow the prompts to sign in.</p>
<h3>Set Your Active Subscription</h3>
<p>If you have multiple subscriptions, list them and set the one you want to use:</p>
<pre><code class=""language-bash"">az account list --output table
az account set --subscription &quot;YOUR_SUBSCRIPTION_NAME_OR_ID&quot;
</code></pre>
<p>Verify your active subscription:</p>
<pre><code class=""language-bash"">az account show --output table
</code></pre>
<h2>Step 2: Create a Resource Group</h2>
<p>Azure uses resource groups to organize related resources. Let&#39;s create one for your VLAB:</p>
<pre><code class=""language-bash"">az group create --name hedgehog-vlab-rg --location westus2
</code></pre>
<p><strong>Tip:</strong> Choose a location close to you. See available regions with <code>az account list-locations --output table</code>.</p>
<h2>Step 3: Create Your VLAB Virtual Machine</h2>
<p>Now we&#39;ll create a VM configured specifically for running the Hedgehog Virtual Lab.</p>
<h3>The Command</h3>
<pre><code class=""language-bash"">az vm create \
  --resource-group hedgehog-vlab-rg \
  --name hedgehog-vlab \
  --location westus2 \
  --size Standard_D32s_v3 \
  --image Canonical:ubuntu-24_04-lts:server:latest \
  --admin-username ubuntu \
  --generate-ssh-keys \
  --os-disk-size-gb 200 \
  --security-type Standard
</code></pre>
<h3>What This Command Does</h3>
<p>Let&#39;s break down the key flags:</p>
<ul>
<li><strong><code>--resource-group hedgehog-vlab-rg</code></strong>: Places the VM in the resource group we just created</li>
<li><strong><code>--size Standard_D32s_v3</code></strong>: Creates a VM with 32 vCPUs and 128GB RAM that supports nested virtualization</li>
<li><strong><code>--image Canonical:ubuntu-24_04-lts:server:latest</code></strong>: Uses Ubuntu 24.04 LTS, which is tested and recommended for VLAB</li>
<li><strong><code>--os-disk-size-gb 200</code></strong>: Creates a 200GB disk (VLAB needs substantial disk space for all the virtual switches and servers)</li>
<li><strong><code>--security-type Standard</code></strong>: Uses standard security (Trusted Launch can interfere with nested virtualization)</li>
<li><strong><code>--generate-ssh-keys</code></strong>: Automatically creates SSH keys for secure access</li>
</ul>
<p><strong>Important about Security Type:</strong> We explicitly set <code>--security-type Standard</code> because the default Trusted Launch security type may be incompatible with nested virtualization. This is a critical setting for VLAB to work properly.</p>
<p><strong>Prefer using the Portal?</strong> You can configure most of these settings in the Azure Portal, but make sure to:</p>
<ol>
<li>Select a VM size from the Dv3, Dv4, Dv5, Ev3, Ev4, or Ev5 series (these support nested virtualization)</li>
<li>Set Security Type to &quot;Standard&quot; (found in the Management tab)</li>
<li>Use Ubuntu 24.04 LTS as the operating system</li>
</ol>
<h3>Customize for Your Environment</h3>
<p>Before running the command, you may want to replace:</p>
<ul>
<li><code>westus2</code> with your preferred region</li>
<li><code>hedgehog-vlab-rg</code> with a different resource group name</li>
<li><code>hedgehog-vlab</code> with a different VM name</li>
</ul>
<h3>Run the Command</h3>
<p>Copy the command with your values and run it. The VM creation takes several minutes. You&#39;ll see JSON output showing the VM details when it completes.</p>
<p><strong>Note the public IP address</strong> in the output—you&#39;ll need it to connect!</p>
<h2>Step 4: Connect to Your VM</h2>
<p>Once your VM is created, you can connect via SSH.</p>
<h3>Get the Public IP Address</h3>
<p>If you didn&#39;t note it from the creation output:</p>
<pre><code class=""language-bash"">az vm show --resource-group hedgehog-vlab-rg --name hedgehog-vlab \
  --show-details --query publicIps -o tsv
</code></pre>
<h3>Connect via SSH</h3>
<pre><code class=""language-bash"">ssh ubuntu@YOUR_VM_PUBLIC_IP
</code></pre>
<p>On first connection, you&#39;ll be asked to verify the host key fingerprint—type &quot;yes&quot; to continue.</p>
<p>You should see an Ubuntu welcome message and a command prompt.</p>
<h2>Next Steps: Installing the VLAB</h2>
<p>Congratulations! You now have a VM ready for the Hedgehog Virtual Lab. The installation process from here is identical regardless of whether you&#39;re running on Azure, Google Cloud, AWS, or bare metal.</p>
<h3>What Comes Next</h3>
<p>You&#39;ll need to:</p>
<ol>
<li>Install Docker, QEMU/KVM, and other prerequisites</li>
<li>Install the <code>hhfab</code> utility</li>
<li>Initialize and run the VLAB</li>
<li>Access the control node and switches</li>
</ol>
<p>All of these steps are covered in detail in the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">VLAB Overview documentation</a> starting at the <strong>&quot;Installing Prerequisites&quot;</strong> section.</p>
<p><strong>Look for our companion module:</strong> We&#39;re creating a separate &quot;Setting up the VLAB&quot; module that will walk through the installation steps in the same friendly format as this guide—stay tuned!</p>
<h2>Troubleshooting</h2>
<h3>&quot;az: command not found&quot;</h3>
<p>Install the Azure CLI following the <a href=""https://learn.microsoft.com/cli/azure/install-azure-cli"">official installation guide</a>.</p>
<h3>Authentication Issues</h3>
<p>Make sure you&#39;re logged in with <code>az login</code> and your account has the necessary permissions to create resources in your subscription.</p>
<h3>VM Creation Fails</h3>
<ul>
<li><strong>Quota issues</strong>: Your subscription may not have enough quota for a 32-vCPU instance. Check your quotas in the Azure Portal under Subscriptions → Usage + quotas.</li>
<li><strong>Region availability</strong>: Try a different region if the VM size isn&#39;t available. Check availability with <code>az vm list-skus --location westus2 --size Standard_D --output table</code>.</li>
<li><strong>Image not found</strong>: Verify the Ubuntu image with <code>az vm image list --publisher Canonical --offer ubuntu-24_04-lts --all --output table</code>.</li>
</ul>
<h3>Can&#39;t SSH to the VM</h3>
<ul>
<li>Wait a minute or two after creation for the VM to fully boot</li>
<li>Check that your Network Security Group (NSG) allows SSH (port 22)—it should be open by default</li>
<li>Verify your SSH key was generated correctly in <code>~/.ssh/</code></li>
<li>Try <code>az vm show --resource-group hedgehog-vlab-rg --name hedgehog-vlab --query powerState -o tsv</code> to confirm the VM is running</li>
</ul>
<h3>Nested Virtualization Not Working</h3>
<p>Make sure you:</p>
<ol>
<li>Selected a VM size from a series that supports nested virtualization (Dv3, Dv4, Dv5, Ev3, Ev4, Ev5, Fsv2)</li>
<li>Set Security Type to &quot;Standard&quot; (not Trusted Launch)</li>
</ol>
<h2>Cost Considerations</h2>
<p>The Standard_D32s_v3 instance is a substantial VM and will incur meaningful costs if left running. Here are some tips:</p>
<ul>
<li><strong>Deallocate when not in use</strong>: <code>az vm deallocate --resource-group hedgehog-vlab-rg --name hedgehog-vlab</code></li>
<li><strong>Start when needed</strong>: <code>az vm start --resource-group hedgehog-vlab-rg --name hedgehog-vlab</code></li>
<li><strong>Delete when done</strong>: <code>az group delete --name hedgehog-vlab-rg --yes --no-wait</code> (deletes the entire resource group)</li>
</ul>
<p>When deallocated, you won&#39;t be charged for compute, but you will still pay for storage. To completely stop charges, delete the resource group.</p>
<p><strong>Tip:</strong> Use auto-shutdown to automatically deallocate the VM at a scheduled time each day. You can configure this in the Azure Portal under the VM&#39;s settings.</p>
<h2>Resources</h2>
<ul>
<li><a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">Hedgehog VLAB Documentation</a></li>
<li><a href=""https://learn.microsoft.com/cli/azure/"">Azure CLI Documentation</a></li>
<li><a href=""https://learn.microsoft.com/azure/virtual-machines/nested-virtualization"">Azure Nested Virtualization Overview</a></li>
<li><a href=""https://azure.microsoft.com/pricing/calculator/"">Azure VM Pricing Calculator</a></li>
</ul>
<hr>
<p><strong>Ready to continue?</strong> Once you&#39;ve completed the VLAB installation steps from the <a href=""https://docs.hedgehog.cloud/latest/vlab/overview/"">official documentation</a>, you&#39;ll have a complete Hedgehog Fabric environment to explore and learn!</p>
",12,15,,"[""accessing-the-hedgehog-virtual-lab-with-amazon-web-services"",{""title"":""Microsoft Azure account with an active subscription"",""url"":""https://azure.microsoft.com/free/""},{""title"":""Azure CLI (az) installed"",""url"":""https://learn.microsoft.com/en-us/cli/azure/install-azure-cli""},""Basic familiarity with command-line tools"",""some-nonexistent-module-slug""]","hedgehog,virtual-lab,azure,onboarding,access"
197554690682,Introduction to Kubernetes,intro-to-kubernetes,[object Object],"Learn Kubernetes fundamentals by deploying your first application and exploring pods, services, and core cluster operations with kubectl.","<p>Welcome to your first Kubernetes module! In this hands-on lab, you&#39;ll learn the fundamentals of Kubernetes by deploying your first containerized application.</p>
<h2>Lab: Deploy Your First Application</h2>
<h3>Prerequisites</h3>
<ul>
<li>Access to a Kubernetes cluster (minikube, kind, or cloud provider)</li>
<li>kubectl CLI installed and configured</li>
</ul>
<h3>Step 1: Verify Cluster Access</h3>
<p>First, let&#39;s verify that your Kubernetes cluster is running and accessible:</p>
<pre><code class=""language-bash"">kubectl cluster-info
</code></pre>
<p>You should see output showing your cluster endpoints. Next, check the nodes in your cluster:</p>
<pre><code class=""language-bash"">kubectl get nodes
</code></pre>
<p>This will display all nodes in your cluster along with their status.</p>
<h3>Step 2: Create a Namespace</h3>
<p>Namespaces provide logical isolation for your resources. Create a namespace for this lab:</p>
<pre><code class=""language-bash"">kubectl create namespace learn-k8s
</code></pre>
<p>Verify the namespace was created:</p>
<pre><code class=""language-bash"">kubectl get namespaces
</code></pre>
<h3>Step 3: Deploy Your First Pod</h3>
<p>A Pod is the smallest deployable unit in Kubernetes. Let&#39;s deploy a simple nginx web server:</p>
<pre><code class=""language-bash"">kubectl run nginx --image=nginx:latest --namespace=learn-k8s
</code></pre>
<p>Check the status of your pod:</p>
<pre><code class=""language-bash"">kubectl get pods --namespace=learn-k8s
</code></pre>
<p>Wait until the STATUS shows &quot;Running&quot;.</p>
<h3>Step 4: Inspect the Pod</h3>
<p>Get detailed information about your pod:</p>
<pre><code class=""language-bash"">kubectl describe pod nginx --namespace=learn-k8s
</code></pre>
<p>This command shows extensive details including events, container status, and resource allocations.</p>
<h3>Step 5: Access Pod Logs</h3>
<p>View the logs from your nginx container:</p>
<pre><code class=""language-bash"">kubectl logs nginx --namespace=learn-k8s
</code></pre>
<p>Follow logs in real-time:</p>
<pre><code class=""language-bash"">kubectl logs -f nginx --namespace=learn-k8s
</code></pre>
<p>Press Ctrl+C to stop following logs.</p>
<h3>Step 6: Execute Commands in the Pod</h3>
<p>You can run commands inside a running container:</p>
<pre><code class=""language-bash"">kubectl exec nginx --namespace=learn-k8s -- nginx -v
</code></pre>
<p>Open an interactive shell:</p>
<pre><code class=""language-bash"">kubectl exec -it nginx --namespace=learn-k8s -- /bin/bash
</code></pre>
<p>Type <code>exit</code> to leave the shell.</p>
<h3>Step 7: Expose the Pod with a Service</h3>
<p>Create a Service to make your nginx pod accessible:</p>
<pre><code class=""language-bash"">kubectl expose pod nginx --port=80 --namespace=learn-k8s --type=NodePort
</code></pre>
<p>Get the service details:</p>
<pre><code class=""language-bash"">kubectl get service nginx --namespace=learn-k8s
</code></pre>
<h3>Step 8: Test Connectivity</h3>
<p>Port-forward to access the service locally:</p>
<pre><code class=""language-bash"">kubectl port-forward service/nginx 8080:80 --namespace=learn-k8s
</code></pre>
<p>In another terminal or browser, visit <code>http://localhost:8080</code>. You should see the nginx welcome page.</p>
<h3>Step 9: Scale with a Deployment</h3>
<p>While we created a single Pod, in production you&#39;ll use Deployments. Create a deployment:</p>
<pre><code class=""language-bash"">kubectl create deployment web --image=nginx:latest --replicas=3 --namespace=learn-k8s
</code></pre>
<p>Watch the pods being created:</p>
<pre><code class=""language-bash"">kubectl get pods --namespace=learn-k8s --watch
</code></pre>
<h3>Step 10: Clean Up</h3>
<p>Remove all resources in the namespace:</p>
<pre><code class=""language-bash"">kubectl delete namespace learn-k8s
</code></pre>
<p>Verify deletion:</p>
<pre><code class=""language-bash"">kubectl get namespaces
</code></pre>
<h2>Concepts: Understanding Kubernetes Fundamentals</h2>
<h3>What is Kubernetes?</h3>
<p>Kubernetes (often abbreviated as K8s) is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Originally developed by Google and now maintained by the Cloud Native Computing Foundation (CNCF), Kubernetes has become the de facto standard for container orchestration.</p>
<h3>Why Kubernetes?</h3>
<p>As applications grow in complexity and scale, managing containers manually becomes impractical. Kubernetes solves this by providing:</p>
<ul>
<li><strong>Automated rollouts and rollbacks</strong>: Deploy new versions safely with automatic rollback on failure</li>
<li><strong>Service discovery and load balancing</strong>: Automatic DNS and load balancing for your services</li>
<li><strong>Storage orchestration</strong>: Automatically mount storage systems of your choice</li>
<li><strong>Self-healing</strong>: Restarts failed containers, replaces containers, and kills containers that don&#39;t respond to health checks</li>
<li><strong>Secret and configuration management</strong>: Deploy and update secrets and configuration without rebuilding images</li>
<li><strong>Horizontal scaling</strong>: Scale your application up and down with a simple command</li>
</ul>
<h3>Core Kubernetes Components</h3>
<p><strong>Control Plane Components:</strong></p>
<p>The control plane manages the cluster and makes global decisions about the cluster:</p>
<ul>
<li><strong>API Server (kube-apiserver)</strong>: The front-end for the Kubernetes control plane. All administrative tasks go through the API server.</li>
<li><strong>etcd</strong>: Consistent and highly-available key-value store used as Kubernetes&#39; backing store for all cluster data.</li>
<li><strong>Scheduler (kube-scheduler)</strong>: Watches for newly created Pods and assigns them to nodes.</li>
<li><strong>Controller Manager (kube-controller-manager)</strong>: Runs controller processes that regulate the state of the cluster.</li>
</ul>
<p><strong>Node Components:</strong></p>
<p>These components run on every node:</p>
<ul>
<li><strong>kubelet</strong>: An agent that ensures containers are running in a Pod.</li>
<li><strong>kube-proxy</strong>: Maintains network rules on nodes, allowing network communication to your Pods.</li>
<li><strong>Container Runtime</strong>: Software responsible for running containers (e.g., Docker, containerd, CRI-O).</li>
</ul>
<h3>Kubernetes Objects</h3>
<p>Kubernetes uses objects to represent the state of your cluster. Key objects include:</p>
<p><strong>Pod</strong>: The smallest deployable unit, representing one or more containers that share storage and network resources.</p>
<p><strong>Deployment</strong>: Manages a replicated application, providing declarative updates for Pods and ReplicaSets.</p>
<p><strong>Service</strong>: An abstract way to expose an application running on a set of Pods as a network service.</p>
<p><strong>Namespace</strong>: Virtual clusters backed by the same physical cluster, used for dividing cluster resources between multiple users.</p>
<p><strong>ConfigMap &amp; Secret</strong>: Objects for storing configuration data and sensitive information separately from application code.</p>
<h3>The Kubernetes Architecture</h3>
<p>Kubernetes follows a master-worker architecture:</p>
<ol>
<li><strong>Master Node (Control Plane)</strong>: Makes global decisions and detects/responds to cluster events</li>
<li><strong>Worker Nodes</strong>: Run your application workloads in Pods</li>
</ol>
<p>The control plane maintains the desired state of the cluster, while kubelet on each node ensures that containers are running and healthy.</p>
<h3>Desired State Management</h3>
<p>Kubernetes operates on the principle of <strong>desired state</strong>. You declare the desired state of your application (e.g., &quot;I want 3 replicas of my nginx app&quot;), and Kubernetes works continuously to maintain that state. If a Pod crashes, Kubernetes automatically starts a new one to maintain your desired count.</p>
<h3>Benefits of the Kubernetes Approach</h3>
<ol>
<li><strong>Declarative Configuration</strong>: Describe what you want, not how to achieve it</li>
<li><strong>Infrastructure as Code</strong>: Version control your infrastructure configurations</li>
<li><strong>Portability</strong>: Run on any cloud provider or on-premises</li>
<li><strong>Ecosystem</strong>: Extensive ecosystem of tools and extensions</li>
<li><strong>Community</strong>: Large, active community and enterprise support</li>
</ol>
<p>Understanding these fundamentals is crucial as you continue your Kubernetes journey. Every advanced concept builds on these core principles.</p>
<h2>Resources</h2>
<ul>
<li><a href=""https://kubernetes.io/docs/"">Official Kubernetes Documentation</a></li>
<li><a href=""https://kubernetes.io/docs/tutorials/kubernetes-basics/"">Kubernetes Basics Tutorial</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubectl/cheatsheet/"">kubectl Cheat Sheet</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubernetes-api/"">Kubernetes API Reference</a></li>
<li><a href=""https://www.cncf.io/certification/cka/"">CNCF Kubernetes Certification</a></li>
</ul>
",999,30,,"[""Basic understanding of containers and Docker"",""Access to a Kubernetes cluster"",""kubectl CLI installed""]","kubernetes,containers,orchestration,docker"
197554690683,"Kubernetes Networking: Services, Ingress, and Network Policies",kubernetes-networking,[object Object],"Master Kubernetes networking: expose services, configure Ingress, and enforce Network Policies to secure and route traffic between pods.","<p>Master Kubernetes networking concepts including Services, Ingress controllers, and Network Policies for secure communication between pods.</p>
<h2>Lab: Kubernetes Networking in Action</h2>
<h3>Prerequisites</h3>
<ul>
<li>Completed &quot;Introduction to Kubernetes&quot; module</li>
<li>Access to a Kubernetes cluster</li>
<li>kubectl CLI configured</li>
<li>Ingress controller installed (e.g., nginx-ingress)</li>
<li>Cluster can pull public container images (e.g., <code>curlimages/curl</code>)</li>
</ul>
<h3>Step 1: Create a Namespace and Deployment</h3>
<p>Create a namespace:</p>
<pre><code class=""language-bash"">kubectl create namespace networking-lab
</code></pre>
<p>Create a deployment with multiple replicas:</p>
<pre><code class=""language-bash"">kubectl create deployment web --image=nginx:latest --replicas=3 --namespace=networking-lab
</code></pre>
<p>Verify the pods are running:</p>
<pre><code class=""language-bash"">kubectl get pods -n networking-lab -o wide
</code></pre>
<p>Note the IP addresses of each pod.</p>
<h3>Step 2: Create a ClusterIP Service</h3>
<p>Create a file named <code>service-clusterip.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: networking-lab
spec:
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
</code></pre>
<p>Apply the Service:</p>
<pre><code class=""language-bash"">kubectl apply -f service-clusterip.yaml
</code></pre>
<p>Get the Service details:</p>
<pre><code class=""language-bash"">kubectl get service web-service -n networking-lab
</code></pre>
<p>Test the Service from within the cluster using a lightweight curl image:</p>
<pre><code class=""language-bash"">kubectl run test-client --image=curlimages/curl:8.5.0 --namespace=networking-lab --rm -it --restart=Never -- -s http://web-service
</code></pre>
<h3>Step 3: Explore Service Discovery</h3>
<p>Services are automatically registered in DNS:</p>
<pre><code class=""language-bash"">kubectl run dns-test --image=busybox --namespace=networking-lab --rm -it --restart=Never -- nslookup web-service
</code></pre>
<p>The service is accessible at: <code>web-service.networking-lab.svc.cluster.local</code></p>
<h3>Step 4: Create a NodePort Service</h3>
<p>Create a file named <code>service-nodeport.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: Service
metadata:
  name: web-nodeport
  namespace: networking-lab
spec:
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
</code></pre>
<p>Apply the NodePort Service:</p>
<pre><code class=""language-bash"">kubectl apply -f service-nodeport.yaml
</code></pre>
<p>Get the Service info:</p>
<pre><code class=""language-bash"">kubectl get service web-nodeport -n networking-lab
</code></pre>
<p>Access the service on any node&#39;s IP at port 30080.</p>
<h3>Step 5: Create a LoadBalancer Service</h3>
<p>Create a file named <code>service-loadbalancer.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: Service
metadata:
  name: web-loadbalancer
  namespace: networking-lab
spec:
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: LoadBalancer
</code></pre>
<p>Apply the LoadBalancer Service:</p>
<pre><code class=""language-bash"">kubectl apply -f service-loadbalancer.yaml
</code></pre>
<p>Wait for external IP assignment:</p>
<pre><code class=""language-bash"">kubectl get service web-loadbalancer -n networking-lab --watch
</code></pre>
<p>(This requires a cloud provider or MetalLB)</p>
<h3>Step 6: Create an Ingress Resource</h3>
<p>First, create a second deployment:</p>
<pre><code class=""language-bash"">kubectl create deployment api --image=hashicorp/http-echo --namespace=networking-lab -- -text=&quot;API Response&quot;
</code></pre>
<p>Create a Service for the API:</p>
<pre><code class=""language-bash"">kubectl expose deployment api --port=5678 --namespace=networking-lab
</code></pre>
<p>Create a file named <code>ingress.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: networking-lab
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api
            port:
              number: 5678
</code></pre>
<p>Apply the Ingress:</p>
<pre><code class=""language-bash"">kubectl apply -f ingress.yaml
</code></pre>
<p>Check Ingress status:</p>
<pre><code class=""language-bash"">kubectl get ingress -n networking-lab
</code></pre>
<p>Test with curl by resolving the hostnames to the Ingress IP (either update <code>/etc/hosts</code> or use <code>--resolve</code>):</p>
<pre><code class=""language-bash"">curl --resolve web.example.com:80:&lt;ingress-ip&gt; http://web.example.com
curl --resolve api.example.com:80:&lt;ingress-ip&gt; http://api.example.com
</code></pre>
<h3>Step 7: Implement Network Policies</h3>
<p>By default, all pods can communicate with each other. Let&#39;s restrict this.</p>
<p>Create a file named <code>network-policy-deny-all.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: networking-lab
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
</code></pre>
<p>Apply the policy:</p>
<pre><code class=""language-bash"">kubectl apply -f network-policy-deny-all.yaml
</code></pre>
<p>Test connectivity (it should fail). <code>--max-time</code> stops curl after five seconds so the command exits quickly:</p>
<pre><code class=""language-bash"">kubectl run deny-check --image=curlimages/curl:8.5.0 --namespace=networking-lab --rm -it --restart=Never -- --max-time 5 -sf http://web-service
</code></pre>
<h3>Step 8: Allow Specific Traffic</h3>
<p>Create a file named <code>network-policy-allow-web.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-traffic
  namespace: networking-lab
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
    ports:
    - protocol: TCP
      port: 80
</code></pre>
<p>Apply the policy:</p>
<pre><code class=""language-bash"">kubectl apply -f network-policy-allow-web.yaml
</code></pre>
<p>Also allow DNS:</p>
<pre><code class=""language-yaml"">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: networking-lab
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
</code></pre>
<pre><code class=""language-bash"">kubectl apply -f network-policy-allow-dns.yaml
</code></pre>
<p>Now test again (should work):</p>
<pre><code class=""language-bash"">kubectl run allow-check --image=curlimages/curl:8.5.0 --namespace=networking-lab --rm -it --restart=Never -- -sf http://web-service
</code></pre>
<h3>Step 9: Verify Network Policy Enforcement</h3>
<p>Try accessing from a different namespace (should fail):</p>
<pre><code class=""language-bash"">kubectl create namespace other-namespace
kubectl run external-check --image=curlimages/curl:8.5.0 --namespace=other-namespace --rm -it --restart=Never -- --max-time 5 -sf http://web-service.networking-lab
</code></pre>
<h3>Step 10: Clean Up</h3>
<p>Delete the namespace:</p>
<pre><code class=""language-bash"">kubectl delete namespace networking-lab
kubectl delete namespace other-namespace
</code></pre>
<h2>Concepts: Understanding Kubernetes Networking</h2>
<h3>The Kubernetes Networking Model</h3>
<p>Kubernetes imposes the following fundamental requirements on any networking implementation:</p>
<ol>
<li><strong>Pods can communicate with all other pods</strong> on any node without NAT</li>
<li><strong>Agents on a node</strong> can communicate with all pods on that node</li>
<li><strong>Pods in the host network</strong> can communicate with all pods on all nodes without NAT</li>
</ol>
<p>This creates a &quot;flat&quot; network where every pod gets its own IP address and can communicate directly with other pods.</p>
<h3>Services: The Abstraction Layer</h3>
<p>Pods are ephemeral—they can be created, destroyed, and moved around. Their IP addresses change. Services provide a stable endpoint for accessing a set of pods.</p>
<p><strong>How Services Work:</strong></p>
<ol>
<li>Service gets a stable virtual IP (ClusterIP)</li>
<li>kube-proxy maintains iptables/IPVS rules on each node</li>
<li>Traffic to the Service IP is load-balanced to backend pods</li>
<li>Service selector determines which pods receive traffic</li>
</ol>
<h3>Service Types</h3>
<p><strong>ClusterIP (Default)</strong></p>
<ul>
<li>Service is only accessible within the cluster</li>
<li>Gets a virtual IP from the service CIDR range</li>
<li>Used for internal communication between services</li>
</ul>
<p><strong>NodePort</strong></p>
<ul>
<li>Exposes service on each node&#39;s IP at a static port (30000-32767)</li>
<li>Automatically creates a ClusterIP service</li>
<li>External traffic → NodePort → ClusterIP → Pods</li>
<li>Use case: Simple external access, development environments</li>
</ul>
<p><strong>LoadBalancer</strong></p>
<ul>
<li>Exposes service externally using cloud provider&#39;s load balancer</li>
<li>Automatically creates NodePort and ClusterIP services</li>
<li>Cloud provider provisions external load balancer</li>
<li>Use case: Production external access on cloud platforms</li>
</ul>
<p><strong>ExternalName</strong></p>
<ul>
<li>Maps service to a DNS name</li>
<li>No proxying, just DNS CNAME record</li>
<li>Use case: Accessing external services with a consistent internal name</li>
</ul>
<h3>Ingress: HTTP/HTTPS Routing</h3>
<p>Ingress provides HTTP and HTTPS routing to services based on hostnames and paths. Unlike Services, Ingress is not a service type—it&#39;s a separate resource that sits in front of services.</p>
<p><strong>Benefits of Ingress:</strong></p>
<ul>
<li><strong>Single entry point</strong>: One load balancer for multiple services</li>
<li><strong>Host-based routing</strong>: Route based on hostname (virtual hosting)</li>
<li><strong>Path-based routing</strong>: Route based on URL path</li>
<li><strong>TLS termination</strong>: Handle SSL/TLS certificates centrally</li>
<li><strong>Cost effective</strong>: One load balancer instead of many</li>
</ul>
<p><strong>Ingress Controllers:</strong></p>
<p>Ingress resources don&#39;t do anything by themselves—you need an Ingress controller:</p>
<ul>
<li><strong>NGINX Ingress Controller</strong>: Most popular, feature-rich</li>
<li><strong>Traefik</strong>: Modern, easy to use, automatic Let&#39;s Encrypt</li>
<li><strong>HAProxy Ingress</strong>: High performance</li>
<li><strong>Cloud-specific</strong>: AWS ALB, Google Cloud Load Balancer, Azure Application Gateway</li>
</ul>
<h3>DNS in Kubernetes</h3>
<p>Kubernetes runs a DNS service (typically CoreDNS) that provides DNS resolution for services and pods.</p>
<p><strong>Service DNS Names:</strong></p>
<p>Format: <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></p>
<ul>
<li>Within same namespace: Just use <code>&lt;service-name&gt;</code></li>
<li>Cross-namespace: Use <code>&lt;service-name&gt;.&lt;namespace&gt;</code></li>
<li>Fully qualified: Use full FQDN</li>
</ul>
<p><strong>Pod DNS Names:</strong></p>
<p>Format: <code>&lt;pod-ip-with-dashes&gt;.&lt;namespace&gt;.pod.cluster.local</code></p>
<p>Example: <code>10-244-1-5.default.pod.cluster.local</code></p>
<h3>Network Policies: Microsegmentation</h3>
<p>By default, Kubernetes allows all pods to communicate with each other. Network Policies provide firewall rules to control pod-to-pod traffic.</p>
<p><strong>Network Policy Features:</strong></p>
<ul>
<li><strong>Pod selector</strong>: Which pods the policy applies to</li>
<li><strong>Ingress rules</strong>: Incoming traffic controls</li>
<li><strong>Egress rules</strong>: Outgoing traffic controls</li>
<li><strong>Namespace selector</strong>: Allow traffic from specific namespaces</li>
<li><strong>IP block selector</strong>: Allow traffic from specific CIDR blocks</li>
</ul>
<p><strong>Important Notes:</strong></p>
<ul>
<li>Network Policies are additive (allow-list, not deny-list)</li>
<li>Requires a CNI plugin that supports Network Policies (Calico, Cilium, Weave)</li>
<li>Default deny-all is a common starting point</li>
<li>Policies are applied at the pod level, not the service level</li>
</ul>
<h3>Container Network Interface (CNI)</h3>
<p>Kubernetes uses CNI plugins to set up pod networking. Popular CNI plugins include:</p>
<ul>
<li><strong>Calico</strong>: Feature-rich, supports Network Policies, BGP routing</li>
<li><strong>Cilium</strong>: eBPF-based, high performance, observability</li>
<li><strong>Flannel</strong>: Simple, overlay network</li>
<li><strong>Weave Net</strong>: Encrypted overlay network</li>
<li><strong>AWS VPC CNI</strong>: Native AWS VPC networking</li>
<li><strong>Multus</strong>: Multiple network interfaces per pod</li>
</ul>
<h3>Common Networking Patterns</h3>
<p><strong>Service Mesh (Advanced):</strong></p>
<ul>
<li>Istio, Linkerd, Consul Connect</li>
<li>Advanced traffic management, security, observability</li>
<li>Sidecar proxies for each pod</li>
</ul>
<p><strong>East-West Traffic:</strong></p>
<ul>
<li>Service-to-service communication within the cluster</li>
<li>Use ClusterIP services</li>
<li>Implement Network Policies for security</li>
</ul>
<p><strong>North-South Traffic:</strong></p>
<ul>
<li>External traffic entering/leaving the cluster</li>
<li>Use LoadBalancer or Ingress</li>
<li>Implement TLS termination at Ingress</li>
</ul>
<p><strong>Multi-Cluster Networking:</strong></p>
<ul>
<li>Connect pods across multiple clusters</li>
<li>Tools: Submariner, Cilium Cluster Mesh, Istio multi-cluster</li>
</ul>
<h3>Best Practices</h3>
<ol>
<li><strong>Use Services for discovery</strong>: Don&#39;t rely on pod IPs</li>
<li><strong>Implement Network Policies</strong>: Start with default deny-all</li>
<li><strong>Use Ingress for HTTP/HTTPS</strong>: More cost-effective than multiple LoadBalancers</li>
<li><strong>Monitor network traffic</strong>: Use tools like Cilium Hubble</li>
<li><strong>Plan IP address space</strong>: Avoid CIDR conflicts</li>
<li><strong>Secure with TLS</strong>: Use cert-manager for automatic certificate management</li>
<li><strong>Test network policies</strong>: Verify policies work as expected</li>
<li><strong>Document network architecture</strong>: Maintain diagrams of traffic flows</li>
</ol>
<p>Understanding Kubernetes networking is essential for building secure, scalable applications.</p>
<h2>Resources</h2>
<ul>
<li><a href=""https://kubernetes.io/docs/concepts/cluster-administration/networking/"">Kubernetes Networking Documentation</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/services-networking/service/"">Services</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/services-networking/ingress/"">Ingress</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/services-networking/network-policies/"">Network Policies</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"">DNS for Services and Pods</a></li>
</ul>
",999,50,,"[""Completed Introduction to Kubernetes module"",""Understanding of basic networking concepts (TCP/IP, DNS)"",""Access to a Kubernetes cluster with Ingress controller"",""Ability to pull images from Docker Hub (curlimages/curl)""]","kubernetes,networking,services,ingress,network-policies"
197554637405,Kubernetes Storage: Persistent Volumes and Claims,kubernetes-storage,[object Object],"Learn persistent storage in Kubernetes using Persistent Volumes, Persistent Volume Claims, and StorageClasses with a hands-on lab.","<p>Learn how to manage persistent storage in Kubernetes using Persistent Volumes (PVs), Persistent Volume Claims (PVCs), and Storage Classes.</p>
<h2>Lab: Working with Persistent Storage</h2>
<h3>Prerequisites</h3>
<ul>
<li>Completed &quot;Introduction to Kubernetes&quot; module</li>
<li>Access to a Kubernetes cluster with a storage provisioner</li>
<li>kubectl CLI configured</li>
</ul>
<h3>Step 1: Create a Namespace</h3>
<p>Create a namespace for this lab:</p>
<pre><code class=""language-bash"">kubectl create namespace storage-lab
</code></pre>
<h3>Step 2: Create a Persistent Volume</h3>
<p>Create a file named <code>pv.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /mnt/data
</code></pre>
<p>Apply the PersistentVolume:</p>
<pre><code class=""language-bash"">kubectl apply -f pv.yaml
</code></pre>
<p>Verify the PV was created:</p>
<pre><code class=""language-bash"">kubectl get pv
</code></pre>
<p>The STATUS should show &quot;Available&quot;.</p>
<h3>Step 3: Create a Persistent Volume Claim</h3>
<p>Create a file named <code>pvc.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
  namespace: storage-lab
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: manual
</code></pre>
<p>Apply the PVC:</p>
<pre><code class=""language-bash"">kubectl apply -f pvc.yaml
</code></pre>
<p>Check the PVC status:</p>
<pre><code class=""language-bash"">kubectl get pvc -n storage-lab
</code></pre>
<p>The PVC should be &quot;Bound&quot; to the PV.</p>
<h3>Step 4: Use the PVC in a Pod</h3>
<p>Create a file named <code>pod-with-pvc.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: Pod
metadata:
  name: storage-pod
  namespace: storage-lab
spec:
  containers:
  - name: app
    image: nginx:latest
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: storage
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc
</code></pre>
<p>Create the pod:</p>
<pre><code class=""language-bash"">kubectl apply -f pod-with-pvc.yaml
</code></pre>
<p>Wait for the pod to be running:</p>
<pre><code class=""language-bash"">kubectl wait --for=condition=Ready pod/storage-pod -n storage-lab --timeout=60s
</code></pre>
<h3>Step 5: Write Data to Persistent Storage</h3>
<p>Write some content to the persistent volume:</p>
<pre><code class=""language-bash"">kubectl exec -it storage-pod -n storage-lab -- /bin/bash -c &quot;echo &#39;Hello from persistent storage!&#39; &gt; /usr/share/nginx/html/index.html&quot;
</code></pre>
<p>Verify the content:</p>
<pre><code class=""language-bash"">kubectl exec storage-pod -n storage-lab -- cat /usr/share/nginx/html/index.html
</code></pre>
<h3>Step 6: Test Persistence</h3>
<p>Delete the pod:</p>
<pre><code class=""language-bash"">kubectl delete pod storage-pod -n storage-lab
</code></pre>
<p>Recreate the pod using the same YAML:</p>
<pre><code class=""language-bash"">kubectl apply -f pod-with-pvc.yaml
</code></pre>
<p>Wait for the pod to be ready:</p>
<pre><code class=""language-bash"">kubectl wait --for=condition=Ready pod/storage-pod -n storage-lab --timeout=60s
</code></pre>
<p>Verify the data persisted:</p>
<pre><code class=""language-bash"">kubectl exec storage-pod -n storage-lab -- cat /usr/share/nginx/html/index.html
</code></pre>
<p>You should see the same content!</p>
<h3>Step 7: Explore Storage Classes</h3>
<p>List available storage classes in your cluster:</p>
<pre><code class=""language-bash"">kubectl get storageclass
</code></pre>
<p>Get details on a storage class:</p>
<pre><code class=""language-bash"">kubectl describe storageclass &lt;storage-class-name&gt;
</code></pre>
<h3>Step 8: Create a Dynamic PVC</h3>
<p>Create a file named <code>dynamic-pvc.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
  namespace: storage-lab
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: &lt;default-storage-class&gt;
</code></pre>
<p>Replace <code>&lt;default-storage-class&gt;</code> with your cluster&#39;s default storage class, then apply:</p>
<pre><code class=""language-bash"">kubectl apply -f dynamic-pvc.yaml
</code></pre>
<p>Watch the PV be automatically provisioned:</p>
<pre><code class=""language-bash"">kubectl get pv,pvc -n storage-lab
</code></pre>
<h3>Step 9: Create a StatefulSet with Storage</h3>
<p>Create a file named <code>statefulset.yaml</code>:</p>
<pre><code class=""language-yaml"">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
  namespace: storage-lab
spec:
  serviceName: &quot;web&quot;
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 1Gi
</code></pre>
<p>Create the StatefulSet:</p>
<pre><code class=""language-bash"">kubectl apply -f statefulset.yaml
</code></pre>
<p>Watch the pods and PVCs being created:</p>
<pre><code class=""language-bash"">kubectl get pods,pvc -n storage-lab -w
</code></pre>
<p>Each pod gets its own PVC automatically!</p>
<h3>Step 10: Clean Up</h3>
<p>Delete all resources:</p>
<pre><code class=""language-bash"">kubectl delete namespace storage-lab
kubectl delete pv my-pv
</code></pre>
<h2>Concepts: Understanding Kubernetes Storage</h2>
<h3>The Storage Problem in Kubernetes</h3>
<p>By default, containers are ephemeral—when a Pod is deleted, all its data is lost. For stateful applications like databases, this is unacceptable. Kubernetes provides several abstractions to handle persistent storage.</p>
<h3>Storage Architecture</h3>
<p>Kubernetes storage architecture consists of several key components:</p>
<ol>
<li><strong>Persistent Volumes (PV)</strong>: Cluster-wide storage resources</li>
<li><strong>Persistent Volume Claims (PVC)</strong>: Requests for storage by users</li>
<li><strong>Storage Classes</strong>: Dynamic provisioning of storage</li>
<li><strong>Volume Plugins</strong>: Interfaces to storage systems</li>
</ol>
<h3>Persistent Volumes (PV)</h3>
<p>A PersistentVolume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It&#39;s a cluster resource, just like a node.</p>
<p><strong>Key Properties:</strong></p>
<ul>
<li><strong>Capacity</strong>: Size of the storage</li>
<li><strong>Access Modes</strong>: How the volume can be mounted (ReadWriteOnce, ReadOnlyMany, ReadWriteMany)</li>
<li><strong>Reclaim Policy</strong>: What happens when the PVC is deleted (Retain, Recycle, Delete)</li>
<li><strong>Storage Class</strong>: For dynamic provisioning</li>
<li><strong>Volume Type</strong>: The underlying storage system (hostPath, NFS, AWS EBS, etc.)</li>
</ul>
<h3>Persistent Volume Claims (PVC)</h3>
<p>A PersistentVolumeClaim is a request for storage by a user. It&#39;s similar to how a Pod consumes node resources; a PVC consumes PV resources.</p>
<p><strong>Binding Process:</strong></p>
<ol>
<li>User creates a PVC with desired storage size and access mode</li>
<li>Kubernetes finds a matching PV (or dynamically provisions one)</li>
<li>The PVC is bound to the PV</li>
<li>Pods can now use the PVC</li>
</ol>
<h3>Access Modes</h3>
<p>Storage can be mounted in different modes:</p>
<ul>
<li><strong>ReadWriteOnce (RWO)</strong>: Volume can be mounted read-write by a single node</li>
<li><strong>ReadOnlyMany (ROX)</strong>: Volume can be mounted read-only by many nodes</li>
<li><strong>ReadWriteMany (RWX)</strong>: Volume can be mounted read-write by many nodes</li>
</ul>
<p>Not all storage types support all access modes. For example, AWS EBS only supports RWO.</p>
<h3>Storage Classes</h3>
<p>Storage Classes provide a way to describe different &quot;classes&quot; of storage. They enable dynamic provisioning of PVs.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>No manual PV creation by administrators</li>
<li>Automatic provisioning when PVC is created</li>
<li>Different tiers of storage (SSD vs HDD, fast vs slow)</li>
<li>Cloud provider integration (AWS EBS, Google Persistent Disk, Azure Disk)</li>
</ul>
<p><strong>Example Storage Class:</strong></p>
<pre><code class=""language-yaml"">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: &quot;3000&quot;
</code></pre>
<h3>Reclaim Policies</h3>
<p>When a PVC is deleted, the PV can be handled in three ways:</p>
<ul>
<li><strong>Retain</strong>: Manual reclamation—PV still exists with data</li>
<li><strong>Delete</strong>: Automatically delete the PV and underlying storage</li>
<li><strong>Recycle</strong>: (Deprecated) Basic scrub and make available for new claim</li>
</ul>
<p>For production workloads with important data, &quot;Retain&quot; is typically the safest choice.</p>
<h3>StatefulSets and Storage</h3>
<p>StatefulSets are designed for stateful applications. They provide:</p>
<ul>
<li><strong>Stable network identifiers</strong>: Each pod gets a predictable name</li>
<li><strong>Stable storage</strong>: PVCs persist across pod rescheduling</li>
<li><strong>Ordered deployment and scaling</strong>: Pods are created/deleted in order</li>
</ul>
<p>The <code>volumeClaimTemplates</code> field in a StatefulSet automatically creates a PVC for each pod replica, ensuring each has dedicated storage.</p>
<h3>Best Practices</h3>
<ol>
<li><strong>Use Storage Classes</strong>: Enable dynamic provisioning for flexibility</li>
<li><strong>Size appropriately</strong>: Request storage sizes based on actual needs</li>
<li><strong>Choose correct access mode</strong>: Use RWO when possible for better performance</li>
<li><strong>Set resource limits</strong>: Prevent storage exhaustion</li>
<li><strong>Back up data</strong>: Implement backup strategies for critical data</li>
<li><strong>Monitor usage</strong>: Track storage consumption and performance</li>
<li><strong>Test persistence</strong>: Verify data survives pod restarts</li>
<li><strong>Use StatefulSets for stateful apps</strong>: Databases, message queues, etc.</li>
</ol>
<h3>Common Use Cases</h3>
<ul>
<li><strong>Databases</strong>: PostgreSQL, MySQL, MongoDB with persistent data</li>
<li><strong>Message Queues</strong>: RabbitMQ, Kafka with durable messages</li>
<li><strong>Shared Configuration</strong>: ConfigMaps and Secrets mounted as volumes</li>
<li><strong>Log Aggregation</strong>: Persistent storage for log files</li>
<li><strong>CI/CD Artifacts</strong>: Build artifacts and caches</li>
</ul>
<p>Understanding storage is crucial for running stateful applications successfully in Kubernetes.</p>
<h2>Resources</h2>
<ul>
<li><a href=""https://kubernetes.io/docs/concepts/storage/"">Kubernetes Storage Documentation</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/storage/persistent-volumes/"">Persistent Volumes</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/storage/storage-classes/"">Storage Classes</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"">StatefulSets</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/storage/volume-snapshots/"">Volume Snapshots</a></li>
</ul>
",999,45,,"[""Completed Introduction to Kubernetes module"",""Basic understanding of storage concepts"",""Access to a Kubernetes cluster with storage provisioner""]","kubernetes,storage,persistent-volumes,stateful"
197744301899,Course 1 Recap & Forward Map,fabric-operations-foundations-recap,[object Object],"Celebrate Course 1 wins, reinforce key concepts, and map your progression into provisioning and connectivity.","<p><strong>Course:</strong> Course 1 - Foundations &amp; Interfaces
<strong>Duration:</strong> 8-10 minutes
<strong>Prerequisites:</strong> Modules 1.1, 1.2, and 1.3 (completed)</p>
<hr>
<h2>Introduction</h2>
<h3>You&#39;ve Come a Long Way</h3>
<p>Three modules ago, you might have thought managing network infrastructure meant SSH-ing into switches, typing arcane commands, and hoping nothing breaks. Now you know better.</p>
<p>You&#39;ve explored a production-style fabric using kubectl. You&#39;ve created a VPC using GitOps. You&#39;ve monitored switches with Grafana dashboards. You&#39;ve learned to think like a Hedgehog operator.</p>
<p>Before we move forward, let&#39;s take a moment to consolidate what you&#39;ve learned—and preview where you&#39;re going next.</p>
<h3>Course 1 Complete</h3>
<p>This module marks the completion of <strong>Course 1: Foundations &amp; Interfaces</strong>. You&#39;ve built the foundation—now it&#39;s time to build on it.</p>
<p><strong>What this module covers:</strong></p>
<ul>
<li>Quick recap of Modules 1.1-1.3 (what you&#39;ve learned)</li>
<li>Knowledge check questions (test your understanding)</li>
<li>Preview of Course 2 (where you&#39;re going next)</li>
<li>Confidence-building for the next phase</li>
</ul>
<p><strong>What&#39;s different about this module:</strong></p>
<ul>
<li>No hands-on lab (this is reflection time)</li>
<li>Self-check questions (answers provided, non-graded)</li>
<li>Shorter duration (8-10 minutes)</li>
<li>Focus on consolidation and readiness</li>
</ul>
<p>Let&#39;s celebrate what you&#39;ve accomplished and prepare for what&#39;s next.</p>
<hr>
<h2>Course 1 Summary</h2>
<h3>Module 1.1 Recap: Welcome to Fabric Operations</h3>
<p><strong>What You Learned:</strong></p>
<p>Hedgehog Fabric is <strong>pre-installed, declaratively-managed infrastructure</strong>. Your role as a Fabric Operator is to provision, validate, monitor, and troubleshoot network resources—not to install switches or design topologies from scratch.</p>
<p>You learned that network resources are represented as <strong>Kubernetes CRDs</strong>: switches, servers, connections, and VPCs all exist as objects you can inspect with kubectl.</p>
<p><strong>Key Moment:</strong></p>
<p>You successfully explored the fabric topology using kubectl—listing switches, servers, and connections—without touching a single switch CLI. That was your first taste of abstraction as empowerment.</p>
<p><strong>Confidence Win:</strong></p>
<p>&quot;I can navigate a fabric environment safely using kubectl commands.&quot;</p>
<hr>
<h3>Module 1.2 Recap: How Hedgehog Works</h3>
<p><strong>What You Learned:</strong></p>
<p>The <strong>GitOps workflow</strong> became clear: Git is the source of truth, ArgoCD deploys changes, the Fabric Controller orchestrates configuration, Agent CRDs act as a bridge, and Switch Agents execute on each switch.</p>
<p>You were introduced to <strong>three operational interfaces</strong>:</p>
<ul>
<li><strong>Gitea</strong>: Write and configure (edit YAML, commit changes)</li>
<li><strong>ArgoCD</strong>: Deploy and observe (sync status, deployment progress)</li>
<li><strong>Grafana</strong>: Monitor and validate (dashboards, metrics, trends)</li>
</ul>
<p>You created <strong>your first VPC</strong> (<code>myfirst-vpc</code>) using the browser-based GitOps workflow.</p>
<p><strong>Key Moment:</strong></p>
<p>You committed a VPC configuration to Git, watched ArgoCD sync it, validated with kubectl, and observed it in Grafana—all without SSHing to a single switch. The abstraction clicked.</p>
<p><strong>Confidence Win:</strong></p>
<p>&quot;I can create a VPC using the GitOps workflow and observe the reconciliation happen across all three interfaces.&quot;</p>
<hr>
<h3>Module 1.3 Recap: Mastering the Three Interfaces</h3>
<p><strong>What You Learned:</strong></p>
<p>You mastered <strong>when to use each interface</strong>:</p>
<ul>
<li><strong>kubectl</strong>: Read and inspect current state, check events, troubleshoot issues</li>
<li><strong>Gitea</strong>: Write configurations, audit change history, review diffs</li>
<li><strong>Grafana</strong>: Observe trends over time, monitor health, aggregate metrics</li>
</ul>
<p>You learned that Hedgehog VPCs use <strong>event-based reconciliation</strong>: no error events means successful deployment. You toured <strong>all 6 Grafana dashboards</strong>: Fabric, Platform, Interfaces, Logs, Node Exporter, and Switch CRM.</p>
<p>Most importantly, you learned a <strong>troubleshooting methodology</strong>: Check configuration (Gitea) → Verify deployment (kubectl) → Monitor health (Grafana).</p>
<p><strong>Key Moment:</strong></p>
<p>You navigated all three interfaces fluently, correlating information across tools to understand fabric state. You learned to select the right tool for the job.</p>
<p><strong>Confidence Win:</strong></p>
<p>&quot;I know which interface to use for any given task, and I can troubleshoot by correlating data across kubectl, Gitea, and Grafana.&quot;</p>
<hr>
<h3>How It All Fits Together</h3>
<p><strong>The Full Operator Workflow:</strong></p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│ COURSE 1: What You Now Understand                       │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  1. FABRIC FOUNDATION (Module 1.1)                      │
│     • Pre-installed spine-leaf topology                 │
│     • Switches, servers, connections (Day 1 complete)   │
│     • Kubernetes-native abstractions (CRDs)             │
│                                                          │
│  2. GITOPS CONTROL MODEL (Module 1.2)                   │
│     • Git → ArgoCD → Fabric Controller → Agents         │
│     • Declarative desired state (you declare intent)    │
│     • Automatic reconciliation (system does the work)   │
│                                                          │
│  3. THREE OPERATIONAL INTERFACES (Module 1.3)           │
│     • Gitea: Write/configure/audit                      │
│     • kubectl: Read/inspect/troubleshoot                │
│     • Grafana: Observe/monitor/trend                    │
│                                                          │
│  RESULT: You can operate a Hedgehog Fabric confidently  │
│                                                          │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>A Day in Your Life as a Fabric Operator:</strong></p>
<ol>
<li>Check Grafana Fabric Dashboard (morning health check)</li>
<li>Provision new VPC via Gitea (project requirement)</li>
<li>Watch ArgoCD sync the change (verify deployment)</li>
<li>Use kubectl to validate reconciliation (check events)</li>
<li>Monitor Grafana Interfaces Dashboard (confirm VLANs applied)</li>
<li>Move on to next task, confident the fabric handled the details</li>
</ol>
<p><strong>You Now Operate Like a Hyperscaler:</strong></p>
<ul>
<li>Infrastructure as code (Git)</li>
<li>Automated reconciliation (controllers)</li>
<li>Multi-layer observability (events, metrics, logs)</li>
<li>Confidence through abstraction (no manual switch config)</li>
</ul>
<hr>
<h2>Knowledge Check</h2>
<p>These questions help reinforce key concepts. They&#39;re not graded—they&#39;re for your own confidence check.</p>
<h3>Question 1: Fabric Foundation</h3>
<p><strong>Scenario:</strong> Your manager asks, &quot;What&#39;s a Fabric Operator&#39;s primary responsibility?&quot;</p>
<p>Which answer best describes your role?</p>
<ul>
<li>A) Design network topologies and select switch hardware</li>
<li>B) Provision VPCs, validate connectivity, and monitor fabric health</li>
<li>C) Manually configure BGP on switches</li>
<li>D) Install and cable physical switches</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Provision VPCs, validate connectivity, and monitor fabric health</p>
<p><strong>Explanation:</strong></p>
<p>As a Fabric Operator, you manage Day 2 operations on pre-installed infrastructure. The physical fabric (switches, cabling) is already deployed. Your focus is on creating and managing virtual network resources (VPCs), ensuring connectivity works, and monitoring fabric health.</p>
<ul>
<li><strong>A is incorrect:</strong> Design is typically done during Day 0/1 (before operator role)</li>
<li><strong>C is incorrect:</strong> You don&#39;t manually configure switches—controllers handle that</li>
<li><strong>D is incorrect:</strong> Physical installation is Day 0 work, not Day 2 operations</li>
</ul>
<p><strong>Module 1.1 Reference:</strong> Fabric Operator role definition, Day 2 operations focus</p>
</details>

<hr>
<h3>Question 2: GitOps Workflow</h3>
<p><strong>Scenario:</strong> You need to create a new VPC called <code>dev-network</code>. What&#39;s the correct order of operations?</p>
<ul>
<li>A) kubectl apply → ArgoCD sync → Git commit → Grafana check</li>
<li>B) Git commit (Gitea) → ArgoCD sync → kubectl validate → Grafana monitor</li>
<li>C) Grafana monitor → Git commit → kubectl apply → ArgoCD sync</li>
<li>D) ArgoCD sync → kubectl validate → Git commit → Grafana check</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Git commit (Gitea) → ArgoCD sync → kubectl validate → Grafana monitor</p>
<p><strong>Explanation:</strong></p>
<p>The GitOps workflow follows this sequence:</p>
<ol>
<li><strong>Git commit (Gitea):</strong> Create VPC YAML file in the <code>vpcs/</code> directory, commit to repository (source of truth)</li>
<li><strong>ArgoCD sync:</strong> ArgoCD detects the Git change and deploys the VPC CRD to the cluster</li>
<li><strong>kubectl validate:</strong> Verify VPC was created, check events for reconciliation status</li>
<li><strong>Grafana monitor:</strong> Observe fabric metrics to confirm VPC is operational</li>
</ol>
<p><strong>Why this order matters:</strong></p>
<ul>
<li>Git must be first (source of truth in GitOps)</li>
<li>ArgoCD reacts to Git changes (automation)</li>
<li>kubectl validates what ArgoCD deployed (verification)</li>
<li>Grafana monitors runtime behavior (observability)</li>
</ul>
<p>Other options break the GitOps pattern or have incorrect sequencing.</p>
<p><strong>Module 1.2 Reference:</strong> GitOps control model, five-actor flow</p>
</details>

<hr>
<h3>Question 3: Interface Selection for Troubleshooting</h3>
<p><strong>Scenario:</strong> You suspect a VPC isn&#39;t working correctly. Which interface should you check FIRST to understand the problem?</p>
<ul>
<li>A) Gitea (check VPC configuration)</li>
<li>B) Grafana (check switch health)</li>
<li>C) kubectl (check VPC events for errors)</li>
<li>D) ArgoCD (check sync status)</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> C) kubectl (check VPC events for errors)</p>
<p><strong>Explanation:</strong></p>
<p>When troubleshooting a non-working VPC, start with <code>kubectl describe vpc &lt;name&gt;</code> to check events. Hedgehog uses event-based reconciliation for VPCs—error events immediately tell you what&#39;s wrong (e.g., &quot;Subnet overlaps with existing VPC&quot;).</p>
<p><strong>After checking events:</strong></p>
<ul>
<li>If events show a <strong>configuration error</strong> → Fix in Gitea (go back to source)</li>
<li>If events show <strong>no errors but VPC still not working</strong> → Check Grafana for switch health</li>
<li>If <strong>VPC doesn&#39;t exist</strong> → Check ArgoCD sync status</li>
</ul>
<p><strong>Why kubectl first:</strong>
Starting with kubectl events gives you the most direct troubleshooting information. Events tell you exactly what failed during reconciliation.</p>
<p><strong>Module 1.3 Reference:</strong> Troubleshooting methodology, event-based reconciliation</p>
</details>

<hr>
<h3>Question 4: Three Interfaces - Decision Making</h3>
<p><strong>Match each task to the appropriate interface:</strong></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Interface</th>
</tr>
</thead>
<tbody><tr>
<td>1. View historical CPU usage trends for spine-01 over 24 hours</td>
<td>?</td>
</tr>
<tr>
<td>2. See who changed the <code>prod-vpc</code> configuration last week</td>
<td>?</td>
</tr>
<tr>
<td>3. Check if a VPC has reconciliation errors</td>
<td>?</td>
</tr>
<tr>
<td>4. Create a new VPC with subnet 10.20.30.0/24</td>
<td>?</td>
</tr>
</tbody></table>
<details>
<summary>Answers & Explanation</summary>

<p><strong>Answers:</strong></p>
<ol>
<li><strong>Grafana</strong> - Time-series metrics and trends (Node Exporter Dashboard shows CPU over time)</li>
<li><strong>Gitea</strong> - Audit trail / commit history (shows author, timestamp, and what changed)</li>
<li><strong>kubectl</strong> - Current state and events (<code>kubectl describe vpc &lt;name&gt;</code> shows events)</li>
<li><strong>Gitea</strong> - Write configuration (create YAML file in <code>vpcs/</code>, commit to Git)</li>
</ol>
<p><strong>Key Principle:</strong></p>
<ul>
<li><strong>kubectl = Read</strong> (inspect current state, troubleshoot)</li>
<li><strong>Gitea = Write</strong> (configure, audit history)</li>
<li><strong>Grafana = Observe</strong> (trends, metrics, monitoring)</li>
</ul>
<p><strong>Why these choices:</strong></p>
<ol>
<li>Grafana stores time-series data (kubectl only shows current snapshot)</li>
<li>Gitea tracks all Git commits with author and timestamp</li>
<li>kubectl events show reconciliation results in real-time</li>
<li>GitOps workflow requires Git as source of truth</li>
</ol>
<p><strong>Module 1.3 Reference:</strong> Interface decision matrix, tool selection framework</p>
</details>

<hr>
<h3>Knowledge Check Summary</h3>
<p><strong>How did you do?</strong></p>
<ul>
<li><strong>4/4 correct</strong>: Excellent! You&#39;ve mastered the foundational concepts.</li>
<li><strong>3/4 correct</strong>: Strong understanding. Review any questions you missed.</li>
<li><strong>2/4 or fewer</strong>: Revisit Modules 1.1-1.3 to reinforce concepts before continuing.</li>
</ul>
<p><strong>Remember:</strong> These are for your benefit, not graded assessment. Course 2 builds on these foundations, so make sure you feel confident before moving forward.</p>
<hr>
<h2>Forward Map to Course 2</h2>
<h3>Course 1 vs Course 2: What Changes?</h3>
<p><strong>Course 1 (Where You Are Now):</strong></p>
<ul>
<li>✅ Understood how Hedgehog Fabric works</li>
<li>✅ Learned the GitOps workflow</li>
<li>✅ Mastered the three operational interfaces</li>
<li>✅ Created a simple VPC (<code>myfirst-vpc</code>)</li>
<li>✅ Observed fabric health in Grafana</li>
</ul>
<p><strong>Key characteristic:</strong> <strong>Exploration and understanding</strong> (mostly read-only focus + one simple VPC)</p>
<hr>
<p><strong>Course 2 (Where You&#39;re Going):</strong></p>
<ul>
<li>🎯 Design and create VPCs from scratch (subnet planning, VLAN allocation)</li>
<li>🎯 Attach servers to VPCs (VPCAttachment CRD)</li>
<li>🎯 Validate end-to-end connectivity (testing workflows)</li>
<li>🎯 Decommission resources safely (cleanup procedures)</li>
<li>🎯 Handle real-world provisioning scenarios</li>
</ul>
<p><strong>Key characteristic:</strong> <strong>Hands-on provisioning operations</strong> (creating, modifying, validating, deleting)</p>
<hr>
<h3>Preview: Course 2 Modules</h3>
<h4>Module 2.1: Define VPC Network</h4>
<p><strong>What You&#39;ll Learn:</strong></p>
<ul>
<li>VPC design considerations (subnet sizing, IP allocation strategies)</li>
<li>VLAN selection from namespaces (avoiding conflicts)</li>
<li>Multi-subnet VPCs (multiple L2 domains within one VPC)</li>
<li>DHCP configuration options (ranges, custom DNS, reservations)</li>
<li>IPv4/VLAN namespace concepts and constraints</li>
</ul>
<p><strong>Hands-On:</strong></p>
<ul>
<li>Create a VPC with multiple subnets</li>
<li>Configure DHCP with custom ranges and options</li>
<li>Plan IP allocation for a multi-tenant scenario</li>
</ul>
<p><strong>What&#39;s New:</strong></p>
<p>You&#39;ll make <strong>design decisions</strong> (not just follow templates). Which subnet size? Which VLAN? These choices have operational consequences. Module 2.1 teaches you to think through requirements before creating VPCs.</p>
<hr>
<h4>Module 2.2: Attach Servers to VPC</h4>
<p><strong>What You&#39;ll Learn:</strong></p>
<ul>
<li>VPCAttachment CRD (binding servers to VPC subnets)</li>
<li>Connection types (MCLAG, ESLAG, unbundled, bundled)</li>
<li>Subnet selection for attachments</li>
<li>Server inventory review (which servers are available?)</li>
<li>Native vs tagged VLAN configuration</li>
</ul>
<p><strong>Hands-On:</strong></p>
<ul>
<li>Attach a server to your VPC</li>
<li>Verify switch configuration (VLAN on correct ports)</li>
<li>Observe VPCAttachment reconciliation in Grafana</li>
<li>Troubleshoot attachment failures</li>
</ul>
<p><strong>What&#39;s New:</strong></p>
<p>You&#39;ll connect <strong>virtual networks (VPCs) to physical infrastructure (servers)</strong>. This bridges abstraction layers. Understanding how VPCAttachments translate into switch port configurations is key to troubleshooting connectivity.</p>
<hr>
<h4>Module 2.3: Connectivity Validation</h4>
<p><strong>What You&#39;ll Learn:</strong></p>
<ul>
<li>End-to-end connectivity testing workflows</li>
<li>Using kubectl to inspect Agent CRD interface states</li>
<li>Reading Grafana metrics to confirm traffic flow</li>
<li>Troubleshooting common connectivity issues (DHCP, reachability)</li>
<li>Understanding DHCP relay (switches forwarding DHCP requests)</li>
</ul>
<p><strong>Hands-On:</strong></p>
<ul>
<li>Validate server can obtain DHCP lease from VPC</li>
<li>Test server-to-server connectivity within VPC</li>
<li>Use Grafana Interfaces Dashboard to confirm VLANs applied</li>
<li>Diagnose and fix a misconfigured attachment</li>
</ul>
<p><strong>What&#39;s New:</strong></p>
<p>You&#39;ll <strong>validate that your configurations actually work</strong> (not just trust reconciliation). Testing builds operational confidence. You&#39;ll learn systematic validation approaches that apply to any VPC deployment.</p>
<hr>
<h4>Module 2.4: Decommission &amp; Cleanup</h4>
<p><strong>What You&#39;ll Learn:</strong></p>
<ul>
<li>Safe VPC deletion order (attachments first, then VPC)</li>
<li>Verifying no active servers before deletion</li>
<li>GitOps deletion workflow (delete YAML files from Git)</li>
<li>Cleanup validation (ensure switches removed configuration)</li>
<li>When to archive vs delete configurations</li>
</ul>
<p><strong>Hands-On:</strong></p>
<ul>
<li>Safely decommission a VPC</li>
<li>Remove VPCAttachments first (proper dependency order)</li>
<li>Verify cleanup in kubectl and Grafana</li>
<li>Understand orphaned resource prevention</li>
</ul>
<p><strong>What&#39;s New:</strong></p>
<p>You&#39;ll learn to <strong>undo your work safely</strong>. Decommissioning is a critical Day 2 skill often overlooked in training. Understanding deletion order prevents operational issues like orphaned VLANs or stranded switch configurations.</p>
<hr>
<h3>Course 2: What You&#39;ll Be Able to Do</h3>
<p>By the end of Course 2, you&#39;ll be able to:</p>
<ol>
<li><strong>Design VPCs</strong> tailored to specific workload requirements</li>
<li><strong>Provision VPCs</strong> using GitOps workflow confidently and independently</li>
<li><strong>Attach servers</strong> to VPCs with correct connection types</li>
<li><strong>Validate connectivity</strong> end-to-end (DHCP, ping, traffic)</li>
<li><strong>Troubleshoot</strong> provisioning issues using all three interfaces systematically</li>
<li><strong>Decommission</strong> VPCs and attachments safely without leaving orphaned resources</li>
<li><strong>Operate independently</strong> for common Day 2 provisioning tasks</li>
</ol>
<p><strong>This is when you become productive as a Fabric Operator.</strong></p>
<p>Course 1 gave you the knowledge. Course 2 gives you the skills.</p>
<hr>
<h2>Motivation &amp; Next Steps</h2>
<h3>You&#39;re Ready</h3>
<p><strong>Course 1 gave you the foundation:</strong></p>
<ul>
<li>You understand the fabric architecture and GitOps control model</li>
<li>You know the GitOps workflow and how reconciliation works</li>
<li>You&#39;ve mastered the three operational interfaces</li>
<li>You&#39;ve created a VPC and observed reconciliation</li>
</ul>
<p><strong>Course 2 will give you the skills:</strong></p>
<ul>
<li>VPC design and provisioning for real workloads</li>
<li>Server connectivity and validation</li>
<li>Systematic validation and troubleshooting</li>
<li>Lifecycle management (create → use → decommission)</li>
</ul>
<p><strong>The difference?</strong></p>
<p>Course 1 was about <strong>understanding how Hedgehog works</strong>. Course 2 is about <strong>getting work done as a Fabric Operator</strong>.</p>
<hr>
<h3>Confidence Statement</h3>
<p>You&#39;ve completed Course 1, which means:</p>
<p>✅ You can navigate Hedgehog Fabric using kubectl
✅ You understand GitOps workflow and why it matters
✅ You know which interface to use for any task
✅ You&#39;ve successfully created a VPC using the three interfaces
✅ You can troubleshoot by correlating data across kubectl, Gitea, and Grafana</p>
<p><strong>You&#39;re not an expert yet—that&#39;s not the goal.</strong> You&#39;re a <strong>confident beginner</strong> ready to tackle hands-on provisioning operations. That&#39;s exactly where you should be.</p>
<hr>
<h3>What Happens Next</h3>
<p><strong>Immediate Next Step:</strong></p>
<p>Begin <strong>Course 2, Module 2.1: Define VPC Network</strong></p>
<p>You&#39;ll design your first production-style VPC from scratch, making real design decisions about subnets and VLANs. Everything you learned in Course 1 will be applied.</p>
<p><strong>Mindset for Course 2:</strong></p>
<ul>
<li><strong>Experiment:</strong> You&#39;re working in a lab environment—try things, make mistakes, learn from them</li>
<li><strong>Validate:</strong> Use all three interfaces to confirm your work</li>
<li><strong>Troubleshoot:</strong> When things don&#39;t work (and they won&#39;t always), you have the tools to diagnose</li>
<li><strong>Escalate:</strong> If you&#39;re stuck, that&#39;s normal—support is part of learning, not a sign of weakness</li>
</ul>
<p><strong>Remember the Learning Philosophy:</strong></p>
<ul>
<li>Confidence before comprehensiveness (master core operations first)</li>
<li>Focus on what matters most (common tasks, not edge cases)</li>
<li>Learn by doing, not watching (hands-on every module)</li>
<li>Support is strength, not weakness (escalate when appropriate)</li>
</ul>
<hr>
<h3>Final Encouragement</h3>
<p>Three modules ago, this might have seemed daunting:</p>
<blockquote>
<p>&quot;Manage a data center network using GitOps and Kubernetes? That&#39;s not for me.&quot;</p>
</blockquote>
<p>Now you know it&#39;s not just possible—<strong>you&#39;ve already done it.</strong></p>
<p>You&#39;ve explored a fabric, created a VPC, monitored switches, and troubleshot issues using kubectl, Gitea, and Grafana. The foundation is solid.</p>
<p>The leap from Course 1 to Course 2 is smaller than it feels. You have the foundation. Course 2 adds depth and practice through repetition and real-world scenarios.</p>
<p><strong>Trust your preparation. You&#39;ve earned this confidence.</strong></p>
<p>Welcome to Course 2. Let&#39;s get to work.</p>
<hr>
<h2>Reference</h2>
<h3>Course 1 Module Links</h3>
<p><strong>Module 1.1: Welcome to Fabric Operations</strong></p>
<ul>
<li>Fabric Operator role definition</li>
<li>Kubernetes-native management overview</li>
<li>kubectl basics for fabric exploration</li>
</ul>
<p><strong>Module 1.2: How Hedgehog Works</strong></p>
<ul>
<li>GitOps workflow (Git → ArgoCD → Fabric Controller → Agents)</li>
<li>Three interfaces introduction (Gitea, ArgoCD, Grafana)</li>
<li>VPC creation walkthrough</li>
</ul>
<p><strong>Module 1.3: Mastering the Three Interfaces</strong></p>
<ul>
<li>Interface decision matrix (when to use which tool)</li>
<li>Event-based reconciliation model</li>
<li>All 6 Grafana dashboards</li>
<li>Troubleshooting methodology</li>
</ul>
<h3>Key Concepts Reference</h3>
<p><strong>GitOps Workflow:</strong></p>
<pre><code>Git (source of truth)
  ↓
ArgoCD (deployment automation)
  ↓
Fabric Controller (orchestration)
  ↓
Agent CRDs (bridge to switches)
  ↓
Switch Agents (execution on switches)
</code></pre>
<p><strong>Three-Interface Model:</strong></p>
<table>
<thead>
<tr>
<th>Interface</th>
<th>Role</th>
<th>When to Use</th>
</tr>
</thead>
<tbody><tr>
<td><strong>kubectl</strong></td>
<td>Read/Inspect</td>
<td>Check state, view events, troubleshoot</td>
</tr>
<tr>
<td><strong>Gitea</strong></td>
<td>Write/Audit</td>
<td>Create configs, review history</td>
</tr>
<tr>
<td><strong>Grafana</strong></td>
<td>Observe/Monitor</td>
<td>View trends, monitor health</td>
</tr>
</tbody></table>
<p><strong>Troubleshooting Flow:</strong></p>
<ol>
<li><strong>Gitea</strong> - Verify configuration is correct</li>
<li><strong>kubectl</strong> - Check deployment status and events</li>
<li><strong>Grafana</strong> - Monitor runtime health</li>
</ol>
<h3>Grafana Dashboards Quick Reference</h3>
<ol>
<li><strong>Fabric Dashboard</strong> - Overall fabric health (daily check)</li>
<li><strong>Platform Dashboard</strong> - Control plane health (Fabricator, K8s API)</li>
<li><strong>Interfaces Dashboard</strong> - Port status and traffic</li>
<li><strong>Logs Dashboard</strong> - Aggregated syslog</li>
<li><strong>Node Exporter Dashboard</strong> - Switch hardware metrics</li>
<li><strong>Switch CRM Dashboard</strong> - ASIC capacity monitoring</li>
</ol>
<h3>Course 2 Preview</h3>
<p><strong>Module 2.1:</strong> Define VPC Network (VPC design considerations)
<strong>Module 2.2:</strong> Attach Servers to VPC (VPCAttachment CRD)
<strong>Module 2.3:</strong> Connectivity Validation (testing workflows)
<strong>Module 2.4:</strong> Decommission &amp; Cleanup (safe deletion)</p>
<h3>Related Documentation</h3>
<ul>
<li><a href=""../../../network-like-hyperscaler/hedgehogLearningPhilosophy.md"">Hedgehog Learning Philosophy</a></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md"">CRD Reference</a></li>
<li><a href=""../../../network-like-hyperscaler/research/WORKFLOWS.md"">Workflow Reference</a></li>
<li><a href=""../../../network-like-hyperscaler/MODULE_DEPENDENCY_GRAPH.md"">Module Dependency Graph</a></li>
<li><a href=""../../../network-like-hyperscaler/PROJECT_PLAN.md"">Project Plan</a></li>
</ul>
<hr>
<p><strong>Course 1 Complete!</strong> 🎉 You&#39;re ready to continue to Course 2, Module 2.1: Define VPC Network.</p>
",104,10,,,"hedgehog,recap,readiness,learning-path,networking"
197744345017,How Hedgehog Works: The Control Model,fabric-operations-how-it-works,[object Object],"Understand Hedgehog's declarative control loop, create your first VPC, and trace reconciliation from CRDs to switches.","<p><strong>Course:</strong> Course 1 - Foundations &amp; Interfaces
<strong>Duration:</strong> 15 minutes
<strong>Prerequisites:</strong> Module 1.1: Welcome to Fabric Operations</p>
<hr>
<h2>Introduction</h2>
<h3>The Magic Explained</h3>
<p>In Module 1.1, you used kubectl to explore the fabric. You saw switches, servers, and connections—all represented as Kubernetes resources. But how does typing <code>kubectl apply -f vpc.yaml</code> actually configure switches?</p>
<p>It&#39;s not magic. It&#39;s <strong>declarative infrastructure</strong> with a well-designed control loop. Understanding this pattern is the key to operating confidently: you&#39;ll know what&#39;s happening behind the scenes, when things are working, and how to detect when they&#39;re not.</p>
<h3>From Intent to Reality</h3>
<p><strong>Traditional networking:</strong> You configure each switch manually. If a switch reboots, you reconfigure it. If you want consistency, you write scripts.</p>
<p><strong>Hedgehog approach:</strong> You declare your intent once. The fabric continuously reconciles actual state to match desired state. If a switch reboots, it automatically rejoins and reconfigures itself.</p>
<p>This is how hyperscalers run networks at scale—and now you can too.</p>
<h3>What You&#39;ll Learn</h3>
<p>This module demystifies the control model:</p>
<ul>
<li>How CRDs represent network intent</li>
<li>How controllers watch for changes and take action</li>
<li>How switch agents apply configurations</li>
<li>How to observe this process in real-time</li>
</ul>
<p>You&#39;ll create your first VPC and watch the reconciliation happen step by step.</p>
<h3>Learning Objectives</h3>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Explain the CRD reconciliation pattern</strong> - Describe how desired state (kubectl apply) becomes actual state (switch configuration)</li>
<li><strong>Identify the key components</strong> - Recognize Fabric Controller, Agent CRDs, and switch agents in the control flow</li>
<li><strong>Observe reconciliation in action</strong> - Watch Kubernetes events as a VPC is created and configured</li>
<li><strong>Interpret Agent CRD status</strong> - Use Agent CRD to view switch operational state and reconciliation progress</li>
<li><strong>Understand abstraction boundaries</strong> - Explain what the operator manages (CRDs) vs. what the system manages (switch configs)</li>
</ol>
<hr>
<h2>Core Concepts</h2>
<h3>Concept 1: Desired State vs. Current State</h3>
<p>Every declarative system has two views of the world:</p>
<p><strong>Desired State:</strong> What you want (defined in CRDs)</p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
  name: my-vpc
spec:
  subnets:
    default:
      subnet: 10.10.1.0/24
      vlan: 1001
</code></pre>
<p><strong>Current State:</strong> What exists right now (switch configurations, actual VLANs, routes)</p>
<p><strong>The Control Loop:</strong> Continuously measures the gap and takes actions to close it.</p>
<pre><code>┌─────────────┐
│ Desired     │  &quot;I want VPC with VLAN 1001&quot;
│ State       │
│ (CRD)       │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ Controller  │  &quot;Compare desired vs. current&quot;
│ Loop        │  &quot;Generate switch configs&quot;
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ Current     │  &quot;VLAN 1001 is configured on switches&quot;
│ State       │
│ (Switches)  │
└─────────────┘
</code></pre>
<p>If current ≠ desired, the controller takes action. If current = desired, nothing happens (idempotent).</p>
<h3>Concept 2: The Hedgehog Control Plane Components</h3>
<p><strong>Three key actors:</strong></p>
<p><strong>1. Fabric Controller (Control Plane)</strong></p>
<ul>
<li>Runs in Kubernetes (fab namespace)</li>
<li>Watches for CRD changes (VPC, VPCAttachment, etc.)</li>
<li>Computes desired switch configurations</li>
<li>Generates Agent CRD specs (instructions for switches)</li>
<li>Doesn&#39;t directly touch switches—works through Agents</li>
</ul>
<p><strong>2. Agent CRD (Bridge)</strong></p>
<ul>
<li>One Agent CRD per switch (lives in Kubernetes)</li>
<li>Contains two key sections:<ul>
<li><code>spec</code>: Desired configuration (written by Fabric Controller)</li>
<li><code>status</code>: Current state (reported by switch agent)</li>
</ul>
</li>
<li>The &quot;contract&quot; between controller and switch</li>
</ul>
<p><strong>3. Switch Agent (On Each Switch)</strong></p>
<ul>
<li>Runs on SONiC switches (not in Kubernetes)</li>
<li>Watches its Agent CRD for spec changes</li>
<li>Applies configurations via gNMI (SONiC config interface)</li>
<li>Reports status back to Agent CRD (NOS version, interface states, BGP neighbors, etc.)</li>
</ul>
<p><strong>Data Flow:</strong></p>
<pre><code>You                  Fabric Controller       Agent CRD         Switch Agent        SONiC Switch
│                           │                    │                  │                    │
├─ kubectl apply vpc.yaml ─&gt;│                    │                  │                    │
│                           │                    │                  │                    │
│                           ├─ Watch VPC CRD     │                  │                    │
│                           ├─ Compute config    │                  │                    │
│                           │                    │                  │                    │
│                           ├─ Update Agent spec ───&gt;               │                    │
│                           │                    │                  │                    │
│                           │                    │&lt;── Watch spec ───┤                    │
│                           │                    │                  │                    │
│                           │                    │                  ├─ Apply via gNMI ──&gt;│
│                           │                    │                  │                    │
│                           │                    │                  │&lt;─── Config ACK ────┤
│                           │                    │                  │                    │
│                           │                    │&lt;── Report status─┤                    │
│                           │                    │                  │                    │
│&lt;─── kubectl get vpc ──────┤                    │                  │                    │
│     (shows Ready)         │                    │                  │                    │
</code></pre>
<h3>Concept 3: Reconciliation and Events</h3>
<p>When you apply a CRD, a series of events occurs:</p>
<p><strong>Example: Creating a VPC</strong></p>
<ol>
<li><strong>Created:</strong> VPC object appears in Kubernetes</li>
<li><strong>Reconciling:</strong> Fabric Controller picks up the change</li>
<li><strong>Computing:</strong> Controller calculates which switches need configuration</li>
<li><strong>Agent Update:</strong> Controller writes specs to relevant Agent CRDs</li>
<li><strong>Applying:</strong> Switch agents apply configurations via gNMI</li>
<li><strong>Status Report:</strong> Agents report success back to Agent CRD status</li>
<li><strong>Ready:</strong> VPC status becomes Ready (when all switches configured)</li>
</ol>
<p>Each step generates <strong>Kubernetes events</strong> that you can observe:</p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=my-vpc

# Example events:
# Normal  Created         VPC created
# Normal  Reconciling     Fabric controller processing
# Normal  AgentUpdated    Agent specs generated
# Normal  Ready           VPC reconciliation complete
</code></pre>
<p>Events are your window into the control loop.</p>
<h3>Concept 4: Why This Model Matters for Operators</h3>
<p><strong>Benefits:</strong></p>
<ol>
<li><strong>Self-healing:</strong> Switch reboots? Agent reapplies config automatically</li>
<li><strong>Declarative:</strong> Describe what you want, not how to configure it</li>
<li><strong>Idempotent:</strong> Apply the same YAML repeatedly—safe and predictable</li>
<li><strong>Observable:</strong> Events and Agent status show exactly what&#39;s happening</li>
<li><strong>Git-friendly:</strong> CRDs are YAML files—version control, code review, GitOps</li>
</ol>
<p><strong>What You Control:</strong></p>
<ul>
<li>VPC definitions (subnets, VLANs, DHCP)</li>
<li>VPCAttachments (which servers connect to which VPCs)</li>
<li>VPCPeering (inter-VPC connectivity)</li>
<li>External connectivity (border leaf peering)</li>
</ul>
<p><strong>What the System Controls:</strong></p>
<ul>
<li>Individual switch configurations (SONiC config DB)</li>
<li>BGP peering between switches</li>
<li>VXLAN tunnel setup</li>
<li>Route distribution</li>
</ul>
<p>You work at a higher abstraction level—the system handles the complexity. This isn&#39;t a limitation; it&#39;s <strong>empowerment</strong>. You&#39;re freed from managing thousands of configuration lines across dozens of switches, allowing you to focus on network intent rather than implementation details.</p>
<h3>Concept 5: The Agent CRD as Source of Truth</h3>
<p>Remember from Module 1.1: Most Hedgehog CRDs have minimal status fields. The real operational state lives in <strong>Agent CRDs</strong>.</p>
<p><strong>Agent CRD contains:</strong></p>
<ul>
<li>NOS version (SONiC version)</li>
<li>Platform info (switch model)</li>
<li>Interface states (up/down, speed)</li>
<li>BGP neighbor status (established, down)</li>
<li>ASIC resource usage (routes, nexthops)</li>
<li>System health metrics</li>
</ul>
<p><strong>Why this matters:</strong></p>
<ul>
<li>VPC says &quot;Ready&quot; but you want to see <em>which switches</em> configured it → Check Agent CRDs</li>
<li>Troubleshooting connectivity → Check Agent CRD for interface status</li>
<li>Monitoring fabric health → Watch Agent CRD status fields</li>
</ul>
<p>Agent CRDs are your deep observability layer.</p>
<hr>
<h2>Hands-On Lab</h2>
<h3>Lab Title: Create a VPC and Observe Reconciliation</h3>
<p><strong>Overview:</strong></p>
<p>You&#39;ll create your first VPC, watch Kubernetes events as it reconciles, and inspect Agent CRDs to see how switches report their state. This is your first time <em>creating</em> something (not just exploring), and you&#39;ll see the control loop in action.</p>
<p><strong>Environment:</strong></p>
<ul>
<li>Same Hedgehog vlab from Module 1.1</li>
<li>kubectl access to create VPCs</li>
<li>Events visible during active reconciliation</li>
</ul>
<hr>
<h3>Task 1: Prepare to Observe</h3>
<p><strong>Objective:</strong> Open a second terminal to watch events in real-time (optional but recommended)</p>
<p><strong>Steps (if using two terminals):</strong></p>
<pre><code class=""language-bash""># Terminal 1: You&#39;ll create the VPC here
# (this is your main terminal)

# Terminal 2 (optional): Watch events continuously
kubectl get events --watch --field-selector involvedObject.kind=VPC

# This will stream events as they happen
# You&#39;ll see events appear as you create the VPC in Terminal 1
</code></pre>
<p><strong>Note:</strong> If you only have one terminal, you can view events after creating the VPC—it works fine, just less dramatic!</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Have terminal(s) ready</li>
<li>✅ kubectl access verified</li>
</ul>
<hr>
<h3>Task 2: Create Your First VPC</h3>
<p><strong>Objective:</strong> Create a simple VPC and apply it to the fabric</p>
<p><strong>Steps:</strong></p>
<p><strong>Create VPC YAML:</strong></p>
<pre><code class=""language-bash"">cat &gt; test-vpc.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
  name: test-vpc
  namespace: default
spec:
  ipv4Namespace: default
  vlanNamespace: default
  subnets:
    default:
      subnet: 10.99.1.0/24
      gateway: 10.99.1.1
      vlan: 1999
      dhcp:
        enable: true
        range:
          start: 10.99.1.10
          end: 10.99.1.99
EOF
</code></pre>
<p><strong>Apply the VPC:</strong></p>
<pre><code class=""language-bash"">kubectl apply -f test-vpc.yaml

# Expected output:
# vpc.vpc.githedgehog.com/test-vpc created
</code></pre>
<p><strong>Immediately check events (Terminal 1):</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=test-vpc --sort-by=&#39;.lastTimestamp&#39;

# You should see events like:
# Normal  Created         VPC test-vpc created
# Normal  Reconciling     Processing VPC configuration
# (more events as reconciliation progresses)
</code></pre>
<p><strong>Validation:</strong></p>
<pre><code class=""language-bash""># Check VPC status
kubectl get vpc test-vpc

# Expected output:
# NAME       AGE
# test-vpc   30s

# Get full VPC details
kubectl get vpc test-vpc -o yaml | grep -A 5 status:

# Status may be empty {} initially, or show conditions
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC created without errors</li>
<li>✅ Events visible showing reconciliation</li>
<li>✅ VPC object exists in cluster</li>
</ul>
<hr>
<h3>Task 3: Observe Agent CRD Updates</h3>
<p><strong>Objective:</strong> See how the Fabric Controller updates Agent CRDs with configuration</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># View all Agent CRDs (one per switch)
kubectl get agents -n fab

# Expected output:
# NAME       AGE
# leaf-01    Xh
# leaf-02    Xh
# ... (7 agents total)
</code></pre>
<p><strong>Inspect an Agent to see configuration:</strong></p>
<pre><code class=""language-bash""># Look at leaf-01&#39;s Agent CRD
kubectl get agent leaf-01 -n fab -o yaml | head -50

# Look for these sections:
# - spec: Contains desired configuration (from Fabric Controller)
# - status: Contains switch operational state (from switch agent)
</code></pre>
<p><strong>Check Agent status for VPC-related info:</strong></p>
<pre><code class=""language-bash""># View Agent status (switch reported state)
kubectl get agent leaf-01 -n fab -o yaml | grep -A 20 &quot;status:&quot;

# Look for:
# - nos.version: SONiC version
# - platform: Switch platform info
# - interfaces: Interface states
# - bgpNeighbors: BGP peering status (if relevant)
</code></pre>
<p><strong>Observation Questions:</strong></p>
<ol>
<li>Can you see the NOS version in the Agent status? _______</li>
<li>Are interfaces listed in the status? _______</li>
<li>What information does the Agent provide that VPC doesn&#39;t? _______</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can view Agent CRDs</li>
<li>✅ Can see spec and status sections</li>
<li>✅ Understand Agent shows switch operational state</li>
</ul>
<hr>
<h3>Task 4: Verify VPC is Ready</h3>
<p><strong>Objective:</strong> Confirm reconciliation completed successfully</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># Check VPC status (wait 30-60 seconds after creation)
kubectl get vpc test-vpc -o yaml | grep -A 10 status:

# Look for status fields (may be minimal or empty)
</code></pre>
<p><strong>Check recent events:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=test-vpc --sort-by=&#39;.lastTimestamp&#39; | tail -10

# Look for &quot;Ready&quot; or &quot;ReconcileSuccess&quot; events
</code></pre>
<p><strong>Describe the VPC:</strong></p>
<pre><code class=""language-bash"">kubectl describe vpc test-vpc

# Look at the Events section at the bottom
# Should show creation, reconciliation, and completion
</code></pre>
<p><strong>Key Insight:</strong></p>
<p>Even if VPC status is minimal, <strong>events tell the story</strong>. You can see:</p>
<ul>
<li>When VPC was created</li>
<li>When controller reconciled it</li>
<li>If any errors occurred</li>
<li>When reconciliation completed</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC shows no error events</li>
<li>✅ Reconciliation events visible</li>
<li>✅ VPC object stable (can be retrieved without errors)</li>
</ul>
<hr>
<h3>Task 5: Optional - Modify and Re-Reconcile</h3>
<p><strong>Objective:</strong> See reconciliation happen again when you change desired state</p>
<p><strong>Steps:</strong></p>
<p><strong>Modify the VPC (add a second subnet):</strong></p>
<pre><code class=""language-bash"">cat &gt; test-vpc-updated.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
  name: test-vpc
  namespace: default
spec:
  ipv4Namespace: default
  vlanNamespace: default
  subnets:
    default:
      subnet: 10.99.1.0/24
      gateway: 10.99.1.1
      vlan: 1999
      dhcp:
        enable: true
        range:
          start: 10.99.1.10
          end: 10.99.1.99
    backend:
      subnet: 10.99.2.0/24
      gateway: 10.99.2.1
      vlan: 1998
EOF
</code></pre>
<p><strong>Apply the update:</strong></p>
<pre><code class=""language-bash"">kubectl apply -f test-vpc-updated.yaml

# Expected output:
# vpc.vpc.githedgehog.com/test-vpc configured
</code></pre>
<p><strong>Watch events:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=test-vpc --sort-by=&#39;.lastTimestamp&#39; | tail -10

# You should see new reconciliation events as the update is applied
</code></pre>
<p><strong>Observe:</strong> The controller detects the change, updates Agent specs, switches reconfigure—all automatically.</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Update applied without errors</li>
<li>✅ New reconciliation events visible</li>
<li>✅ VPC now has two subnets (verify with <code>kubectl get vpc test-vpc -o yaml</code>)</li>
</ul>
<hr>
<h3>Lab Summary</h3>
<p><strong>What you did:</strong></p>
<ul>
<li>✅ Created your first VPC using kubectl apply</li>
<li>✅ Watched Kubernetes events during reconciliation</li>
<li>✅ Inspected Agent CRDs to see switch operational state</li>
<li>✅ Verified VPC reconciliation completed successfully</li>
<li>✅ (Optional) Modified VPC and watched re-reconciliation</li>
</ul>
<p><strong>What you learned:</strong></p>
<ul>
<li>Declarative infrastructure: You declare desired state, the system reconciles</li>
<li>Control loop: Fabric Controller → Agent CRD → Switch Agent → SONiC</li>
<li>Events are your observability window into reconciliation</li>
<li>Agent CRDs contain deep switch operational state</li>
<li>Changes are automatically reconciled (self-healing)</li>
</ul>
<p><strong>Key insight:</strong> You didn&#39;t log into a single switch, yet network configuration happened across the fabric. That&#39;s the power of the abstraction—and why understanding the control model lets you operate confidently.</p>
<hr>
<h2>Wrap-Up</h2>
<h3>Key Takeaways</h3>
<ol>
<li><strong>Desired state (CRD) → Actual state (switches)</strong> via the reconciliation loop</li>
<li><strong>Three actors:</strong> Fabric Controller, Agent CRD, Switch Agent</li>
<li><strong>Events show progress:</strong> Watch reconciliation happen in real-time</li>
<li><strong>Agent CRDs are deep observability:</strong> Switch operational state lives here</li>
<li><strong>Abstraction means power:</strong> You manage intent, the system handles complexity</li>
</ol>
<h3>Real-World Impact</h3>
<p>You now understand:</p>
<ul>
<li>Why kubectl apply is safe (idempotent, declarative)</li>
<li>What &quot;reconciling&quot; means (closing the gap between desired and actual)</li>
<li>Where to look when troubleshooting (events, Agent CRD status)</li>
<li>Why Hedgehog is self-healing (continuous reconciliation)</li>
</ul>
<h3>Preview of Module 1.3</h3>
<p>Next, you&#39;ll go deeper into <strong>kubectl and YAML workflows</strong>: how to write VPC specs, validate before applying, use kubectl efficiently, and follow best practices for managing CRDs. You&#39;ll become fluent in the primary tool you&#39;ll use daily.</p>
<hr>
<h2>Assessment</h2>
<h3>Quiz Questions</h3>
<p><strong>Question 1: Multiple Choice</strong></p>
<p>In the Hedgehog control model, what is the role of the <strong>Fabric Controller</strong>?</p>
<ul>
<li>A) Directly configures switches via SSH</li>
<li>B) Watches CRDs, computes configurations, updates Agent CRD specs</li>
<li>C) Runs on switches and applies configurations</li>
<li>D) Stores VPC definitions in a database</li>
</ul>
<p><strong>Correct Answer:</strong> B</p>
<p><strong>Explanation:</strong></p>
<p>The Fabric Controller runs in Kubernetes, watches for CRD changes (VPCs, VPCAttachments, etc.), computes desired switch configurations, and writes them to Agent CRD specs. It doesn&#39;t directly touch switches (that&#39;s the Switch Agent&#39;s job, C), doesn&#39;t use SSH (A), and CRDs are stored in Kubernetes etcd, not a separate database (D).</p>
<hr>
<p><strong>Question 2: Scenario-Based</strong></p>
<p>You create a VPC with <code>kubectl apply -f vpc.yaml</code>. What is the correct order of operations?</p>
<ol>
<li>Switch Agent applies config via gNMI</li>
<li>Fabric Controller updates Agent CRD spec</li>
<li>VPC CRD created in Kubernetes</li>
<li>Fabric Controller watches VPC CRD and computes config</li>
</ol>
<p><strong>Answer:</strong> 3 → 4 → 2 → 1</p>
<p><strong>Explanation:</strong></p>
<p>Correct flow: VPC CRD created (3), Fabric Controller watches and computes (4), Controller updates Agent spec (2), Switch Agent applies via gNMI (1). This is the reconciliation loop in action.</p>
<hr>
<p><strong>Question 3: True/False</strong></p>
<p>True or False: If a switch reboots, you must manually re-run <code>kubectl apply</code> to reconfigure it.</p>
<p><strong>Answer:</strong> False</p>
<p><strong>Explanation:</strong></p>
<p>The switch agent automatically re-reads its Agent CRD when it comes back online and reapplies all configurations. This is <strong>self-healing</strong>—one of the key benefits of declarative infrastructure. Manual intervention is not required.</p>
<hr>
<p><strong>Question 4: Multiple Choice</strong></p>
<p>Where can you find detailed information about a switch&#39;s BGP neighbors and interface states?</p>
<ul>
<li>A) VPC CRD status field</li>
<li>B) Agent CRD status field</li>
<li>C) Fabric Controller logs only</li>
<li>D) You must SSH into the switch</li>
</ul>
<p><strong>Correct Answer:</strong> B</p>
<p><strong>Explanation:</strong></p>
<p>Agent CRD status contains comprehensive switch operational state reported by the switch agent, including BGP neighbors, interface states, ASIC resources, and NOS version. VPC status (A) is minimal. Controller logs (C) help but aren&#39;t the primary source. SSH (D) is possible but not the operator&#39;s tool—kubectl is.</p>
<hr>
<p><strong>Question 5: Practical</strong></p>
<p>You create a VPC but want to verify reconciliation completed successfully. What kubectl commands would you use? (List 2)</p>
<p><strong>Answer:</strong></p>
<pre><code class=""language-bash""># Option 1: Check events for reconciliation status
kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt; --sort-by=&#39;.lastTimestamp&#39;

# Option 2: Describe VPC to see events in summary
kubectl describe vpc &lt;vpc-name&gt;

# Option 3: Get VPC status (if populated)
kubectl get vpc &lt;vpc-name&gt; -o yaml | grep -A 10 status:
</code></pre>
<p><strong>Rubric:</strong></p>
<ul>
<li>Full credit: Any 2 valid commands that show reconciliation status</li>
<li>Partial credit: 1 valid command</li>
<li>No credit: Commands that don&#39;t reveal reconciliation state</li>
</ul>
<hr>
<h3>Practical Assessment</h3>
<p><strong>Task:</strong> Create a VPC, verify it reconciled successfully, then delete it cleanly.</p>
<p><strong>Steps:</strong></p>
<ol>
<li>Create VPC with <code>kubectl apply -f vpc.yaml</code></li>
<li>Use kubectl to verify reconciliation (events or describe)</li>
<li>Delete VPC with <code>kubectl delete vpc &lt;name&gt;</code></li>
<li>Verify deletion completed (no lingering resources)</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC created without errors</li>
<li>✅ Used appropriate kubectl commands to verify reconciliation</li>
<li>✅ Can explain what events indicate success</li>
<li>✅ Deleted VPC cleanly</li>
<li>✅ Understands reconciliation happened both on create and delete</li>
</ul>
<hr>
<h2>Reference</h2>
<h3>Hedgehog CRDs Used in This Module</h3>
<p><strong>VPC</strong> - Virtual Private Cloud with isolated subnets</p>
<ul>
<li>View: <code>kubectl get vpc</code></li>
<li>Inspect: <code>kubectl describe vpc &lt;name&gt;</code></li>
<li>Apply: <code>kubectl apply -f vpc.yaml</code></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md#vpc"">Full Reference</a></li>
</ul>
<p><strong>Agent</strong> - Switch agent containing detailed operational state</p>
<ul>
<li>View: <code>kubectl get agents -n fab</code></li>
<li>Inspect: <code>kubectl get agent &lt;switch-name&gt; -n fab -o yaml</code></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md#agent"">Full Reference</a></li>
</ul>
<h3>kubectl Commands Reference</h3>
<p><strong>VPC creation:</strong></p>
<pre><code class=""language-bash"">kubectl apply -f vpc.yaml        # Create or update VPC
kubectl get vpc &lt;name&gt;           # View VPC summary
kubectl describe vpc &lt;name&gt;      # View VPC details and events
kubectl get vpc &lt;name&gt; -o yaml   # View full VPC YAML
kubectl delete vpc &lt;name&gt;        # Delete VPC
</code></pre>
<p><strong>Event observation:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=&lt;name&gt;  # Filter events by resource
kubectl get events --watch                                      # Watch events in real-time
kubectl get events --sort-by=&#39;.lastTimestamp&#39;                   # Sort events by time
kubectl describe vpc &lt;name&gt;                                     # View events in context
</code></pre>
<p><strong>Agent CRD inspection:</strong></p>
<pre><code class=""language-bash"">kubectl get agents -n fab                         # List all agents
kubectl get agent &lt;switch-name&gt; -n fab -o yaml    # View agent details
kubectl describe agent &lt;switch-name&gt; -n fab       # View agent summary
</code></pre>
<h3>Workflow Reference</h3>
<p>This module uses <strong>Workflow 1: Create VPC from Scratch</strong> from the workflow reference:</p>
<ul>
<li><a href=""../../../network-like-hyperscaler/research/WORKFLOWS.md#workflow-1-create-vpc-from-scratch"">WORKFLOWS.md - Workflow 1</a></li>
</ul>
<h3>Related Documentation</h3>
<ul>
<li><a href=""../../../network-like-hyperscaler/hedgehogLearningPhilosophy.md"">Hedgehog Learning Philosophy</a></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md"">CRD Reference</a></li>
<li><a href=""../../../network-like-hyperscaler/MODULE_DEPENDENCY_GRAPH.md"">Module Dependency Graph</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> Ready to continue to Module 1.3: Interfaces: kubectl/YAML.</p>
",102,15,,,"hedgehog,control-plane,kubernetes,vpc,automation"
197728932205,Mastering the Three Interfaces,fabric-operations-mastering-interfaces,[object Object],"Master kubectl, GitOps, and Grafana workflows to operate Hedgehog Fabric with confidence across all interfaces.","<p><strong>Course:</strong> Course 1 - Foundations &amp; Interfaces
<strong>Duration:</strong> 15 minutes
<strong>Prerequisites:</strong> Module 1.2: How Hedgehog Works</p>
<hr>
<h2>Introduction</h2>
<h3>The Fabric Operator&#39;s Toolkit</h3>
<p>You&#39;ve created your first VPC and watched the reconciliation magic happen. But in real operations, you&#39;ll face different scenarios:</p>
<ul>
<li><strong>Troubleshooting:</strong> &quot;This VPC isn&#39;t working—what&#39;s wrong?&quot;</li>
<li><strong>Auditing:</strong> &quot;Who changed the production VPC configuration last week?&quot;</li>
<li><strong>Monitoring:</strong> &quot;Is spine-01 healthy right now?&quot;</li>
</ul>
<p>Each scenario calls for a different tool. Knowing <strong>when</strong> to use kubectl, <strong>when</strong> to use Gitea, and <strong>when</strong> to use Grafana is what separates confident operators from confused ones.</p>
<p>This module teaches you to <strong>choose the right interface for the job</strong>—not just how to use each tool, but when and why.</p>
<h3>What You&#39;ll Learn</h3>
<p>This module provides hands-on experience with all three operational interfaces:</p>
<ul>
<li><strong>kubectl:</strong> Your lens into current cluster state</li>
<li><strong>Gitea:</strong> Your configuration history and audit trail</li>
<li><strong>Grafana:</strong> Your window into fabric health over time</li>
</ul>
<p>You&#39;ll practice reading each interface, correlating information across them, and applying a systematic troubleshooting methodology.</p>
<h3>Learning Objectives</h3>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Select the appropriate interface</strong> for specific operational tasks (read current state, audit configuration, monitor health)</li>
<li><strong>Interpret kubectl output</strong> including events, reconciliation status, and resource relationships</li>
<li><strong>Navigate Gitea</strong> for configuration audit, commit history, and change diffs</li>
<li><strong>Read Grafana dashboards</strong> for fabric health monitoring across all 6 Hedgehog dashboards</li>
<li><strong>Correlate information across interfaces</strong> to troubleshoot configuration issues systematically</li>
<li><strong>Apply troubleshooting methodology</strong> using the three-interface approach for common problems</li>
</ol>
<h3>Setting Expectations</h3>
<p>This module builds on your VPC creation experience from Module 1.2. We&#39;ll use the <code>myfirst-vpc</code> you created to explore each interface systematically. By the end, you&#39;ll know exactly which tool to reach for when faced with any operational task.</p>
<hr>
<h2>The Three Interfaces Framework</h2>
<h3>Your Operational Dashboard</h3>
<p>Think of managing Hedgehog Fabric like flying an aircraft. You don&#39;t use a single instrument—you use different displays for different purposes:</p>
<ul>
<li><strong>Altimeter</strong> (current altitude) → <strong>kubectl</strong> (current cluster state)</li>
<li><strong>Flight log</strong> (where you&#39;ve been) → <strong>Gitea</strong> (configuration history)</li>
<li><strong>System health</strong> (engine status, fuel) → <strong>Grafana</strong> (fabric health metrics)</li>
</ul>
<p>Each interface provides unique information you can&#39;t get from the others.</p>
<h3>Interface Roles</h3>
<table>
<thead>
<tr>
<th>Interface</th>
<th>Primary Role</th>
<th>When to Use</th>
<th>Information Type</th>
</tr>
</thead>
<tbody><tr>
<td><strong>kubectl</strong></td>
<td><strong>Read/Inspect</strong></td>
<td>Check current state, view events, troubleshoot issues</td>
<td>Real-time cluster state</td>
</tr>
<tr>
<td><strong>Gitea</strong></td>
<td><strong>Write/Audit</strong></td>
<td>Create/modify configs, review history, audit changes</td>
<td>Declarative desired state</td>
</tr>
<tr>
<td><strong>Grafana</strong></td>
<td><strong>Observe/Monitor</strong></td>
<td>View health trends, monitor metrics, identify patterns</td>
<td>Time-series operational data</td>
</tr>
</tbody></table>
<h3>Decision Matrix: Which Tool Do I Use?</h3>
<p><strong>Use kubectl when you need to:</strong></p>
<ul>
<li>✅ Check if a VPC exists and is reconciled</li>
<li>✅ View error events from failed configurations</li>
<li>✅ List all resources of a specific type</li>
<li>✅ See current switch agent status</li>
<li>✅ Troubleshoot why something isn&#39;t working right now</li>
</ul>
<p><strong>Use Gitea when you need to:</strong></p>
<ul>
<li>✅ Create a new VPC configuration file</li>
<li>✅ Answer &quot;who changed this VPC and when?&quot;</li>
<li>✅ Compare current configuration to last week&#39;s version</li>
<li>✅ Review configuration before it gets deployed</li>
<li>✅ Maintain compliance audit trails</li>
</ul>
<p><strong>Use Grafana when you need to:</strong></p>
<ul>
<li>✅ View fabric health trends over the past 24 hours</li>
<li>✅ Check if switches are experiencing CPU pressure</li>
<li>✅ Monitor interface traffic and error rates</li>
<li>✅ Aggregate logs from multiple switches</li>
<li>✅ Identify when a problem started occurring</li>
</ul>
<p><strong>Key Insight:</strong> These interfaces complement each other. A complete troubleshooting flow often uses all three: Gitea to verify configuration, kubectl to check deployment status, Grafana to monitor runtime health.</p>
<hr>
<h2>Part 1: The Read Interface - kubectl</h2>
<h3>Your Window into Current State</h3>
<p>kubectl shows you what&#39;s happening <strong>right now</strong> in the cluster. Think of it as taking a snapshot of the fabric at this exact moment. It&#39;s your go-to tool for real-time inspection and troubleshooting.</p>
<hr>
<h3>Task 1.1: Inspect Your VPC</h3>
<p><strong>Objective:</strong> View the VPC you created in Module 1.2 and verify its reconciliation</p>
<p><strong>Why this matters:</strong> Before troubleshooting any VPC issue, you need to confirm it exists and check for error events. kubectl is your first diagnostic tool.</p>
<p><strong>Commands:</strong></p>
<pre><code class=""language-bash""># List all VPCs in the cluster
kubectl get vpcs

# Get detailed information about your VPC
kubectl get vpc myfirst-vpc -o yaml

# Check for reconciliation events (most important!)
kubectl describe vpc myfirst-vpc
</code></pre>
<p><strong>Expected output from kubectl describe:</strong></p>
<pre><code>Name:         myfirst-vpc
Namespace:    default
Labels:       fabric.githedgehog.com/ipv4ns=default
              fabric.githedgehog.com/vlanns=default
Annotations:  kubectl.kubernetes.io/last-applied-configuration: {...}
API Version:  vpc.githedgehog.com/v1beta1
Kind:         VPC
Metadata:
  Creation Timestamp:  2025-10-16T01:00:00Z
  Generation:          1
  Resource Version:    123456
  UID:                 abc-123-def
Spec:
  Ipv4Namespace:  default
  Subnets:
    Default:
      Dhcp:
        Enable:  true
        Range:
          End:    10.0.10.250
          Start:  10.0.10.10
      Gateway:  10.0.10.1
      Subnet:   10.0.10.0/24
      Vlan:     1010
  Vlan Namespace:  default
Events:            &lt;none&gt;
</code></pre>
<p><strong>What to look for:</strong></p>
<ul>
<li><strong>Events: &lt;none&gt;</strong> - This is <strong>good news</strong>! Hedgehog VPCs use event-based reconciliation. No error events means the VPC reconciled successfully.</li>
<li><strong>Spec fields present</strong> - Your configuration is stored in the cluster</li>
<li><strong>Gateway computed</strong> - Hedgehog automatically assigned 10.0.10.1</li>
</ul>
<p><strong>Understanding Event-Based Reconciliation:</strong></p>
<p>Hedgehog VPCs don&#39;t populate complex status fields. Instead, they use a simpler &quot;no news is good news&quot; model:</p>
<ul>
<li><strong>No error events</strong> = VPC is working correctly</li>
<li><strong>Error/Warning events</strong> = Something needs attention (the event message tells you what)</li>
</ul>
<p>This approach actually makes Day 2 operations <strong>easier</strong>: you don&#39;t need to parse status fields, just check for error events.</p>
<p><strong>Validation exercise:</strong></p>
<p>Run <code>kubectl describe vpc myfirst-vpc</code> and answer:</p>
<ul>
<li>Are there any error or warning events? (Expected: No)</li>
<li>What gateway IP was assigned? (Expected: 10.0.10.1)</li>
</ul>
<hr>
<h3>Task 1.2: Explore Fabric Resources</h3>
<p><strong>Objective:</strong> Discover what other Hedgehog resources exist and their relationships</p>
<p><strong>Why this matters:</strong> Understanding the full topology helps you troubleshoot connectivity issues and plan VPC attachments.</p>
<p><strong>Commands:</strong></p>
<pre><code class=""language-bash""># List all switches (agents)
kubectl get agents -A

# List all connections between switches and servers
kubectl get connections -A

# List all VPC attachments (servers connected to VPCs)
kubectl get vpcattachments -A
</code></pre>
<p><strong>Expected output - Switches:</strong></p>
<pre><code>NAMESPACE   NAME       ROLE          DESCR           APPLIED   VERSION
default     leaf-01    server-leaf   VS-01 MCLAG 1   2m        v0.87.4
default     leaf-02    server-leaf   VS-02 MCLAG 1   3m        v0.87.4
default     leaf-03    server-leaf   VS-03 ESLAG 1   2m        v0.87.4
default     leaf-04    server-leaf   VS-04 ESLAG 1   3m        v0.87.4
default     leaf-05    server-leaf   VS-05           5m        v0.87.4
default     spine-01   spine         VS-06           4m        v0.87.4
default     spine-02   spine         VS-07           3m        v0.87.4
</code></pre>
<p><strong>What this tells you:</strong></p>
<ul>
<li>🔍 7 switches total: 2 spines, 5 server-leaf switches</li>
<li>🔍 ROLE column distinguishes spine from leaf</li>
<li>🔍 APPLIED column shows last successful config push</li>
<li>🔍 VERSION shows the Hedgehog agent version running on each switch</li>
</ul>
<p><strong>Expected output - Connections:</strong></p>
<pre><code>NAME                                 TYPE           AGE
leaf-01--mclag-domain--leaf-02       mclag-domain   2h
server-01--mclag--leaf-01--leaf-02   mclag          2h
server-02--mclag--leaf-01--leaf-02   mclag          2h
server-03--unbundled--leaf-01        unbundled      2h
server-04--bundled--leaf-02          bundled        2h
...
</code></pre>
<p><strong>What this tells you:</strong></p>
<ul>
<li>🔍 Connection names are self-documenting: <code>server--type--switches</code></li>
<li>🔍 Types include mclag (dual-homed), eslag, bundled (port-channel), unbundled (single link)</li>
<li>🔍 Connections are pre-configured during fabric installation (Day 1, not Day 2)</li>
</ul>
<p><strong>Validation exercise:</strong></p>
<p>Run the commands above and answer:</p>
<ul>
<li>How many spine switches are in the fabric? (Expected: 2)</li>
<li>What connection type is used by server-01? (Expected: mclag)</li>
</ul>
<hr>
<h3>Task 1.3: Understanding kubectl describe vs get</h3>
<p><strong>Objective:</strong> Learn when to use <code>describe</code> versus <code>get</code> for different information needs</p>
<p><strong>Why this matters:</strong> <code>describe</code> is human-readable and shows events (critical for troubleshooting). <code>get -o yaml</code> gives you exact field values for validation.</p>
<p><strong>Commands:</strong></p>
<pre><code class=""language-bash""># Human-readable summary with events
kubectl describe vpc myfirst-vpc

# Raw YAML for exact field inspection
kubectl get vpc myfirst-vpc -o yaml

# Tabular list view
kubectl get vpcs
</code></pre>
<p><strong>When to use each:</strong></p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Best For</th>
<th>Example Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><code>kubectl describe</code></td>
<td>Troubleshooting</td>
<td>Check for error events during VPC creation</td>
</tr>
<tr>
<td><code>kubectl get -o yaml</code></td>
<td>Field validation</td>
<td>Verify exact VLAN ID or subnet CIDR</td>
</tr>
<tr>
<td><code>kubectl get</code> (tabular)</td>
<td>Quick inventory</td>
<td>List all VPCs to see what exists</td>
</tr>
</tbody></table>
<p><strong>Key teaching point:</strong></p>
<p>The Events section at the bottom of <code>kubectl describe</code> output is <strong>your first troubleshooting checkpoint</strong>. Always check events before diving deeper into logs or switch state.</p>
<hr>
<h3>Part 1 Summary</h3>
<p><strong>What you practiced:</strong></p>
<ul>
<li>✅ Inspecting VPC state with kubectl</li>
<li>✅ Understanding event-based reconciliation (no errors = success)</li>
<li>✅ Listing fabric resources (switches, connections)</li>
<li>✅ Choosing between <code>describe</code> and <code>get</code> commands</li>
</ul>
<p><strong>kubectl Quick Reference:</strong></p>
<pre><code class=""language-bash""># VPC operations
kubectl get vpcs                      # List all VPCs
kubectl describe vpc &lt;name&gt;           # Check events (most important!)
kubectl get vpc &lt;name&gt; -o yaml        # View full configuration

# Fabric topology
kubectl get agents -A                 # List switches
kubectl get connections -A            # List server connections
kubectl get vpcattachments -A         # List VPC-to-server bindings

# General troubleshooting
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20
</code></pre>
<hr>
<h2>Part 2: The Write Interface - Gitea</h2>
<h3>Your Configuration Time Machine</h3>
<p>Gitea provides Git-based version control for your network configurations. Every change is tracked: who made it, when, and why. This isn&#39;t just compliance—it&#39;s <strong>operational confidence</strong>. You can see exactly what changed when things break, and you can roll back safely.</p>
<hr>
<h3>Task 2.1: View Commit History</h3>
<p><strong>Objective:</strong> Understand when and why configurations changed</p>
<p><strong>Why this matters:</strong> When troubleshooting, knowing <strong>when</strong> a configuration changed helps correlate problems with specific changes. For audits, you need to answer &quot;who changed what, when.&quot;</p>
<p><strong>Steps:</strong></p>
<ol>
<li>Open Gitea in your browser: <code>http://localhost:3001</code></li>
<li>Navigate to the <code>student/hedgehog-config</code> repository</li>
<li>Click <strong>Commits</strong> in the top navigation</li>
<li>Review the commit history</li>
</ol>
<p><strong>Expected commit history (similar to):</strong></p>
<pre><code>fb8f8fe  Fix VPC name to be within 11 character limit        1 hour ago   student
9fbaa1c  Create my first VPC for Module 1.2 lab              1 hour ago   student
392b8de  Add test VPC for GitOps workflow validation          2 hours ago  student
07bae96  Fix VPCAttachment subnet format                      2 hours ago  student
...
</code></pre>
<p><strong>What each commit shows:</strong></p>
<ul>
<li>📜 <strong>Commit SHA</strong> (fb8f8fe) - Unique identifier for this exact configuration state</li>
<li>📜 <strong>Commit message</strong> - Human-readable description of what changed</li>
<li>📜 <strong>Timestamp</strong> - When the change was made</li>
<li>📜 <strong>Author</strong> - Who made the change</li>
</ul>
<p><strong>Validation exercise:</strong></p>
<p>Find the commit where you created <code>myfirst-vpc</code>:</p>
<ul>
<li>What&#39;s the commit message? (Expected: Something like &quot;Create my first VPC&quot;)</li>
<li>When was it created? (Expected: Approximately 1 hour ago, or when you completed Module 1.2)</li>
</ul>
<p><strong>Why Git for network configuration?</strong></p>
<p>Traditional networking has no native audit trail. If someone misconfigures a switch, you might never know who or when unless you manually check logs. Git gives you:</p>
<ul>
<li><strong>Accountability:</strong> Every change is attributed to an author</li>
<li><strong>History:</strong> Complete timeline of all configuration changes</li>
<li><strong>Rollback:</strong> Can revert to any previous configuration state</li>
<li><strong>Code review:</strong> Changes can be reviewed before deployment</li>
</ul>
<hr>
<h3>Task 2.2: View File Changes (Diffs)</h3>
<p><strong>Objective:</strong> Compare configurations across commits to understand what changed</p>
<p><strong>Why this matters:</strong> When a VPC stops working after a change, seeing the <strong>exact diff</strong> helps you identify the problem. Diffs also help during code reviews.</p>
<p><strong>Steps:</strong></p>
<ol>
<li>In Gitea commit history, click on any commit (e.g., the latest one)</li>
<li>Gitea shows the <strong>diff</strong> view with changes highlighted</li>
<li>Observe additions (green, + prefix) and deletions (red, - prefix)</li>
</ol>
<p><strong>Example diff view:</strong></p>
<pre><code class=""language-diff"">vpcs/my-first-vpc.yaml → vpcs/myfirst-vpc.yaml

apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
- name: my-first-vpc     # RED: Removed (12 characters - invalid)
+ name: myfirst-vpc      # GREEN: Added (11 characters - valid)
  namespace: default
spec:
  ipv4Namespace: default
  vlanNamespace: default
  subnets:
    default:
      subnet: 10.0.10.0/24
      vlan: 1010
</code></pre>
<p><strong>What the diff tells you:</strong></p>
<ul>
<li>🔄 <strong>Red lines (-)</strong> show what was removed</li>
<li>🔄 <strong>Green lines (+)</strong> show what was added</li>
<li>🔄 <strong>Context lines</strong> (no prefix) show unchanged configuration</li>
<li>🔄 <strong>File rename</strong> detected (my-first-vpc.yaml → myfirst-vpc.yaml)</li>
</ul>
<p><strong>Real-world scenario:</strong></p>
<p>Imagine a colleague tells you: &quot;I updated prod-vpc yesterday and now DHCP isn&#39;t working.&quot;</p>
<p>With Gitea, you can:</p>
<ol>
<li>Find yesterday&#39;s commits</li>
<li>View the diff</li>
<li>Spot the issue: <code>dhcp.enable: true</code> changed to <code>dhcp.enable: false</code></li>
<li>Revert the file and commit the fix</li>
</ol>
<p><strong>Validation exercise:</strong></p>
<p>Click on a commit that modified a VPC file and answer:</p>
<ul>
<li>What field(s) changed?</li>
<li>Can you identify what was added vs. removed?</li>
</ul>
<hr>
<h3>Task 2.3: Explore Repository Structure</h3>
<p><strong>Objective:</strong> Understand how Hedgehog configurations are organized</p>
<p><strong>Why this matters:</strong> Knowing where to create new VPC files, where to find examples, and how directories are structured makes you efficient during operations.</p>
<p><strong>Steps:</strong></p>
<ol>
<li>Navigate to the repository root</li>
<li>Browse the directory structure</li>
</ol>
<p><strong>Expected structure:</strong></p>
<pre><code>hedgehog-config/
├── README.md                    # Repository documentation
├── vpcs/                        # VPC configurations
│   ├── README.md                # VPC-specific documentation
│   ├── myfirst-vpc.yaml         # Your VPC from Module 1.2
│   ├── test-vpc.yaml            # Example VPC
│   └── vpc-example-1.yaml       # Template VPC
└── vpc-attachments/             # VPC attachment configurations
    ├── README.md                # Attachment documentation
    └── attachment-example.yaml  # Example attachment
</code></pre>
<p><strong>What this structure tells you:</strong></p>
<ul>
<li>📁 <strong>Organized by resource type</strong> (vpcs/, vpc-attachments/)</li>
<li>📁 <strong>README files</strong> provide documentation and examples</li>
<li>📁 <strong>Flat structure</strong> (no deep nesting) makes files easy to find</li>
<li>📁 <strong>Example files</strong> serve as templates for new resources</li>
</ul>
<p><strong>Where to create new resources:</strong></p>
<ul>
<li>New VPC? → Create YAML file in <code>vpcs/</code> directory</li>
<li>New attachment? → Create YAML file in <code>vpc-attachments/</code> directory</li>
</ul>
<p><strong>Validation exercise:</strong></p>
<p>Answer these questions:</p>
<ul>
<li>If you need to create a new VPC, where would you place the file? (Expected: <code>vpcs/</code> directory)</li>
<li>What&#39;s the purpose of the example files? (Expected: Templates to copy/modify for new resources)</li>
</ul>
<hr>
<h3>Part 2 Summary</h3>
<p><strong>What you practiced:</strong></p>
<ul>
<li>✅ Viewing Git commit history in Gitea</li>
<li>✅ Reading diffs to understand configuration changes</li>
<li>✅ Navigating repository structure</li>
<li>✅ Understanding why Git provides audit trail for compliance</li>
</ul>
<p><strong>Gitea Quick Reference:</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>How to Access</th>
<th>Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Commits Tab</strong></td>
<td>Click &quot;Commits&quot; in top nav</td>
<td>View change history</td>
</tr>
<tr>
<td><strong>Diff View</strong></td>
<td>Click on any commit</td>
<td>See what changed</td>
</tr>
<tr>
<td><strong>File Browser</strong></td>
<td>Click on files/folders</td>
<td>Navigate repository</td>
</tr>
<tr>
<td><strong>Blame View</strong></td>
<td>Click &quot;Blame&quot; on file</td>
<td>See who last changed each line (advanced)</td>
</tr>
</tbody></table>
<p><strong>Key takeaway:</strong> Gitea is your <strong>configuration audit trail</strong>. Use it to answer &quot;who/what/when&quot; questions about changes, and to compare configurations across time.</p>
<hr>
<h2>Part 3: The Observe Interface - Grafana</h2>
<h3>Your Fabric Health Dashboard</h3>
<p>Grafana visualizes metrics and logs collected from switches over time. While kubectl shows you &quot;what&#39;s happening right now,&quot; Grafana shows you <strong>trends</strong>: Is CPU usage increasing? Are interfaces dropping packets? When did this error pattern start?</p>
<p>Hedgehog provides <strong>6 pre-built dashboards</strong> covering different aspects of fabric health. In this task, you&#39;ll tour all six to understand what each one monitors.</p>
<p><strong>Important timing note:</strong> We&#39;ll take a <strong>skim approach</strong> (6 dashboards in 5-6 minutes). The goal is familiarity, not mastery. You&#39;ll see these dashboards again in Course 3 (Observability) where we&#39;ll go deeper.</p>
<hr>
<h3>Grafana Dashboard Tour Overview</h3>
<p><strong>Access:</strong> <code>http://localhost:3000</code> (username: <code>admin</code>, password: <code>prom-operator</code>)</p>
<p><strong>The 6 Hedgehog Dashboards:</strong></p>
<ol>
<li><strong>Fabric Dashboard</strong> - Overall fabric health (MOST IMPORTANT)</li>
<li><strong>Platform Dashboard</strong> - Control plane health (MOST IMPORTANT)</li>
<li><strong>Interfaces Dashboard</strong> - Port status and traffic</li>
<li><strong>Logs Dashboard</strong> - Aggregated syslog</li>
<li><strong>Node Exporter Dashboard</strong> - Switch hardware metrics</li>
<li><strong>Switch CRM Dashboard</strong> - ASIC capacity monitoring</li>
</ol>
<p><strong>Time allocation:</strong></p>
<ul>
<li>Dashboards 1-2: ~1 minute each (critical for daily operations)</li>
<li>Dashboards 3-6: ~30-60 seconds each (overview only)</li>
</ul>
<hr>
<h3>Task 3.1: Fabric Dashboard</h3>
<p><strong>Purpose:</strong> High-level fabric health overview - your daily starting point</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/ab831ceb-cf5c-474a-b7e9-83dcd075c218/fabric</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>Switch Status</strong> - Green/red indicators for each switch</li>
<li><strong>Fabric Topology</strong> - Visual representation of spine-leaf architecture</li>
<li><strong>Active VPCs</strong> - Count of deployed VPCs</li>
<li><strong>Connection Health</strong> - MCLAG/ESLAG status</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ All switches showing green status</li>
<li>✅ VPC count matches <code>kubectl get vpcs | wc -l</code> output</li>
<li>✅ No red indicators or alerts</li>
<li>✅ All connections showing as established</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>This is your <strong>first checkpoint each day</strong>. Before making any changes to the fabric, glance at this dashboard to confirm everything is healthy. Red indicators warrant investigation before you proceed.</p>
<p><strong>Validation exercise:</strong></p>
<p>Open the Fabric dashboard and answer:</p>
<ul>
<li>How many switches are showing &quot;Up&quot; status? (Expected: 7 - all switches)</li>
<li>Does the VPC count match what you see in kubectl? (Expected: Yes)</li>
</ul>
<hr>
<h3>Task 3.2: Platform Dashboard</h3>
<p><strong>Purpose:</strong> Hedgehog control plane health (Kubernetes &amp; Fabricator controller)</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/f8a648b9-5510-49ca-9273-952ba6169b7b/platform</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>Control Node Status</strong> - CPU, memory, disk usage of control plane</li>
<li><strong>Fabricator Controller</strong> - Reconciliation rate, error count</li>
<li><strong>Kubernetes API</strong> - API server request rate and latency</li>
<li><strong>etcd Health</strong> - Cluster database metrics</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ Control node CPU &lt; 80%</li>
<li>✅ Fabricator error count = 0</li>
<li>✅ API server responding with low latency</li>
<li>✅ etcd healthy with no frequent leader elections</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>The control plane is the <strong>brain of Hedgehog</strong>. If the control plane is unhealthy (high CPU, Fabricator errors), your VPC changes won&#39;t apply correctly. Monitor this dashboard during and after VPC deployments.</p>
<p><strong>What to expect:</strong> CPU/memory spikes during reconciliation are normal. Sustained high usage or error counts are not.</p>
<p><strong>Validation exercise:</strong></p>
<p>Open the Platform dashboard and observe:</p>
<ul>
<li>What&#39;s the current CPU usage? (Expected: Typically &lt; 50%, varies by activity)</li>
<li>Are there any Fabricator errors? (Expected: 0 errors)</li>
</ul>
<hr>
<h3>Task 3.3: Interfaces Dashboard</h3>
<p><strong>Purpose:</strong> Switch port utilization, link status, and error rates</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/a5e5b12d-b340-4753-8f83-af8d54304822/interfaces</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>Port Status</strong> - Up/down state for all interfaces</li>
<li><strong>Traffic Rates</strong> - Ingress/egress bandwidth usage</li>
<li><strong>Error Counters</strong> - CRC errors, drops, discards</li>
<li><strong>Interface Types</strong> - Fabric links, server links, MCLAG peer links</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ All fabric links (spine↔leaf) showing up</li>
<li>✅ Error rates &lt; 0.1% (occasional errors are normal, sustained errors indicate problems)</li>
<li>✅ Traffic patterns match expected workload</li>
<li>✅ No excessive packet drops</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>Interface health is your early warning system for <strong>physical layer issues</strong>: bad cables, transceiver failures, or link saturation. If users report connectivity problems, check this dashboard for interface errors.</p>
<p><strong>Validation exercise:</strong></p>
<p>Open the Interfaces dashboard and check:</p>
<ul>
<li>Are all fabric links (spine-to-leaf) showing &quot;Up&quot;? (Expected: Yes)</li>
<li>Do you see any interfaces with high error rates? (Expected: No, or very low)</li>
</ul>
<hr>
<h3>Task 3.4: Logs Dashboard</h3>
<p><strong>Purpose:</strong> Aggregated syslog messages from all switches</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/c42a51e5-86a8-42a0-b1c9-d1304ae655bc/logs</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>Recent Logs</strong> - Stream of syslog messages</li>
<li><strong>Log Levels</strong> - Count by severity (INFO, WARNING, ERROR)</li>
<li><strong>Top Log Sources</strong> - Which switches are logging most</li>
<li><strong>Search Box</strong> - Filter logs by keyword</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ Mostly INFO level logs (routine operations)</li>
<li>✅ No ERROR level logs (or investigate if present)</li>
<li>✅ WARNING logs during config changes are normal</li>
<li>✅ Logs from all 7 switches present</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>Logs are your <strong>black box recorder</strong> for the fabric. When troubleshooting intermittent issues or reconstructing events, searchable, time-stamped logs from all switches are essential.</p>
<p><strong>Validation exercise:</strong></p>
<p>Open the Logs dashboard and try:</p>
<ul>
<li>Search for &quot;VPC&quot; or &quot;myfirst-vpc&quot; in the search box</li>
<li>Do you see logs related to your VPC creation? (Expected: Yes, VNI assignment and VLAN configuration logs)</li>
</ul>
<hr>
<h3>Task 3.5: Node Exporter Dashboard</h3>
<p><strong>Purpose:</strong> Switch hardware metrics (CPU, memory, disk, system load)</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/rYdddlPWA/node-exporter-full-2</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>CPU Usage</strong> - Per-switch CPU utilization</li>
<li><strong>Memory Usage</strong> - RAM consumption</li>
<li><strong>Disk I/O</strong> - Read/write rates and disk space</li>
<li><strong>System Load</strong> - 1/5/15 minute load averages</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ CPU usage &lt; 80% (brief spikes during config push are okay)</li>
<li>✅ Memory usage stable (not steadily growing)</li>
<li>✅ Disk space &gt; 20% free</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>These are your <strong>switch vital signs</strong>—Linux OS-level metrics. High CPU or memory usage can indicate switch software issues, not network issues. Disk space problems can prevent logging.</p>
<p><strong>Quick skim:</strong> This dashboard has many panels. Focus on the top-level summary panels for CPU and memory. Deep dives happen during capacity planning or performance troubleshooting.</p>
<hr>
<h3>Task 3.6: Switch CRM Dashboard</h3>
<p><strong>Purpose:</strong> Critical Resource Monitoring - ASIC resources (TCAM, route tables, neighbors)</p>
<p><strong>URL:</strong> <code>http://localhost:3000/d/fb08315c-cabb-4da7-9db9-2e17278f1781/switch-critical-resources</code></p>
<p><strong>Key panels to look for:</strong></p>
<ul>
<li><strong>TCAM Utilization</strong> - ACL and route table space used</li>
<li><strong>Route Count</strong> - IPv4/IPv6 routes programmed</li>
<li><strong>ARP/ND Neighbors</strong> - Neighbor table entries</li>
<li><strong>VXLAN Tunnels</strong> - VTEP count</li>
</ul>
<p><strong>What healthy looks like:</strong></p>
<ul>
<li>✅ TCAM usage &lt; 80% (leaving room for growth)</li>
<li>✅ Route count matches expected topology</li>
<li>✅ Neighbor entries stable</li>
<li>✅ VXLAN tunnels correlate with VPC count</li>
</ul>
<p><strong>Why this dashboard matters:</strong></p>
<p>Switch ASICs have <strong>finite hardware resources</strong>. TCAM (Ternary Content Addressable Memory) stores ACLs and routes. If you approach capacity limits, the switch may reject new VPCs or routes. This dashboard helps you plan capacity.</p>
<p><strong>Quick skim:</strong> Note the utilization percentages. If anything is approaching 80%, that&#39;s a signal to plan for fabric expansion.</p>
<hr>
<h3>Part 3 Summary</h3>
<p><strong>What you explored:</strong></p>
<ul>
<li>✅ Purpose of each of the 6 Grafana dashboards</li>
<li>✅ Key metrics to monitor in each dashboard</li>
<li>✅ How to navigate between dashboards</li>
<li>✅ What &quot;healthy&quot; looks like in each context</li>
</ul>
<p><strong>Grafana Dashboard Quick Reference:</strong></p>
<table>
<thead>
<tr>
<th>Dashboard</th>
<th>Use When</th>
<th>Key Metrics</th>
<th>Priority</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Fabric</strong></td>
<td>Daily health check</td>
<td>Switch status, VPC count</td>
<td>🔴 Critical</td>
</tr>
<tr>
<td><strong>Platform</strong></td>
<td>Control plane issues</td>
<td>Fabricator errors, CPU</td>
<td>🔴 Critical</td>
</tr>
<tr>
<td><strong>Interfaces</strong></td>
<td>Link/connectivity problems</td>
<td>Port status, error rates</td>
<td>🟡 Important</td>
</tr>
<tr>
<td><strong>Logs</strong></td>
<td>Troubleshooting, auditing</td>
<td>Error logs, event timeline</td>
<td>🟡 Important</td>
</tr>
<tr>
<td><strong>Node Exporter</strong></td>
<td>Performance issues</td>
<td>CPU, memory, disk</td>
<td>🟢 Monitoring</td>
</tr>
<tr>
<td><strong>CRM</strong></td>
<td>Capacity planning</td>
<td>TCAM usage, route count</td>
<td>🟢 Monitoring</td>
</tr>
</tbody></table>
<p><strong>Key takeaway:</strong> Grafana shows you <strong>trends over time</strong>. Use it to answer questions like &quot;when did this start?&quot; and &quot;is this getting worse?&quot; that kubectl can&#39;t answer.</p>
<hr>
<h2>Integrated Troubleshooting Scenario</h2>
<h3>&quot;My VPC Isn&#39;t Working&quot; - A Decision Tree</h3>
<p>You&#39;ve now practiced using each interface individually. Real troubleshooting requires using <strong>all three together</strong> in a systematic flow. Let&#39;s walk through a common scenario.</p>
<hr>
<h3>The Scenario</h3>
<p>A colleague messages you:</p>
<blockquote>
<p>&quot;I created a VPC called <code>broken-vpc</code> but servers can&#39;t get DHCP addresses. Can you help troubleshoot?&quot;</p>
</blockquote>
<p>Where do you start? Which interface do you check first?</p>
<hr>
<h3>Step 1: Check Configuration (Gitea)</h3>
<p><strong>Question:</strong> Is the VPC configured correctly in Git?</p>
<p><strong>Why start here:</strong> Git is the <strong>source of truth</strong>. If the configuration is wrong in Gitea, nothing else matters—the VPC will be created incorrectly.</p>
<p><strong>Actions:</strong></p>
<ol>
<li>Open Gitea → navigate to <code>student/hedgehog-config</code> repository</li>
<li>Browse to <code>vpcs/</code> directory</li>
<li>Find and open <code>broken-vpc.yaml</code></li>
<li>Review the DHCP configuration</li>
</ol>
<p><strong>What to look for:</strong></p>
<pre><code class=""language-yaml"">spec:
  subnets:
    default:
      dhcp:
        enable: true        # ✅ Should be true
        range:
          start: 10.x.x.10  # ✅ Should be a valid IP in the subnet
          end: 10.x.x.250   # ✅ Should be greater than start
      subnet: 10.x.x.0/24   # ✅ Should match the IP range
      vlan: 1001            # ✅ Should be in valid VLAN range
</code></pre>
<p><strong>If configuration is wrong:</strong></p>
<ul>
<li>Fix it in Gitea (edit the file, commit the change)</li>
<li>Wait for ArgoCD to sync (or trigger manual sync)</li>
<li>The VPC will reconcile with the correct configuration</li>
</ul>
<p><strong>If configuration looks correct:</strong></p>
<ul>
<li>Move to Step 2 (check deployment status)</li>
</ul>
<hr>
<h3>Step 2: Check Deployment Status (kubectl)</h3>
<p><strong>Question:</strong> Did the VPC actually get created in the cluster? Did it reconcile successfully?</p>
<p><strong>Why check this:</strong> The configuration might be correct in Git, but ArgoCD might not have synced it yet, or the Fabricator controller might have encountered an error during reconciliation.</p>
<p><strong>Actions:</strong></p>
<pre><code class=""language-bash""># Check if VPC exists
kubectl get vpc broken-vpc

# Check for error events (MOST IMPORTANT)
kubectl describe vpc broken-vpc

# Focus on the Events section at the bottom
kubectl describe vpc broken-vpc | tail -20
</code></pre>
<p><strong>What to look for in Events:</strong></p>
<p><strong>Healthy VPC (no errors):</strong></p>
<pre><code>Events:  &lt;none&gt;
</code></pre>
<p>No error events means the VPC reconciled successfully. If DHCP still isn&#39;t working, the issue is likely at the runtime layer (check Grafana in Step 3).</p>
<p><strong>VPC with reconciliation errors:</strong></p>
<pre><code>Events:
  Type     Reason             Message
  ----     ------             -------
  Warning  ReconcileFailed    Subnet 10.10.10.0/24 overlaps with existing VPC &#39;prod-vpc&#39;
  Warning  ReconcileFailed    Invalid VLAN 999 not in vlanNamespace &#39;default&#39; range
  Error    DHCPRangeInvalid   DHCP range 10.10.10.10-10.10.10.5 invalid: start &gt; end
</code></pre>
<p><strong>Common error events and what they mean:</strong></p>
<table>
<thead>
<tr>
<th>Error Event</th>
<th>Problem</th>
<th>Fix</th>
</tr>
</thead>
<tbody><tr>
<td>&quot;Subnet overlaps...&quot;</td>
<td>IP conflict with another VPC</td>
<td>Change subnet to non-overlapping range</td>
</tr>
<tr>
<td>&quot;Invalid VLAN...&quot;</td>
<td>VLAN ID not in allowed range</td>
<td>Use VLAN within vlanNamespace range</td>
</tr>
<tr>
<td>&quot;DHCP range invalid&quot;</td>
<td>start IP &gt; end IP, or range outside subnet</td>
<td>Fix DHCP range in Gitea</td>
</tr>
</tbody></table>
<p><strong>If you see error events:</strong></p>
<ul>
<li>The event message tells you exactly what&#39;s wrong</li>
<li>Fix the issue in Gitea (go back to Step 1)</li>
<li>Commit the fix and wait for reconciliation</li>
</ul>
<p><strong>If no error events:</strong></p>
<ul>
<li>VPC is correctly deployed</li>
<li>Move to Step 3 (check runtime health)</li>
</ul>
<hr>
<h3>Step 3: Check Fabric Health (Grafana)</h3>
<p><strong>Question:</strong> Are the switches healthy and able to serve DHCP requests?</p>
<p><strong>Why check this:</strong> Even if the VPC is correctly configured and deployed, switches might be unhealthy (down, CPU overloaded, Fabricator errors) and unable to forward DHCP traffic.</p>
<p><strong>Actions:</strong></p>
<ol>
<li><p><strong>Open Fabric Dashboard</strong></p>
<ul>
<li>URL: <code>http://localhost:3000/d/ab831ceb-cf5c-474a-b7e9-83dcd075c218/fabric</code></li>
<li>Check: Are all switches showing green (Up) status?</li>
<li>Red switches indicate hardware/connectivity issues</li>
</ul>
</li>
<li><p><strong>Open Platform Dashboard</strong></p>
<ul>
<li>URL: <code>http://localhost:3000/d/f8a648b9-5510-49ca-9273-952ba6169b7b/platform</code></li>
<li>Check: Is Fabricator error count = 0?</li>
<li>Errors here mean reconciliation loop is failing</li>
</ul>
</li>
<li><p><strong>Open Logs Dashboard</strong></p>
<ul>
<li>URL: <code>http://localhost:3000/d/c42a51e5-86a8-42a0-b1c9-d1304ae655bc/logs</code></li>
<li>Search for &quot;broken-vpc&quot; or &quot;DHCP&quot;</li>
<li>Look for error messages related to DHCP relay or VPC configuration</li>
</ul>
</li>
</ol>
<p><strong>What to look for:</strong></p>
<ul>
<li><strong>Red switches</strong> → Hardware/connectivity issue, escalate to infrastructure team</li>
<li><strong>Fabricator errors &gt; 0</strong> → Control plane issue, check Fabricator logs with <code>kubectl logs -n fab deployment/fabric-controller-manager</code></li>
<li><strong>DHCP relay errors in logs</strong> → Switches trying but failing to forward DHCP, check network connectivity between servers and switches</li>
</ul>
<p><strong>If Grafana shows healthy state:</strong></p>
<ul>
<li>Switches are up, control plane is healthy, no errors in logs</li>
<li>The issue may be server-side (DHCP client not running, interface down)</li>
<li>Or VPCAttachment might be missing (server not actually connected to VPC)</li>
</ul>
<hr>
<h3>Troubleshooting Decision Tree Diagram</h3>
<pre><code>My VPC Isn&#39;t Working
    ↓
Is the config correct in Gitea?
    ├─ NO → Fix config in Gitea, commit, wait for sync
    └─ YES → ↓
              Does kubectl show the VPC exists?
                  ├─ NO → Check ArgoCD sync status
                  └─ YES → ↓
                            Are there error events in kubectl describe?
                                ├─ YES → Fix issue shown in event message (back to Gitea)
                                └─ NO → ↓
                                          Are switches healthy in Grafana?
                                              ├─ NO → Check Fabric/Platform dashboards, escalate if needed
                                              └─ YES → Check Logs dashboard, verify VPCAttachment, check server DHCP client
</code></pre>
<hr>
<h3>Key Troubleshooting Principles</h3>
<ol>
<li><strong>Start at the source</strong> (Gitea) - Wrong configuration is the most common issue</li>
<li><strong>Verify deployment</strong> (kubectl) - Check that desired state reached the cluster</li>
<li><strong>Check runtime health</strong> (Grafana) - Ensure switches can execute the configuration</li>
<li><strong>Follow events</strong> - Error events tell you exactly what&#39;s wrong</li>
<li><strong>Correlate across interfaces</strong> - No single interface has the complete picture</li>
</ol>
<p><strong>Validation exercise:</strong></p>
<p>Answer these questions to check your understanding:</p>
<ul>
<li>If a VPC has no error events in kubectl, what does that mean? (Expected: VPC reconciled successfully)</li>
<li>Where would you look to see who last modified a VPC configuration? (Expected: Gitea commit history)</li>
<li>Which Grafana dashboard shows if switches are up or down? (Expected: Fabric Dashboard)</li>
</ul>
<hr>
<h2>Wrap-Up</h2>
<h3>Key Takeaways</h3>
<p>You&#39;ve now mastered the three-interface operational model:</p>
<ol>
<li><p><strong>kubectl = Current State</strong></p>
<ul>
<li>Use for: Real-time inspection, troubleshooting, checking reconciliation</li>
<li>Key command: <code>kubectl describe</code> to view events</li>
</ul>
</li>
<li><p><strong>Gitea = Configuration History</strong></p>
<ul>
<li>Use for: Auditing changes, reviewing diffs, maintaining compliance</li>
<li>Key feature: Commit history with diffs</li>
</ul>
</li>
<li><p><strong>Grafana = Health Trends</strong></p>
<ul>
<li>Use for: Monitoring metrics over time, identifying patterns, capacity planning</li>
<li>Key dashboards: Fabric (daily check), Platform (control plane), Interfaces (links)</li>
</ul>
</li>
<li><p><strong>Troubleshooting = All Three Together</strong></p>
<ul>
<li>Flow: Gitea (config) → kubectl (deployment) → Grafana (runtime health)</li>
<li>Always check events first</li>
<li>Error events tell you what&#39;s wrong</li>
</ul>
</li>
</ol>
<h3>What You&#39;ve Accomplished</h3>
<ul>
<li>✅ Explored kubectl to inspect VPCs and fabric resources</li>
<li>✅ Navigated Gitea to view commit history and diffs</li>
<li>✅ Toured all 6 Grafana dashboards</li>
<li>✅ Applied systematic troubleshooting methodology</li>
</ul>
<h3>Confidence Check</h3>
<p>You should now feel confident answering:</p>
<ul>
<li>&quot;Which interface should I use to check if a VPC is deployed correctly?&quot; → kubectl</li>
<li>&quot;How do I see who changed a VPC configuration last week?&quot; → Gitea commit history</li>
<li>&quot;Where can I view switch CPU usage trends over the past day?&quot; → Grafana Node Exporter dashboard</li>
<li>&quot;My VPC isn&#39;t working—where do I start troubleshooting?&quot; → Check config in Gitea, then events in kubectl, then health in Grafana</li>
</ul>
<h3>Preview of Module 1.4</h3>
<p>Next module wraps up Course 1 with a recap of what you&#39;ve learned (Modules 1.1-1.3) and previews Course 2, where you&#39;ll use these interfaces to perform actual provisioning operations: creating VPCs, attaching servers, and validating connectivity.</p>
<hr>
<h2>Assessment</h2>
<h3>Question 1: Interface Selection</h3>
<p><strong>Stem:</strong> You need to view historical CPU usage trends for spine-01 over the last 24 hours. Which interface should you use?</p>
<p>A) kubectl
B) Gitea
C) Grafana
D) ArgoCD</p>
<p><strong>Correct Answer:</strong> C (Grafana)</p>
<p><strong>Explanation:</strong></p>
<p>Grafana stores time-series metrics and displays trends over time. kubectl shows current state only (a snapshot). Gitea is for configuration history, not runtime metrics. ArgoCD is for GitOps deployment, not monitoring. The Node Exporter dashboard in Grafana shows CPU usage trends.</p>
<p><strong>Learning Objective:</strong> LO #1 - Select the appropriate interface</p>
<hr>
<h3>Question 2: kubectl Event Interpretation</h3>
<p><strong>Stem:</strong> You run <code>kubectl describe vpc test-vpc</code> and see this in the Events section:</p>
<pre><code>Warning  ReconcileFailed  2m  fabricator  Subnet 10.10.10.0/24 overlaps with existing VPC &#39;prod-vpc&#39;
</code></pre>
<p>What does this mean?</p>
<p>A) The VPC is being deployed normally (this warning is expected)
B) The VPC configuration has an error that must be fixed
C) The VPC is waiting for switches to become available
D) The Fabricator controller needs to be restarted</p>
<p><strong>Correct Answer:</strong> B (The VPC configuration has an error that must be fixed)</p>
<p><strong>Explanation:</strong></p>
<p>Warning and Error events indicate configuration problems that prevent successful reconciliation. This specific event shows a subnet conflict—the VPC cannot be created until you choose a non-overlapping subnet. You need to fix the configuration in Gitea and recommit. No error/warning events = successful reconciliation.</p>
<p><strong>Learning Objective:</strong> LO #2 - Interpret kubectl output</p>
<hr>
<h3>Question 3: Gitea Audit Trail</h3>
<p><strong>Stem:</strong> Your manager asks: &quot;When was the production-vpc modified and by whom?&quot; Which Gitea feature should you use?</p>
<p>A) File browser
B) Commit history
C) Branch manager
D) Pull requests</p>
<p><strong>Correct Answer:</strong> B (Commit history)</p>
<p><strong>Explanation:</strong></p>
<p>The commit history in Gitea shows all changes with timestamps, authors, and commit messages. Each commit records who made the change and when. The file browser shows current file contents but not history. Branch manager and pull requests are for workflow management, not historical audit.</p>
<p><strong>Learning Objective:</strong> LO #3 - Navigate Gitea</p>
<hr>
<h3>Question 4: Grafana Dashboard Selection</h3>
<p><strong>Stem:</strong> You suspect a switch is dropping packets due to a bad cable or transceiver. Which Grafana dashboard is MOST useful?</p>
<p>A) Fabric Dashboard
B) Platform Dashboard
C) Interfaces Dashboard
D) Node Exporter Dashboard</p>
<p><strong>Correct Answer:</strong> C (Interfaces Dashboard)</p>
<p><strong>Explanation:</strong></p>
<p>The Interfaces Dashboard shows detailed per-port metrics including error counters (CRC errors, frame errors), drop counters, and discard counters—all indicators of physical layer issues like bad cables or transceivers. The Fabric Dashboard shows high-level health but not detailed error counters. Platform Dashboard monitors control plane, not switch ports. Node Exporter shows OS metrics, not interface errors.</p>
<p><strong>Learning Objective:</strong> LO #4 - Read Grafana dashboards</p>
<hr>
<h3>Question 5: Troubleshooting Methodology</h3>
<p><strong>Stem:</strong> A VPC appears to be configured correctly in Gitea, but servers aren&#39;t receiving DHCP addresses. What should you check NEXT?</p>
<p>A) Recreate the VPC configuration file in Gitea
B) Use kubectl to verify the VPC was deployed and check for error events
C) Check Grafana for switch health
D) Contact support immediately</p>
<p><strong>Correct Answer:</strong> B (Use kubectl to verify the VPC was deployed and check for error events)</p>
<p><strong>Explanation:</strong></p>
<p>Follow the systematic troubleshooting flow: Config (Gitea) → Deployment (kubectl) → Health (Grafana). If Gitea config looks correct, the next step is to verify the VPC actually deployed successfully using kubectl and check for error events. The events will tell you if reconciliation failed. Only after confirming kubectl shows successful deployment should you move to Grafana to check runtime health.</p>
<p><strong>Learning Objective:</strong> LO #5 - Correlate information across interfaces</p>
<hr>
<h3>Question 6: Interface Correlation</h3>
<p><strong>Stem:</strong> You see in Grafana Fabric Dashboard that spine-01 is showing &quot;Down&quot; status. What should you do FIRST?</p>
<p>A) Reboot spine-01 immediately
B) Check kubectl for spine-01 Agent status and events
C) Edit spine-01 configuration in Gitea
D) Check Platform Dashboard</p>
<p><strong>Correct Answer:</strong> B (Check kubectl for spine-01 Agent status and events)</p>
<p><strong>Explanation:</strong></p>
<p>Grafana shows symptoms (switch down), but kubectl can tell you more detail about WHY: Agent not reporting, configuration errors, reconciliation issues, etc. Always gather more information before taking action. Rebooting (A) without understanding the root cause could make things worse. Editing config (C) is premature—you don&#39;t know if configuration is the issue yet. Platform Dashboard (D) shows control plane health, not specific switch status.</p>
<p><strong>Learning Objective:</strong> LO #6 - Apply troubleshooting methodology</p>
<hr>
<h3>Practical Assessment</h3>
<p><strong>Task:</strong> Using kubectl, Gitea, and Grafana, answer these questions about your fabric:</p>
<ol>
<li><strong>kubectl:</strong> How many VPCs exist in the cluster right now?</li>
<li><strong>Gitea:</strong> Who created the <code>myfirst-vpc</code> configuration and when?</li>
<li><strong>Grafana:</strong> Are all switches showing healthy status in the Fabric Dashboard?</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Uses correct kubectl command (<code>kubectl get vpcs</code>)</li>
<li>✅ Navigates to Gitea commit history and identifies author/timestamp</li>
<li>✅ Opens Grafana Fabric Dashboard and interprets switch status</li>
<li>✅ Can explain which interface was appropriate for each question</li>
<li>✅ Demonstrates understanding of when to use each interface</li>
</ul>
<p><strong>Rubric:</strong></p>
<ul>
<li><strong>Full credit (10 points):</strong> Correctly uses all three interfaces and provides accurate answers</li>
<li><strong>Partial credit (6-9 points):</strong> Uses 2-3 interfaces correctly but struggles with one</li>
<li><strong>Minimal credit (3-5 points):</strong> Uses 1 interface correctly but needs guidance on others</li>
<li><strong>No credit (0-2 points):</strong> Cannot use interfaces appropriately or answers are incorrect</li>
</ul>
<hr>
<h2>Reference</h2>
<h3>kubectl Commands Reference</h3>
<p><strong>VPC inspection:</strong></p>
<pre><code class=""language-bash"">kubectl get vpcs                      # List all VPCs
kubectl describe vpc &lt;name&gt;           # Check events for reconciliation status
kubectl get vpc &lt;name&gt; -o yaml        # View full VPC configuration
</code></pre>
<p><strong>Fabric topology:</strong></p>
<pre><code class=""language-bash"">kubectl get agents -A                 # List all switches (agents)
kubectl get connections -A            # List server connections
kubectl get vpcattachments -A         # List VPC-to-server attachments
</code></pre>
<p><strong>Event inspection:</strong></p>
<pre><code class=""language-bash"">kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20   # Recent events
kubectl get events --field-selector involvedObject.name=&lt;name&gt;  # Events for specific resource
kubectl describe &lt;resource-type&gt; &lt;name&gt;                     # Events in context
</code></pre>
<hr>
<h3>Gitea Quick Reference</h3>
<p><strong>Navigation:</strong></p>
<ul>
<li><strong>Repository URL:</strong> <code>http://localhost:3001/student/hedgehog-config</code></li>
<li><strong>Commits:</strong> Click &quot;Commits&quot; tab to view history</li>
<li><strong>Diff View:</strong> Click on any commit to see changes</li>
<li><strong>File Browser:</strong> Click files/folders to navigate</li>
</ul>
<p><strong>What to use Gitea for:</strong></p>
<ul>
<li>View configuration history (commits)</li>
<li>See what changed between versions (diffs)</li>
<li>Audit who made which changes (author and timestamp)</li>
<li>Browse repository structure (find where to create new files)</li>
</ul>
<hr>
<h3>Grafana Quick Reference</h3>
<p><strong>Access:</strong> <code>http://localhost:3000</code> (admin/prom-operator)</p>
<p><strong>Dashboard URLs:</strong></p>
<ol>
<li><p><strong>Fabric Dashboard</strong> (daily health check)</p>
<ul>
<li>URL: <code>/d/ab831ceb-cf5c-474a-b7e9-83dcd075c218/fabric</code></li>
<li>Use for: Overall fabric health, switch status, VPC count</li>
</ul>
</li>
<li><p><strong>Platform Dashboard</strong> (control plane health)</p>
<ul>
<li>URL: <code>/d/f8a648b9-5510-49ca-9273-952ba6169b7b/platform</code></li>
<li>Use for: Fabricator errors, control node CPU/memory, API latency</li>
</ul>
</li>
<li><p><strong>Interfaces Dashboard</strong> (port health)</p>
<ul>
<li>URL: <code>/d/a5e5b12d-b340-4753-8f83-af8d54304822/interfaces</code></li>
<li>Use for: Interface status, traffic rates, error counters</li>
</ul>
</li>
<li><p><strong>Logs Dashboard</strong> (syslog aggregation)</p>
<ul>
<li>URL: <code>/d/c42a51e5-86a8-42a0-b1c9-d1304ae655bc/logs</code></li>
<li>Use for: Searching logs, troubleshooting, audit trail</li>
</ul>
</li>
<li><p><strong>Node Exporter Dashboard</strong> (system metrics)</p>
<ul>
<li>URL: <code>/d/rYdddlPWA/node-exporter-full-2</code></li>
<li>Use for: CPU, memory, disk usage on switches</li>
</ul>
</li>
<li><p><strong>Switch CRM Dashboard</strong> (capacity monitoring)</p>
<ul>
<li>URL: <code>/d/fb08315c-cabb-4da7-9db9-2e17278f1781/switch-critical-resources</code></li>
<li>Use for: TCAM usage, route table capacity, capacity planning</li>
</ul>
</li>
</ol>
<hr>
<h3>Troubleshooting Flow Reference</h3>
<p><strong>Standard flow for &quot;VPC not working&quot; issues:</strong></p>
<ol>
<li><strong>Gitea</strong> - Verify configuration is correct in Git</li>
<li><strong>kubectl</strong> - Check if VPC exists and has error events</li>
<li><strong>Grafana</strong> - Confirm switches are healthy and operational</li>
</ol>
<p><strong>Quick checks:</strong></p>
<pre><code class=""language-bash""># 1. Check VPC exists and has no errors
kubectl describe vpc &lt;name&gt; | grep -A 5 Events

# 2. Check switch health
kubectl get agents -A

# 3. Check Grafana Fabric Dashboard
# (open in browser)
</code></pre>
<hr>
<h3>Related Documentation</h3>
<ul>
<li><a href=""../../../network-like-hyperscaler/hedgehogLearningPhilosophy.md"">Hedgehog Learning Philosophy</a></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md"">CRD Reference</a></li>
<li><a href=""../../../network-like-hyperscaler/research/OBSERVABILITY.md"">Observability Guide</a></li>
<li><a href=""../../../network-like-hyperscaler/MODULE_DEPENDENCY_GRAPH.md"">Module Dependency Graph</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve mastered the three-interface operational model. Ready to continue to Module 1.4: Course 1 Recap.</p>
",103,20,,,"hedgehog,interfaces,kubectl,gitops,observability"
197744345088,Welcome to Fabric Operations,fabric-operations-welcome,[object Object],"Kick off the Hedgehog Fabric Operator journey, explore the vlab, and build confidence with kubectl-driven operations.","<p><strong>Course:</strong> Course 1 - Foundations &amp; Interfaces
<strong>Duration:</strong> 15 minutes
<strong>Prerequisites:</strong> Basic command-line familiarity, general understanding of networks</p>
<hr>
<h2>Introduction</h2>
<h3>A Day in the Life</h3>
<p>You&#39;re a Fabric Operator at a growing company. This morning, your team needs to onboard three new application servers to the production network. In traditional networking, this might mean: scheduling a maintenance window, manually configuring VLANs on multiple switches, updating routing tables, testing connectivity, and hoping nothing breaks.</p>
<p>With Hedgehog Fabric, you&#39;ll define the desired state in a few lines of YAML, apply it with kubectl, and let the fabric controller handle the rest. The switches configure themselves. The routes populate automatically. The connectivity validates itself.</p>
<p>Sound too good to be true? That&#39;s what hyperscalers figured out years ago. Now you can do it too.</p>
<h3>What You&#39;ll Learn</h3>
<p>This pathway teaches you to <strong>operate</strong> a Hedgehog fabric with confidence. Not to design fabrics from scratch (that&#39;s a different course), but to:</p>
<ul>
<li>Provision network resources (VPCs, server attachments)</li>
<li>Validate connectivity</li>
<li>Monitor fabric health</li>
<li>Troubleshoot common issues</li>
<li>Know when to escalate to support</li>
</ul>
<p>You&#39;ll use <strong>Kubernetes-native tools</strong> (kubectl) to manage a real network fabric. If you know Kubernetes, you&#39;ll feel at home. If you come from traditional networking, you&#39;ll discover a cleaner, safer way to manage network state.</p>
<h3>Learning Objectives</h3>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Describe the Hedgehog Fabric Operator role</strong> - Understand day-to-day responsibilities vs. design/architecture tasks</li>
<li><strong>Articulate the confidence-first learning approach</strong> - Explain why we focus on common operations before edge cases</li>
<li><strong>Navigate the Hedgehog vlab environment</strong> - Use kubectl to view fabric topology and resources</li>
<li><strong>Identify the three-tier resource hierarchy</strong> - Recognize Switches/Servers (wiring) → VPCs → VPCAttachments</li>
<li><strong>Demonstrate kubectl basics</strong> - Execute get and describe commands to inspect fabric state</li>
</ol>
<h3>Setting Expectations</h3>
<p>This pathway follows a proven approach designed to get you productive quickly:</p>
<ul>
<li><strong>Focus on what matters most:</strong> We&#39;ll teach the 80% of tasks you&#39;ll do daily, not the 20% of edge cases you&#39;ll rarely encounter</li>
<li><strong>Confidence before comprehensiveness:</strong> You&#39;ll master core operations before diving into advanced scenarios</li>
<li><strong>Hands-on, always:</strong> Every module includes labs in a real Hedgehog environment</li>
<li><strong>Support is strength:</strong> You&#39;ll learn when and how to escalate effectively (it&#39;s a best practice, not a weakness)</li>
</ul>
<hr>
<h2>Core Concepts</h2>
<h3>Concept 1: The Fabric Operator Role</h3>
<p>You&#39;re not designing the network from scratch—that&#39;s already done. The fabric (switches, servers, connections) is wired and operational.</p>
<p><strong>Your responsibilities:</strong></p>
<ul>
<li><strong>Provision:</strong> Create VPCs, attach servers to networks</li>
<li><strong>Validate:</strong> Confirm connectivity works as expected</li>
<li><strong>Monitor:</strong> Watch fabric health, detect anomalies</li>
<li><strong>Troubleshoot:</strong> Diagnose issues using kubectl and events</li>
<li><strong>Escalate:</strong> Package diagnostics and engage support when needed</li>
</ul>
<p><strong>NOT your job (in this role):</strong></p>
<ul>
<li>Physical switch installation</li>
<li>Initial fabric wiring diagram design</li>
<li>Low-level BGP/EVPN configuration</li>
<li>Hardware troubleshooting</li>
</ul>
<p>Think of it like this: <strong>You manage applications in Kubernetes without configuring kubelet on every node.</strong> Similarly, you manage network resources without manually configuring SONiC on every switch.</p>
<h3>Concept 2: Kubernetes-Native Network Management</h3>
<p>Hedgehog uses <strong>Custom Resource Definitions (CRDs)</strong> to represent network concepts:</p>
<pre><code>Wiring Layer (Physical)
├─ Switch: Physical switch in the fabric
├─ Server: Physical server connected to switches
└─ Connection: How servers connect to switches (MCLAG, ESLAG, etc.)

VPC Layer (Virtual Networks)
├─ VPC: Virtual Private Cloud with isolated subnets
├─ VPCAttachment: Binds a VPC to a server connection
└─ VPCPeering: Connects two VPCs

External Layer (Outside Connectivity)
├─ External: External network definition
├─ ExternalAttachment: Border leaf connection
└─ ExternalPeering: VPC to external routing
</code></pre>
<p>You&#39;ll use <strong>kubectl</strong> just like managing Kubernetes resources:</p>
<pre><code class=""language-bash"">kubectl get vpcs                    # List all VPCs
kubectl describe vpc my-vpc         # Get VPC details
kubectl apply -f vpc.yaml           # Create/update VPC
kubectl get events                  # See what happened
</code></pre>
<h3>Concept 3: Declarative, Self-Healing Infrastructure</h3>
<p><strong>Traditional networking:</strong> Log into each switch, type commands, hope you didn&#39;t make a typo.</p>
<p><strong>Hedgehog approach:</strong></p>
<ol>
<li><strong>Declare desired state</strong> (YAML): &quot;I want VPC &#39;production&#39; with subnet 10.10.1.0/24&quot;</li>
<li><strong>Apply it</strong> (kubectl): The fabric controller receives your intent</li>
<li><strong>Reconciliation happens</strong> (automatic): Controllers configure switches to match desired state</li>
<li><strong>Status reflects reality</strong> (observable): kubectl shows current state and any issues</li>
</ol>
<p>If a switch reboots? The fabric controller automatically reapplies configuration. No manual intervention.</p>
<h3>Concept 4: The Learning Philosophy</h3>
<p>This pathway follows a proven approach:</p>
<ol>
<li><strong>Train for reality, not rote:</strong> You&#39;ll learn workflows, not memorize commands</li>
<li><strong>Focus on what matters:</strong> Common operations, not rare edge cases</li>
<li><strong>Confidence first:</strong> Small wins build competence over time</li>
<li><strong>Learn by doing:</strong> Hands-on labs in every module</li>
<li><strong>Support as strength:</strong> We&#39;ll teach you how to escalate effectively</li>
</ol>
<p>You don&#39;t need to become a networking expert or Kubernetes guru overnight. You need to be <strong>confident and competent</strong> in the core operations. Everything else builds from there.</p>
<hr>
<h2>Hands-On Lab</h2>
<h3>Lab Title: Explore Your Fabric Environment</h3>
<p><strong>Overview:</strong>
You&#39;ll use kubectl to explore the Hedgehog vlab environment, viewing switches, servers, and connections. This is your first hands-on experience—designed to build confidence through successful exploration.</p>
<p><strong>Environment:</strong></p>
<ul>
<li>Hedgehog vlab with default spine-leaf topology</li>
<li>2 spine switches, 5 leaf switches, 10 servers</li>
<li>kubectl already configured and ready to use</li>
</ul>
<p><strong>Important:</strong> All commands in this lab are read-only. You can&#39;t break anything—we&#39;re just exploring.</p>
<hr>
<h3>Task 1: Verify Environment Access</h3>
<p><strong>Objective:</strong> Confirm kubectl can communicate with the Hedgehog control plane</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># Check cluster access
kubectl cluster-info
</code></pre>
<p><strong>Expected output (similar to):</strong></p>
<pre><code>Kubernetes control plane is running at https://127.0.0.1:6443
CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy
</code></pre>
<p><strong>Validation:</strong></p>
<pre><code class=""language-bash""># Verify you can access the fab namespace
kubectl get pods -n fab
</code></pre>
<p><strong>Expected:</strong> Several pods running (fabric-ctrl, fabric-boot, fabricator-ctrl, etc.)</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ kubectl cluster-info shows control plane URL</li>
<li>✅ Pods in fab namespace are Running</li>
</ul>
<hr>
<h3>Task 2: View Fabric Topology</h3>
<p><strong>Objective:</strong> Explore the physical fabric resources (switches, servers, connections)</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># List all switches in the fabric
kubectl get switches
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAME       PROFILE   ROLE          DESCR           GROUPS        AGE
leaf-01    vs        server-leaf   VS-01 MCLAG 1   [&quot;mclag-1&quot;]   11h
leaf-02    vs        server-leaf   VS-02 MCLAG 1   [&quot;mclag-1&quot;]   11h
leaf-03    vs        server-leaf   VS-03 ESLAG 1   [&quot;eslag-1&quot;]   11h
leaf-04    vs        server-leaf   VS-04 ESLAG 1   [&quot;eslag-1&quot;]   11h
leaf-05    vs        server-leaf   VS-05                         11h
spine-01   vs        spine         VS-06                         11h
spine-02   vs        spine         VS-07                         11h
</code></pre>
<p><strong>Count the switches:</strong></p>
<ul>
<li>How many spine switches? _______ (Answer: 2)</li>
<li>How many leaf switches? _______ (Answer: 5)</li>
</ul>
<p><strong>Note:</strong> Look at the ROLE column to distinguish spine from server-leaf switches.</p>
<pre><code class=""language-bash""># List all servers
kubectl get servers
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAME        TYPE   DESCR                        AGE
server-01          S-01 MCLAG leaf-01 leaf-02   11h
server-02          S-02 MCLAG leaf-01 leaf-02   11h
server-03          S-03 Unbundled leaf-01       11h
server-04          S-04 Bundled leaf-02         11h
server-05          S-05 ESLAG leaf-03 leaf-04   11h
server-06          S-06 ESLAG leaf-03 leaf-04   11h
server-07          S-07 Unbundled leaf-03       11h
server-08          S-08 Bundled leaf-04         11h
server-09          S-09 Unbundled leaf-05       11h
server-10          S-10 Bundled leaf-05         11h
</code></pre>
<p><strong>Count the servers:</strong> _______ (Answer: 10)</p>
<p><strong>Note:</strong> The DESCR column shows how each server connects to the fabric—MCLAG, ESLAG, Bundled, or Unbundled.</p>
<pre><code class=""language-bash""># View connections (how servers connect to switches)
kubectl get connections
</code></pre>
<p><strong>Expected output shows various connection types:</strong></p>
<ul>
<li>MCLAG (multi-chassis link aggregation)</li>
<li>ESLAG (EVPN-based ESI LAG)</li>
<li>Bundled (port channel to single switch)</li>
<li>Unbundled (single link)</li>
<li>Fabric (spine-to-leaf interconnects)</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can list switches and count them correctly (7 total)</li>
<li>✅ Can list servers (10 total)</li>
<li>✅ Can view connections showing various types</li>
</ul>
<hr>
<h3>Task 3: Inspect a Specific Resource</h3>
<p><strong>Objective:</strong> Use kubectl describe to view detailed information</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># Get detailed information about leaf-01
kubectl describe switch leaf-01
</code></pre>
<p><strong>What to look for:</strong></p>
<p>The output contains many fields. Focus on these key ones:</p>
<ul>
<li><strong>Role:</strong> Shows the switch&#39;s function (server-leaf or spine)</li>
<li><strong>Groups:</strong> Shows redundancy groups (e.g., mclag-1 means paired with leaf-02)</li>
<li><strong>Redundancy:</strong> Shows the redundancy type (mclag, eslag, or none)</li>
<li><strong>ASN:</strong> The switch&#39;s BGP autonomous system number</li>
<li><strong>Profile:</strong> vs (virtual switch in this lab)</li>
</ul>
<p><strong>You don&#39;t need to understand every field</strong>—we&#39;ll explore more in later modules. For now, practice finding specific information.</p>
<p><strong>Questions to answer:</strong></p>
<ol>
<li>What is the role of leaf-01? _______ (Answer: server-leaf)</li>
<li>Is leaf-01 part of a redundancy group? _______ (Answer: Yes, mclag-1)</li>
</ol>
<pre><code class=""language-bash""># Inspect server-01
kubectl describe server server-01
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li><strong>Description:</strong> Shows which switches it connects to</li>
<li><strong>Connection type:</strong> MCLAG, ESLAG, etc.</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can describe a switch and identify its role</li>
<li>✅ Can describe a server and see its connections</li>
<li>✅ Understand that describe shows more detail than get</li>
</ul>
<hr>
<h3>Task 4: Explore Events (Optional)</h3>
<p><strong>Objective:</strong> See Kubernetes events for fabric resources</p>
<p><strong>Steps:</strong></p>
<pre><code class=""language-bash""># View recent events in the default namespace
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20
</code></pre>
<p><strong>What are events?</strong></p>
<p>Events show what the fabric controller is doing:</p>
<ul>
<li>Reconciling resources</li>
<li>Configuration changes</li>
<li>Status updates</li>
</ul>
<p><strong>Note on Events:</strong></p>
<blockquote>
<p>If you don&#39;t see recent events, that&#39;s normal! Kubernetes events expire after about an hour. In active environments (like when creating VPCs or troubleshooting issues), you&#39;ll see events showing what the fabric controller is doing. We&#39;ll use events extensively in later modules during hands-on provisioning tasks.</p>
</blockquote>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can view events (even if the list is empty or you don&#39;t understand all of them yet)</li>
</ul>
<hr>
<h3>Lab Summary</h3>
<p><strong>What you did:</strong></p>
<ul>
<li>✅ Verified kubectl access to Hedgehog control plane</li>
<li>✅ Explored fabric topology (switches, servers, connections)</li>
<li>✅ Inspected detailed resource information with describe</li>
<li>✅ Observed Kubernetes events</li>
</ul>
<p><strong>What you learned:</strong></p>
<ul>
<li>kubectl is your primary tool for fabric management</li>
<li>Hedgehog represents infrastructure as Kubernetes resources</li>
<li>The fabric has physical resources (switches, servers) and virtual resources (VPCs, which you&#39;ll explore next)</li>
<li>You can inspect state without making any changes (read-only exploration builds confidence)</li>
</ul>
<p><strong>Key takeaway:</strong> You just successfully navigated a production-like fabric environment using only kubectl. No switch CLI, no manual configuration, no fear of breaking things. This is how hyperscalers operate at scale.</p>
<hr>
<h2>Wrap-Up</h2>
<h3>Key Takeaways</h3>
<ol>
<li><strong>Your role:</strong> Fabric Operator managing day-to-day network operations</li>
<li><strong>Your tool:</strong> kubectl to manage Kubernetes-native network resources</li>
<li><strong>Your approach:</strong> Declarative, confidence-building, focused on common tasks</li>
<li><strong>Your support:</strong> Escalation is a strength, not a weakness</li>
</ol>
<h3>Preview of Module 1.2</h3>
<p>Next, you&#39;ll dive deeper into <strong>how Hedgehog works under the hood</strong>: CRD reconciliation, the controller pattern, and how your kubectl commands become switch configurations. You&#39;ll understand the &quot;magic&quot; so it&#39;s not magic anymore—just well-designed automation.</p>
<hr>
<h2>Assessment</h2>
<h3>Quiz Questions</h3>
<p><strong>Question 1: Multiple Choice</strong></p>
<p>What is the primary role of a Hedgehog Fabric Operator?</p>
<ul>
<li>A) Design network topologies and select switch hardware</li>
<li>B) Provision VPCs, validate connectivity, and monitor fabric health</li>
<li>C) Manually configure BGP peering on each switch</li>
<li>D) Write custom Kubernetes controllers for network automation</li>
</ul>
<p><strong>Correct Answer:</strong> B</p>
<p><strong>Explanation:</strong>
Fabric Operators manage day-to-day network operations using kubectl to provision resources (VPCs, attachments), validate connectivity, and monitor health. Physical design (A), low-level protocol configuration (C), and controller development (D) are outside the operator role scope. This pathway focuses on <strong>operating</strong> an existing fabric, not designing or implementing it.</p>
<hr>
<p><strong>Question 2: Scenario-Based</strong></p>
<p>You need to view all switches in the fabric and identify which ones are spines vs. leaves. What kubectl command would you use?</p>
<p><strong>Answer:</strong></p>
<pre><code class=""language-bash"">kubectl get switches
</code></pre>
<p>This lists all switches with their ROLE field showing &quot;spine&quot; or &quot;server-leaf&quot;.</p>
<p><strong>Rubric:</strong></p>
<ul>
<li>Full credit: <code>kubectl get switches</code> (exact or close variation)</li>
<li>Partial credit: Mentions kubectl and switches but wrong syntax</li>
<li>No credit: Suggests logging into switches or using non-kubectl tools</li>
</ul>
<hr>
<p><strong>Question 3: True/False</strong></p>
<p>True or False: In Hedgehog, you must manually log into each switch to configure VLANs when creating a new VPC.</p>
<p><strong>Answer:</strong> False</p>
<p><strong>Explanation:</strong>
Hedgehog uses declarative management. You define the VPC in YAML and apply it with kubectl. The fabric controller automatically configures all necessary switches to realize the desired state. You never manually configure switches for routine operations—that&#39;s the whole point of the abstraction.</p>
<hr>
<p><strong>Question 4: Multiple Choice</strong></p>
<p>According to the Hedgehog learning philosophy, which statement is correct?</p>
<ul>
<li>A) You must master all edge cases before attempting basic operations</li>
<li>B) Focus on common, high-impact tasks to build confidence and immediate productivity</li>
<li>C) Avoid using support—figure everything out independently</li>
<li>D) Memorize all kubectl commands before trying labs</li>
</ul>
<p><strong>Correct Answer:</strong> B</p>
<p><strong>Explanation:</strong>
The learning philosophy emphasizes &quot;Focus on What Matters Most&quot; and &quot;Confidence Before Comprehensiveness.&quot; You&#39;ll learn the 80% of operations you&#39;ll do daily (B), not rare edge cases (A). Support is encouraged when needed (C is wrong). Learning by doing beats memorization (D is wrong).</p>
<hr>
<p><strong>Question 5: Practical - Open Ended</strong></p>
<p>Based on what you explored in the lab, how many total switches are in the vlab environment, and how are they split between spines and leaves?</p>
<p><strong>Answer:</strong>
7 total switches: 2 spines and 5 leaves</p>
<p><strong>Rubric:</strong></p>
<ul>
<li>Full credit: Correct numbers (7 total, 2 spines, 5 leaves)</li>
<li>Partial credit: Correct total but wrong breakdown, or vice versa</li>
<li>No credit: Incorrect numbers</li>
</ul>
<hr>
<h3>Practical Assessment</h3>
<p><strong>Task:</strong> Using only kubectl commands, determine which switches server-05 is connected to.</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Uses <code>kubectl describe server server-05</code> or equivalent</li>
<li>✅ Correctly identifies the connected switches from the description</li>
<li>✅ Can explain whether it&#39;s MCLAG, ESLAG, or another connection type</li>
</ul>
<p><strong>Expected Process:</strong></p>
<pre><code class=""language-bash"">kubectl describe server server-05
# Output shows: &quot;S-05 ESLAG leaf-03 leaf-04&quot;
# Answer: Connected to leaf-03 and leaf-04 via ESLAG
</code></pre>
<hr>
<h2>Reference</h2>
<h3>Hedgehog CRDs Used in This Module</h3>
<ul>
<li><strong>Switch</strong> - Physical switch in the fabric<ul>
<li>View: <code>kubectl get switches</code></li>
<li>Inspect: <code>kubectl describe switch &lt;name&gt;</code></li>
</ul>
</li>
<li><strong>Server</strong> - Physical server connected to switches<ul>
<li>View: <code>kubectl get servers</code></li>
<li>Inspect: <code>kubectl describe server &lt;name&gt;</code></li>
</ul>
</li>
<li><strong>Connection</strong> - Physical and logical connections between devices<ul>
<li>View: <code>kubectl get connections</code></li>
<li>Shows MCLAG, ESLAG, bundled, unbundled, fabric types</li>
</ul>
</li>
</ul>
<h3>kubectl Commands Reference</h3>
<p><strong>Cluster access:</strong></p>
<pre><code class=""language-bash"">kubectl cluster-info              # Verify cluster connectivity
kubectl get pods -n fab           # View fabric control plane pods
</code></pre>
<p><strong>Resource listing:</strong></p>
<pre><code class=""language-bash"">kubectl get switches              # List all switches
kubectl get servers               # List all servers
kubectl get connections           # List all connections
</code></pre>
<p><strong>Resource inspection:</strong></p>
<pre><code class=""language-bash"">kubectl describe switch &lt;name&gt;    # Get detailed switch information
kubectl describe server &lt;name&gt;    # Get detailed server information
</code></pre>
<p><strong>Event viewing:</strong></p>
<pre><code class=""language-bash"">kubectl get events --sort-by=&#39;.lastTimestamp&#39;  # View recent events
</code></pre>
<h3>Related Documentation</h3>
<ul>
<li><a href=""../../../network-like-hyperscaler/hedgehogLearningPhilosophy.md"">Hedgehog Learning Philosophy</a></li>
<li><a href=""../../../network-like-hyperscaler/research/CRD_REFERENCE.md"">CRD Reference</a></li>
<li><a href=""../../../network-like-hyperscaler/MODULE_DEPENDENCY_GRAPH.md"">Module Dependency Graph</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> Ready to continue to Module 1.2: How Hedgehog Works.</p>
",101,15,,,"hedgehog,fabric,onboarding,kubernetes,networking"
198578603876,Connectivity Validation,fabric-operations-connectivity-validation,[object Object],"Validate VPC and VPCAttachment deployments with hands-on testing. Learn to verify connectivity, interpret events, and troubleshoot issues.","<h2>Introduction</h2>
<p>In Modules 2.1 and 2.2, you provisioned infrastructure—creating the <code>web-app-prod</code> VPC with two subnets and attaching two servers (<code>server-01</code> and <code>server-05</code>). You followed the declarative workflow: write YAML, apply it, and trust that the fabric controller handles the rest.</p>
<p>But trust alone isn&#39;t enough in production. Before handing off to the application team, you need to <strong>validate</strong> that everything works as expected. Declarative infrastructure doesn&#39;t eliminate the need for verification—it just changes how you verify.</p>
<p>In this module, you&#39;ll learn validation workflows that Fabric Operators use daily: inspecting VPC configurations, checking VPCAttachment status, reading reconciliation events, exploring Agent CRDs for switch-level state, and building a validation checklist you can use for every deployment.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Validate VPC deployments</strong> - Verify VPC configuration matches desired state</li>
<li><strong>Validate VPCAttachment status</strong> - Confirm server-to-VPC connectivity is operational</li>
<li><strong>Interpret reconciliation events</strong> - Read events to understand what happened during deployment</li>
<li><strong>Use Agent CRDs for validation</strong> - Inspect switch-level configuration state</li>
<li><strong>Troubleshoot deployment issues</strong> - Diagnose and fix common validation failures</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 2.1 completion (VPC Provisioning Essentials)</li>
<li>Module 2.2 completion (VPC Attachments)</li>
<li>Existing <code>web-app-prod</code> VPC with 2 VPCAttachments (server-01, server-05)</li>
<li>kubectl access to Hedgehog fabric</li>
<li>Understanding of Kubernetes events and CRDs</li>
</ul>
<h2>Scenario: Pre-Production Validation</h2>
<p>You&#39;ve deployed the <code>web-app-prod</code> VPC and attached <code>server-01</code> (web tier) and <code>server-05</code> (worker tier). Before handing off to the application team, you need to validate the network connectivity is operational. You&#39;ll verify VPC status, check VPCAttachment reconciliation, inspect Agent CRDs to see switch-level configuration, and run validation tests to ensure everything works as expected. This validation workflow is critical—it catches configuration issues before they impact production traffic.</p>
<h2>Lab Steps</h2>
<h3>Step 1: Review Deployed Infrastructure</h3>
<p><strong>Objective:</strong> Verify expected resources exist and understand current state</p>
<p>Before validating configurations in detail, confirm all expected resources are present.</p>
<p>List VPCs in the fabric:</p>
<pre><code class=""language-bash"">kubectl get vpcs
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME           AGE
web-app-prod   25m
</code></pre>
<p>List VPCAttachments:</p>
<pre><code class=""language-bash"">kubectl get vpcattachments
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME                       AGE
server-01-web-servers      15m
server-05-worker-nodes     12m
</code></pre>
<p>Quick status check of all resources:</p>
<pre><code class=""language-bash""># View VPC with basic info
kubectl get vpc web-app-prod

# View VPCAttachments with basic info
kubectl get vpcattachments
</code></pre>
<p><strong>Verification checklist:</strong></p>
<ul>
<li>✅ VPC <code>web-app-prod</code> exists</li>
<li>✅ VPCAttachment <code>server-01-web-servers</code> exists</li>
<li>✅ VPCAttachment <code>server-05-worker-nodes</code> exists</li>
<li>✅ No obvious errors in basic listing</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC web-app-prod exists</li>
<li>✅ Both VPCAttachments exist (server-01, server-05)</li>
<li>✅ No obvious errors in kubectl get output</li>
</ul>
<h3>Step 2: Validate VPC Configuration</h3>
<p><strong>Objective:</strong> Confirm VPC configuration matches desired state</p>
<p>Use <code>kubectl describe</code> to view comprehensive VPC details:</p>
<pre><code class=""language-bash"">kubectl describe vpc web-app-prod
</code></pre>
<p><strong>Key sections to review in the output:</strong></p>
<p><strong>1. Spec section (what you requested):</strong></p>
<p>Look for:</p>
<ul>
<li><strong>Subnets</strong>: Two subnets should be present (<code>web-servers</code> and <code>worker-nodes</code>)</li>
<li><strong>web-servers subnet</strong>:<ul>
<li>Subnet: 10.10.10.0/24</li>
<li>Gateway: 10.10.10.1</li>
<li>VLAN: 1010</li>
<li>No DHCP configuration (static IP subnet)</li>
</ul>
</li>
<li><strong>worker-nodes subnet</strong>:<ul>
<li>Subnet: 10.10.20.0/24</li>
<li>Gateway: 10.10.20.1</li>
<li>VLAN: 1020</li>
<li>DHCP enabled with range 10.10.20.10-250</li>
</ul>
</li>
</ul>
<p>View subnets in detail:</p>
<pre><code class=""language-bash""># Extract subnet configuration
kubectl get vpc web-app-prod -o yaml | grep -A 20 &quot;subnets:&quot;
</code></pre>
<p><strong>2. Events section (at the bottom):</strong></p>
<pre><code class=""language-bash""># View VPC-specific events
kubectl get events --field-selector involvedObject.name=web-app-prod --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p>Look for:</p>
<ul>
<li><code>Normal  Created</code> - VPC created successfully</li>
<li><code>Normal  Reconciling</code> - Fabric controller processing</li>
<li><code>Normal  Ready</code> - Reconciliation complete</li>
<li><strong>No warning or error events</strong></li>
</ul>
<p>Verify DHCP configuration for worker-nodes:</p>
<pre><code class=""language-bash"">kubectl get vpc web-app-prod -o jsonpath=&#39;{.spec.subnets.worker-nodes.dhcp}&#39; | jq
</code></pre>
<p>Expected output:</p>
<pre><code class=""language-json"">{
  &quot;enable&quot;: true,
  &quot;range&quot;: {
    &quot;start&quot;: &quot;10.10.20.10&quot;,
    &quot;end&quot;: &quot;10.10.20.250&quot;
  }
}
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Subnets configured correctly (CIDRs, gateways, VLANs)</li>
<li>✅ VLANs assigned as expected (1010, 1020)</li>
<li>✅ DHCP enabled for worker-nodes with correct range</li>
<li>✅ No error events in VPC history</li>
</ul>
<h3>Step 3: Validate VPCAttachment Status</h3>
<p><strong>Objective:</strong> Confirm VPCAttachments reconciled successfully</p>
<p>Describe both VPCAttachments to review their status:</p>
<pre><code class=""language-bash""># Check server-01 attachment (MCLAG, static IP subnet)
kubectl describe vpcattachment server-01-web-servers
</code></pre>
<p><strong>Key items to verify:</strong></p>
<ul>
<li><p><strong>Spec section</strong>:</p>
<ul>
<li>Connection: <code>server-01--mclag--leaf-01--leaf-02</code> (correct connection reference)</li>
<li>Subnet: <code>web-app-prod/web-servers</code> (correct VPC/subnet format)</li>
</ul>
</li>
<li><p><strong>Events section</strong>:</p>
<ul>
<li>Look for <code>Normal  Created</code>, <code>Normal  Reconciling</code>, <code>Normal  Applied</code></li>
<li>No warning or error events</li>
<li>Events should indicate configuration was applied to switches</li>
</ul>
</li>
</ul>
<p>Check the second VPCAttachment:</p>
<pre><code class=""language-bash""># Check server-05 attachment (ESLAG, DHCP subnet)
kubectl describe vpcattachment server-05-worker-nodes
</code></pre>
<p>View events for both attachments together:</p>
<pre><code class=""language-bash""># View all VPCAttachment events chronologically
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | grep vpcattachment
</code></pre>
<p>Verify connection references are correct:</p>
<pre><code class=""language-bash""># Verify server-01 connection exists and matches
kubectl get connection server-01--mclag--leaf-01--leaf-02 -n fab

# Verify server-05 connection exists and matches
kubectl get connection server-05--eslag--leaf-03--leaf-04 -n fab
</code></pre>
<p>Expected output for each: Connection details showing the server-to-switch wiring.</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Both attachments show successful reconciliation</li>
<li>✅ Events indicate &quot;Applied&quot; or &quot;Ready&quot; status</li>
<li>✅ No error events</li>
<li>✅ Connection references match expected values (MCLAG for server-01, ESLAG for server-05)</li>
</ul>
<h3>Step 4: Inspect Agent CRDs (Switch-Level Validation)</h3>
<p><strong>Objective:</strong> Verify switch-level configuration state</p>
<p>Agent CRDs are the <strong>source of truth</strong> for switch state. Each switch has an Agent CRD that shows:</p>
<ul>
<li><strong>Spec</strong>: What the fabric controller wants configured</li>
<li><strong>Status</strong>: What the switch agent has actually applied</li>
</ul>
<p><strong>Identify switches involved in connections:</strong></p>
<ul>
<li><strong>server-01</strong> (MCLAG): Connected to <code>leaf-01</code> and <code>leaf-02</code></li>
<li><strong>server-05</strong> (ESLAG): Connected to <code>leaf-03</code> and <code>leaf-04</code></li>
</ul>
<p>List all switches:</p>
<pre><code class=""language-bash"">kubectl get switches -n fab
</code></pre>
<p>View Agent CRD for leaf-01:</p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o yaml
</code></pre>
<p><strong>What to look for in Agent CRD:</strong></p>
<p>The output is extensive, but focus on these areas:</p>
<p><strong>1. Spec section</strong> (desired configuration):</p>
<ul>
<li>Look for VLAN configuration on server-facing ports</li>
<li>Check for VXLAN tunnel configuration</li>
<li>Verify BGP EVPN route configuration</li>
</ul>
<p><strong>2. Status section</strong> (applied configuration):</p>
<ul>
<li>Check if status reflects spec (configuration applied successfully)</li>
<li>Look for any error messages or failed applications</li>
</ul>
<p>View specific switch port configuration (example):</p>
<pre><code class=""language-bash""># Get leaf-01 Agent CRD and look for server-01 port configuration
kubectl get agent leaf-01 -n fab -o yaml | grep -A 10 &quot;E1/5&quot;
</code></pre>
<p>(Note: Port names may vary based on your environment)</p>
<p><strong>Understanding Agent CRDs:</strong></p>
<ul>
<li><strong>Spec = Intent</strong>: What the fabric controller computed for this switch</li>
<li><strong>Status = Reality</strong>: What the switch agent successfully applied</li>
<li><strong>Mismatch = Problem</strong>: If spec and status diverge, configuration failed</li>
</ul>
<p>You don&#39;t need to understand every field in the Agent CRD. The key insight: if VPCAttachment events show success, the Agent CRDs should reflect the configuration.</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can view Agent CRDs for relevant switches (leaf-01, leaf-02, leaf-03, leaf-04)</li>
<li>✅ Agent CRDs contain VPC-related configuration in spec</li>
<li>✅ Understand Agent CRD is source of truth for switch state</li>
</ul>
<h3>Step 5: Validation Methodology (Conceptual)</h3>
<p><strong>Objective:</strong> Understand end-to-end validation approach</p>
<p>In a vlab environment, servers are simulated, so actual ping tests may not be possible. However, understanding the <strong>validation methodology</strong> is critical for production deployments.</p>
<p><strong>Validation layers (from abstract to physical):</strong></p>
<ol>
<li><strong>VPC validation</strong>: Does the VPC exist with correct subnets?</li>
<li><strong>VPCAttachment validation</strong>: Did VPCAttachments reconcile successfully?</li>
<li><strong>Agent CRD validation</strong>: Are switches configured correctly?</li>
<li><strong>Physical connectivity validation</strong>: Can servers actually communicate?</li>
</ol>
<p><strong>Production connectivity tests (when servers are real):</strong></p>
<p><strong>For server-01 (static IP subnet):</strong></p>
<pre><code class=""language-bash""># SSH to server-01
# Manually configure IP (since it&#39;s a static subnet)
sudo ip addr add 10.10.10.10/24 dev eth0
sudo ip route add default via 10.10.10.1

# Test gateway reachability
ping -c 3 10.10.10.1

# Test connectivity to another server in the same VPC (if applicable)
ping -c 3 10.10.10.20
</code></pre>
<p><strong>For server-05 (DHCP subnet):</strong></p>
<pre><code class=""language-bash""># SSH to server-05
# Request DHCP IP
sudo dhclient eth0

# Verify IP received in DHCP range
ip addr show eth0

# Expected: IP in range 10.10.20.10-250

# Test gateway reachability
ping -c 3 10.10.20.1
</code></pre>
<p><strong>In vlab environment:</strong></p>
<p>Since servers are simulated, focus on:</p>
<ul>
<li>kubectl describe output showing no errors</li>
<li>Event inspection showing successful reconciliation</li>
<li>Agent CRD verification showing configuration applied</li>
</ul>
<p><strong>Validation checklist summary:</strong></p>
<ul>
<li>✅ VPC exists with correct subnets</li>
<li>✅ VPCAttachments reconciled without errors</li>
<li>✅ Events show successful configuration application</li>
<li>✅ Agent CRDs contain expected configuration</li>
<li>✅ (Production only) Servers can reach gateway and communicate</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Understand validation methodology</li>
<li>✅ Know what tests to run in production</li>
<li>✅ Can explain expected outcomes for connectivity tests</li>
</ul>
<h2>Concepts &amp; Deep Dive</h2>
<h3>Validation Philosophy</h3>
<p><strong>Why validate declarative infrastructure?</strong></p>
<p>Declarative systems like Kubernetes promise self-healing and correctness, but they&#39;re not magic. Validation catches:</p>
<ul>
<li><strong>Configuration errors</strong>: Typos in YAML, invalid references, CIDR overlaps</li>
<li><strong>Reconciliation failures</strong>: Controller bugs, network issues, switch problems</li>
<li><strong>Environmental issues</strong>: Resource exhaustion, DHCP conflicts, VLAN collisions</li>
<li><strong>Timing issues</strong>: Configuration applied but BGP hasn&#39;t converged yet</li>
</ul>
<p><strong>Trust but verify</strong>: Declarative infrastructure reduces manual work, but operators still need to confirm the desired state matches reality.</p>
<h3>Validation Layers</h3>
<p>Hedgehog infrastructure validation has multiple layers:</p>
<p><strong>Layer 1: Kubernetes resource validation</strong></p>
<ul>
<li>Does the VPC CRD exist?</li>
<li>Do VPCAttachment CRDs exist?</li>
<li>Are there any error events?</li>
<li>Tool: <code>kubectl get</code>, <code>kubectl describe</code></li>
</ul>
<p><strong>Layer 2: Reconciliation validation</strong></p>
<ul>
<li>Did the fabric controller process the resources?</li>
<li>Did reconciliation complete successfully?</li>
<li>Tool: <code>kubectl get events</code></li>
</ul>
<p><strong>Layer 3: Switch-level validation</strong></p>
<ul>
<li>Are switches configured with correct VLANs?</li>
<li>Are VXLAN tunnels established?</li>
<li>Are BGP EVPN routes present?</li>
<li>Tool: Agent CRD inspection</li>
</ul>
<p><strong>Layer 4: Physical connectivity validation</strong></p>
<ul>
<li>Can servers reach their gateway?</li>
<li>Can servers communicate with each other?</li>
<li>Is DHCP working (if enabled)?</li>
<li>Tool: ping, SSH to servers, dhclient</li>
</ul>
<p>Each layer validates a different aspect. Errors at higher layers prevent lower layers from working.</p>
<h3>Agent CRD as Source of Truth</h3>
<p><strong>What is an Agent CRD?</strong></p>
<p>Every switch in the fabric has a corresponding Agent CRD. It&#39;s the operational state record for that switch.</p>
<p><strong>Agent CRD structure:</strong></p>
<pre><code class=""language-yaml"">apiVersion: agent.githedgehog.com/v1beta1
kind: Agent
metadata:
  name: leaf-01
  namespace: fab
spec:
  # What the fabric controller wants configured on this switch
  # Computed from VPCs, VPCAttachments, and Connections
  switchProfile: leaf
  config:
    vlans:
      1010:
        vxlan: 101010
        subnet: 10.10.10.0/24
      1020:
        vxlan: 102020
        subnet: 10.10.20.0/24
    ports:
      E1/5:
        mode: access
        vlan: 1010
status:
  # What the switch agent has applied
  # Updated by the switch agent after applying config
  applied: true
  lastApplied: &quot;2025-10-16T10:30:00Z&quot;
</code></pre>
<p><strong>Spec vs Status:</strong></p>
<ul>
<li><strong>Spec</strong>: Desired state (what should be configured)</li>
<li><strong>Status</strong>: Actual state (what was successfully applied)</li>
</ul>
<p><strong>When spec and status match</strong>: Configuration applied successfully
<strong>When they diverge</strong>: Configuration failed (check agent logs)</p>
<p><strong>Agent CRD workflow:</strong></p>
<ol>
<li>Fabric controller computes switch configuration from VPC/VPCAttachment CRDs</li>
<li>Fabric controller writes configuration to Agent CRD spec</li>
<li>Switch agent (running on the switch or as a pod) watches Agent CRD</li>
<li>Switch agent applies configuration via gNMI to SONiC switch OS</li>
<li>Switch agent updates Agent CRD status with applied state</li>
</ol>
<p><strong>When to inspect Agent CRDs:</strong></p>
<ul>
<li><strong>Troubleshooting</strong>: VPCAttachment succeeded but connectivity fails</li>
<li><strong>Deep validation</strong>: Verifying VLAN/VXLAN/BGP configuration</li>
<li><strong>Learning</strong>: Understanding what happens on switches</li>
</ul>
<h3>Event-Based Validation</h3>
<p>Kubernetes events tell the story of what happened during resource lifecycle.</p>
<p><strong>Event types:</strong></p>
<ul>
<li><strong>Normal</strong>: Successful operations (Created, Reconciling, Applied, Ready)</li>
<li><strong>Warning</strong>: Issues that may need attention (ValidationFailed, ReconciliationRetry)</li>
</ul>
<p><strong>Event timeline for VPC:</strong></p>
<pre><code>LAST SEEN   TYPE     REASON          OBJECT              MESSAGE
2m          Normal   Created         vpc/web-app-prod    VPC created
2m          Normal   Reconciling     vpc/web-app-prod    Processing VPC configuration
1m          Normal   AgentUpdate     vpc/web-app-prod    Updated agent specs for switches
1m          Normal   Ready           vpc/web-app-prod    VPC reconciliation complete
</code></pre>
<p><strong>Event timeline for VPCAttachment:</strong></p>
<pre><code>LAST SEEN   TYPE     REASON          OBJECT                             MESSAGE
3m          Normal   Created         vpcattachment/server-01            VPCAttachment created
3m          Normal   Reconciling     vpcattachment/server-01            Processing attachment
2m          Normal   Applied         vpcattachment/server-01            Configuration applied to leaf-01, leaf-02
2m          Normal   Ready           vpcattachment/server-01            Attachment ready
</code></pre>
<p><strong>Using events for troubleshooting:</strong></p>
<ul>
<li><strong>No events</strong>: Resource not picked up by controller (check controller logs)</li>
<li><strong>Warning events</strong>: Configuration validation failed (fix YAML and reapply)</li>
<li><strong>Reconciling stuck</strong>: Controller retrying (check referenced resources exist)</li>
<li><strong>Applied but connectivity fails</strong>: Configuration applied to switches, problem is elsewhere (check server config)</li>
</ul>
<p><strong>Event retention:</strong></p>
<p>Kubernetes events are typically retained for 1 hour by default. For historical analysis, use logging and monitoring tools.</p>
<h3>Validation Checklist</h3>
<p>Use this comprehensive checklist for every VPC deployment:</p>
<p><strong>VPC Validation:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> VPC exists (<code>kubectl get vpc &lt;name&gt;</code>)</li>
<li><input disabled="""" type=""checkbox""> Subnets configured correctly (CIDR, gateway, VLAN)</li>
<li><input disabled="""" type=""checkbox""> DHCP settings correct (if applicable)</li>
<li><input disabled="""" type=""checkbox""> No error events (<code>kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt;</code>)</li>
</ul>
<p><strong>VPCAttachment Validation:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> VPCAttachments exist (<code>kubectl get vpcattachments</code>)</li>
<li><input disabled="""" type=""checkbox""> Connection references correct and exist</li>
<li><input disabled="""" type=""checkbox""> Subnet references correct (VPC/subnet format)</li>
<li><input disabled="""" type=""checkbox""> Events show successful reconciliation</li>
<li><input disabled="""" type=""checkbox""> No error or warning events</li>
</ul>
<p><strong>Agent CRD Validation (Advanced):</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> VLANs configured on server-facing ports</li>
<li><input disabled="""" type=""checkbox""> VXLAN tunnels established</li>
<li><input disabled="""" type=""checkbox""> BGP EVPN routes present</li>
<li><input disabled="""" type=""checkbox""> Agent status shows applied configuration</li>
</ul>
<p><strong>Server-Level Validation (Production):</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> Server NIC configured with correct IP/VLAN</li>
<li><input disabled="""" type=""checkbox""> Server can ping gateway</li>
<li><input disabled="""" type=""checkbox""> Server can reach other servers in VPC</li>
<li><input disabled="""" type=""checkbox""> DHCP working (if applicable)</li>
</ul>
<p><strong>Common validation workflow:</strong></p>
<ol>
<li>Start with high-level checks (does VPC exist?)</li>
<li>Move to reconciliation checks (did it apply successfully?)</li>
<li>Dive into Agent CRDs only if issues arise</li>
<li>Test physical connectivity last (after configuration confirmed)</li>
</ol>
<h3>Common Validation Failures and Causes</h3>
<p><strong>Issue: VPC shows in kubectl get but describe shows errors</strong></p>
<p><strong>Symptom</strong>: VPC exists but events show warnings</p>
<p><strong>Cause</strong>: Validation error in spec (CIDR overlap, VLAN conflict, invalid configuration)</p>
<p><strong>Investigation</strong>:</p>
<pre><code class=""language-bash"">kubectl describe vpc &lt;name&gt;
kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt;
</code></pre>
<p><strong>Fix</strong>: Review events for specific error, fix YAML, reapply</p>
<hr>
<p><strong>Issue: VPCAttachment reconciliation stuck in &quot;Reconciling&quot; state</strong></p>
<p><strong>Symptom</strong>: Events show &quot;Reconciling&quot; but never &quot;Applied&quot;</p>
<p><strong>Cause</strong>: Connection doesn&#39;t exist, VPC reference invalid, or subnet doesn&#39;t exist</p>
<p><strong>Investigation</strong>:</p>
<pre><code class=""language-bash"">kubectl describe vpcattachment &lt;name&gt;
kubectl get connection &lt;connection-name&gt; -n fab
kubectl get vpc &lt;vpc-name&gt;
</code></pre>
<p><strong>Fix</strong>: Verify connection and VPC exist, correct references in VPCAttachment YAML</p>
<hr>
<p><strong>Issue: Agent CRD spec updated but status not matching</strong></p>
<p><strong>Symptom</strong>: Agent spec shows config, but status doesn&#39;t reflect applied state</p>
<p><strong>Cause</strong>: Switch connectivity issues, agent pod not running, gNMI failure</p>
<p><strong>Investigation</strong>:</p>
<pre><code class=""language-bash"">kubectl get agent &lt;switch-name&gt; -n fab -o yaml
kubectl get pods -n fab | grep agent
kubectl logs &lt;agent-pod-name&gt; -n fab
</code></pre>
<p><strong>Fix</strong>: Check agent pods, verify switch reachability, review agent logs</p>
<hr>
<p><strong>Issue: Server has no connectivity despite successful VPCAttachment</strong></p>
<p><strong>Symptom</strong>: VPCAttachment shows &quot;Applied&quot; but server can&#39;t reach network</p>
<p><strong>Cause</strong>: Server OS network not configured, wrong VLAN on server NIC</p>
<p><strong>Investigation</strong>:</p>
<pre><code class=""language-bash""># SSH to server
ip addr show
ip route show

# Check if VLAN subinterface needed
# For native VLAN: eth0 configured directly
# For tagged VLAN: eth0.1010 subinterface needed
</code></pre>
<p><strong>Fix</strong>: Configure server network, verify VLAN tagging matches fabric configuration</p>
<hr>
<p><strong>Issue: DHCP not assigning IP to server</strong></p>
<p><strong>Symptom</strong>: Server not getting DHCP IP on worker-nodes subnet</p>
<p><strong>Cause</strong>: Server DHCP client not running, DHCP range exhausted, or server NIC not requesting DHCP</p>
<p><strong>Investigation</strong>:</p>
<pre><code class=""language-bash""># On server
sudo systemctl status dhclient
sudo journalctl -u dhclient

# On fabric
kubectl get vpc &lt;vpc-name&gt; -o jsonpath=&#39;{.spec.subnets.&lt;subnet-name&gt;.dhcp}&#39; | jq
</code></pre>
<p><strong>Fix</strong>: Start DHCP client on server, expand DHCP range if exhausted, verify server NIC configuration</p>
<h2>Troubleshooting</h2>
<h3>Issue: VPC validation shows &quot;subnet overlap&quot; warning in events</h3>
<p><strong>Symptom:</strong> VPC describes shows warning event about subnet overlap</p>
<p><strong>Cause:</strong> Subnet CIDR overlaps with another VPC&#39;s subnet</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># List all VPCs and their subnets to identify overlap
kubectl get vpcs -o yaml | grep -E &quot;(name:|subnet:)&quot;

# Example output showing overlap:
#   name: existing-vpc
#     subnet: 10.10.0.0/16    # Overlaps with web-app-prod subnets!
#   name: web-app-prod
#     subnet: 10.10.10.0/24

# Solution: Change web-app-prod subnets to non-overlapping range
# Edit VPC YAML and change subnet CIDRs
# Then reapply:
kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<h3>Issue: VPCAttachment events show &quot;Connection not found&quot;</h3>
<p><strong>Symptom:</strong> VPCAttachment describe shows error event &quot;Connection &#39;server-01&#39; not found&quot;</p>
<p><strong>Cause:</strong> Connection name in VPCAttachment doesn&#39;t match any Connection CRD</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># List connections to find correct name
kubectl get connections -n fab | grep server-01

# Expected output:
# server-01--mclag--leaf-01--leaf-02   2h

# Update VPCAttachment YAML with full connection name
# Wrong:
#   connection: server-01
# Correct:
#   connection: server-01--mclag--leaf-01--leaf-02

# Reapply:
kubectl apply -f server-01-attachment.yaml
</code></pre>
<h3>Issue: Agent CRD shows configuration in spec but status is empty</h3>
<p><strong>Symptom:</strong> Agent CRD spec has VLAN/VXLAN config but status field is empty or outdated</p>
<p><strong>Cause:</strong> Switch agent not running, switch unreachable, or gNMI communication failure</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check if agent pods are running
kubectl get pods -n fab | grep agent

# If agent pod is not running or crashing:
kubectl describe pod &lt;agent-pod-name&gt; -n fab
kubectl logs &lt;agent-pod-name&gt; -n fab

# Check if switch is reachable from agent pod
kubectl exec &lt;agent-pod-name&gt; -n fab -- ping &lt;switch-ip&gt;

# If switch is unreachable, verify switch management connectivity
# If agent is crashing, check logs for gNMI authentication issues
</code></pre>
<h3>Issue: Events show &quot;Applied&quot; but server can&#39;t ping gateway</h3>
<p><strong>Symptom:</strong> VPCAttachment events show configuration applied, but server has no network connectivity</p>
<p><strong>Cause:</strong> Server OS network configuration missing or incorrect</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># SSH to server and check network configuration
ip addr show
ip route show

# For static IP subnet (web-servers):
# Manually configure IP if not present
sudo ip addr add 10.10.10.10/24 dev eth0
sudo ip route add default via 10.10.10.1

# For DHCP subnet (worker-nodes):
# Start DHCP client if not running
sudo dhclient eth0

# Verify VLAN configuration (if using tagged VLANs)
# If VPCAttachment expects VLAN tagging, create subinterface:
sudo ip link add link eth0 name eth0.1020 type vlan id 1020
sudo ip link set eth0.1020 up
sudo dhclient eth0.1020
</code></pre>
<h3>Issue: kubectl describe shows no events for VPC or VPCAttachment</h3>
<p><strong>Symptom:</strong> Resource exists but no events visible in describe output</p>
<p><strong>Cause:</strong> Events expired (1 hour retention by default) or controller not running</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check if fabric controller is running
kubectl get pods -n fab | grep controller

# If controller is not running:
kubectl describe pod &lt;controller-pod-name&gt; -n fab
kubectl logs &lt;controller-pod-name&gt; -n fab

# If events expired, check broader event history:
kubectl get events -n fab --sort-by=&#39;.lastTimestamp&#39; | tail -50

# For long-term event retention, use logging/monitoring tools
</code></pre>
<h3>Issue: DHCP range exhausted, servers not getting IPs</h3>
<p><strong>Symptom:</strong> New servers on DHCP subnet fail to get IP, DHCP events in logs show range exhausted</p>
<p><strong>Cause:</strong> DHCP range too small for number of servers</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check current DHCP configuration
kubectl get vpc web-app-prod -o jsonpath=&#39;{.spec.subnets.worker-nodes.dhcp}&#39; | jq

# Current range: 10.10.20.10 to 10.10.20.250 = 241 IPs

# Solution 1: Expand DHCP range to larger subnet
kubectl edit vpc web-app-prod
# Change subnet from /24 to /23 (512 IPs)
# Update DHCP end range accordingly

# Solution 2: Reclaim unused IPs
# Identify servers that are no longer active and remove VPCAttachments
kubectl get vpcattachments | grep worker-nodes
kubectl delete vpcattachment &lt;unused-attachment&gt;

# Solution 3: Add another DHCP subnet
# Create additional subnet in VPC with separate DHCP range
</code></pre>
<h2>Resources</h2>
<h3>Hedgehog CRDs</h3>
<p><strong>VPC</strong> - Virtual Private Cloud definition</p>
<ul>
<li>View all: <code>kubectl get vpcs</code></li>
<li>View specific: <code>kubectl get vpc &lt;name&gt;</code></li>
<li>Inspect details: <code>kubectl describe vpc &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpc &lt;name&gt; -o yaml</code></li>
</ul>
<p><strong>VPCAttachment</strong> - Binds server connection to VPC subnet</p>
<ul>
<li>View all: <code>kubectl get vpcattachments</code></li>
<li>View specific: <code>kubectl get vpcattachment &lt;name&gt;</code></li>
<li>Inspect: <code>kubectl describe vpcattachment &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpcattachment &lt;name&gt; -o yaml</code></li>
</ul>
<p><strong>Agent</strong> - Per-switch operational state</p>
<ul>
<li>View all: <code>kubectl get agents -n fab</code></li>
<li>View specific: <code>kubectl get agent &lt;switch-name&gt; -n fab</code></li>
<li>Inspect: <code>kubectl describe agent &lt;switch-name&gt; -n fab</code></li>
<li>View YAML: <code>kubectl get agent &lt;switch-name&gt; -n fab -o yaml</code></li>
</ul>
<p><strong>Connection</strong> - Server-to-switch wiring</p>
<ul>
<li>View all: <code>kubectl get connections -n fab</code></li>
<li>View server connections: <code>kubectl get connections -n fab | grep server-</code></li>
</ul>
<h3>kubectl Commands Reference</h3>
<p><strong>Validation commands:</strong></p>
<pre><code class=""language-bash""># List VPCs
kubectl get vpcs

# Describe VPC with full details
kubectl describe vpc &lt;name&gt;

# List VPCAttachments
kubectl get vpcattachments

# Describe VPCAttachment
kubectl describe vpcattachment &lt;name&gt;

# View events sorted by time
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20

# View events for specific resource
kubectl get events --field-selector involvedObject.name=&lt;resource-name&gt;

# View all events in fabric namespace
kubectl get events -n fab --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p><strong>Agent CRD inspection:</strong></p>
<pre><code class=""language-bash""># List all agents (switches)
kubectl get agents -n fab

# View agent YAML (full configuration)
kubectl get agent &lt;switch-name&gt; -n fab -o yaml

# Describe agent with events
kubectl describe agent &lt;switch-name&gt; -n fab

# Filter agent config for specific port
kubectl get agent &lt;switch-name&gt; -n fab -o yaml | grep -A 10 &quot;&lt;port-name&gt;&quot;
</code></pre>
<p><strong>Event filtering:</strong></p>
<pre><code class=""language-bash""># Events for VPC
kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt;

# Events for VPCAttachment
kubectl get events --field-selector involvedObject.name=&lt;attachment-name&gt;

# Events for Agent
kubectl get events --field-selector involvedObject.name=&lt;agent-name&gt; -n fab

# Watch events in real-time
kubectl get events --watch --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p><strong>DHCP verification:</strong></p>
<pre><code class=""language-bash""># Check DHCP configuration for subnet
kubectl get vpc &lt;vpc-name&gt; -o jsonpath=&#39;{.spec.subnets.&lt;subnet-name&gt;.dhcp}&#39; | jq

# View entire VPC YAML
kubectl get vpc &lt;vpc-name&gt; -o yaml
</code></pre>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""./module-2.2-vpc-attachments.md"">Module 2.2: VPC Attachments</a></li>
<li>Next: Module 2.4: Decommission &amp; Cleanup (completes Course 2)</li>
<li>Pathway: <a href=""../../pathways/network-like-hyperscaler.json"">Network Like a Hyperscaler</a></li>
</ul>
<h3>External Documentation</h3>
<ul>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog VPC Documentation</a></li>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog Agent CRD Reference</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/"">Kubernetes Events</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubectl/"">kubectl Command Reference</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve successfully learned VPC and VPCAttachment validation workflows. Ready to learn decommissioning and cleanup in Module 2.4.</p>
",203,20,,,"hedgehog,fabric,validation,connectivity,operations,troubleshooting"
198578603881,Dashboard Interpretation,fabric-operations-dashboard-interpretation,[object Object],"Master Grafana dashboards for fabric health checks. Learn to interpret BGP status, interface errors, hardware metrics, and ASIC resources daily.","<h2>Introduction</h2>
<p>In Module 3.1, you learned how telemetry flows from switches to Prometheus. You ran PromQL queries to see raw metrics—CPU percentages, bandwidth rates, BGP neighbor states. You accessed Prometheus directly and explored the metrics that power your observability stack.</p>
<p>But in daily operations, you won&#39;t be writing PromQL queries every time you want to check fabric health. That&#39;s where <strong>Grafana dashboards</strong> come in—pre-built visualizations that answer common operational questions at a glance.</p>
<p><strong>The Morning Question:</strong></p>
<p>Every morning, a fabric operator asks:</p>
<ul>
<li><strong>Is my fabric healthy?</strong></li>
<li><strong>Are all BGP sessions up?</strong></li>
<li><strong>Are there any interface errors?</strong></li>
<li><strong>Is any switch running hot or out of resources?</strong></li>
</ul>
<p>Grafana dashboards answer these questions in seconds without writing queries.</p>
<p><strong>What You&#39;ll Learn:</strong></p>
<p>Hedgehog provides <strong>6 pre-built Grafana dashboards</strong>:</p>
<ol>
<li><strong>Fabric Dashboard</strong> - BGP underlay health</li>
<li><strong>Interfaces Dashboard</strong> - Interface state and traffic</li>
<li><strong>Platform Dashboard</strong> - Hardware health (PSU, fans, temperature)</li>
<li><strong>Logs Dashboard</strong> - Switch logs and error filtering</li>
<li><strong>Node Exporter Dashboard</strong> - Linux system metrics</li>
<li><strong>Switch Critical Resources Dashboard</strong> - ASIC resource limits</li>
</ol>
<p>You&#39;ll learn to <strong>read</strong> each dashboard (not build them), identify healthy vs unhealthy states, and create a morning health check workflow that takes less than 5 minutes.</p>
<p><strong>Context: Proactive vs Reactive Monitoring</strong></p>
<ul>
<li><strong>Reactive</strong>: Wait for alerts or user reports, then investigate</li>
<li><strong>Proactive</strong>: Check dashboards daily, spot trends before they become problems</li>
</ul>
<p>This module teaches proactive monitoring—catching issues early, building operational confidence, and maintaining fabric health systematically.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Interpret Fabric Dashboard</strong> - Read VPC count, VNI allocation, and BGP session status</li>
<li><strong>Interpret Interfaces Dashboard</strong> - Understand interface state, VLAN config, traffic rates, and errors</li>
<li><strong>Interpret Platform Dashboard</strong> - Read switch resource usage (CPU, memory, disk, temperature)</li>
<li><strong>Interpret Logs Dashboard</strong> - Filter and search switch logs for errors and events</li>
<li><strong>Interpret Node Exporter Dashboard</strong> - Understand detailed Linux system metrics</li>
<li><strong>Interpret Switch Critical Resources Dashboard</strong> - Identify ASIC resource exhaustion risks</li>
<li><strong>Create morning health check workflow</strong> - Use dashboards systematically for daily monitoring</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 3.1 completion (Fabric Telemetry Overview)</li>
<li>Understanding of Prometheus metrics and PromQL</li>
<li>Familiarity with metric types (counters, gauges)</li>
<li>Basic Grafana navigation (from Module 1.3)</li>
</ul>
<h2>Scenario: Morning Health Check Using Grafana Dashboards</h2>
<p>It&#39;s Monday morning, and you&#39;re the on-call fabric operator. Your first task: verify the fabric is healthy before the business day begins. You&#39;ll use Grafana dashboards to perform a systematic health check in under 5 minutes.</p>
<p><strong>Environment Access:</strong></p>
<ul>
<li><strong>Grafana:</strong> <a href=""http://localhost:3000"">http://localhost:3000</a> (username: <code>admin</code>, password: <code>prom-operator</code>)</li>
</ul>
<h3>Task 1: Check BGP Fabric Health (2 minutes)</h3>
<p><strong>Objective:</strong> Verify all BGP sessions are established</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Open Grafana:</strong></p>
<ul>
<li>Navigate to <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li>Sign in (admin / prom-operator)</li>
</ul>
</li>
<li><p><strong>Access Fabric Dashboard:</strong></p>
<ul>
<li>Click <strong>Dashboards</strong> (left sidebar)</li>
<li>Select <strong>&quot;Hedgehog Fabric&quot;</strong> dashboard</li>
</ul>
</li>
<li><p><strong>Check BGP Sessions Overview Panel:</strong></p>
<ul>
<li>Locate &quot;BGP Sessions&quot; panel (top of dashboard)</li>
<li><strong>Expected healthy state:</strong><pre><code>Total Sessions: 40
Established: 40
Down: 0
</code></pre>
</li>
<li>✅ If Established = Total, fabric underlay is healthy</li>
<li>❌ If Down &gt; 0, identify which neighbor is down</li>
</ul>
</li>
<li><p><strong>Review BGP Neighbor Table:</strong></p>
<ul>
<li>Scroll to &quot;BGP Neighbor Status&quot; table</li>
<li>Scan &quot;State&quot; column - all should say &quot;established&quot;</li>
<li>Check &quot;Prefixes Received&quot; - should be &gt; 0</li>
<li><strong>Red flag:</strong> Any row with State ≠ &quot;established&quot;</li>
</ul>
<p><strong>Example healthy row:</strong></p>
<pre><code>Switch    Neighbor         State         Prefixes  Uptime
leaf-01   172.30.128.1    established   12        3d 5h
leaf-01   172.30.128.2    established   12        3d 5h
</code></pre>
</li>
<li><p><strong>Check for Session Flaps:</strong></p>
<ul>
<li>Find &quot;BGP Session Flaps&quot; time series panel</li>
<li>Set time range to &quot;Last 24 hours&quot;</li>
<li><strong>Healthy:</strong> Flat line (no state changes)</li>
<li><strong>Unhealthy:</strong> Spikes indicate sessions going up/down</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ All BGP sessions established</li>
<li>✅ No flaps in last 24 hours</li>
<li>✅ Prefix counts stable</li>
</ul>
<p><strong>If Unhealthy:</strong></p>
<ul>
<li>Note which switch and neighbor has issue</li>
<li>Check Logs Dashboard for BGP-related errors</li>
<li>Proceed to Module 3.3 for event correlation techniques</li>
</ul>
<h3>Task 2: Check Interface Health (1-2 minutes)</h3>
<p><strong>Objective:</strong> Verify all expected interfaces are up and error-free</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Access Interfaces Dashboard:</strong></p>
<ul>
<li>Click <strong>Dashboards</strong> → <strong>&quot;Hedgehog Interfaces&quot;</strong></li>
</ul>
</li>
<li><p><strong>Check Interface State Overview:</strong></p>
<ul>
<li>Locate &quot;Interface Operational State&quot; panel</li>
<li><strong>Healthy:</strong> Expected interfaces show green (Up)</li>
<li><strong>Unhealthy:</strong> Expected interfaces show red (Down)</li>
<li><strong>Note:</strong> Unused ports down = OK</li>
</ul>
</li>
<li><p><strong>Check Error Counters:</strong></p>
<ul>
<li>Find &quot;Interface Errors&quot; panel or table</li>
<li><strong>Healthy:</strong> Error rate = 0 or flat line</li>
<li><strong>Unhealthy:</strong> Growing error count (indicates problem)</li>
</ul>
<p><strong>Example of problem:</strong></p>
<pre><code>Interface    CRC Errors   Frame Errors   Discards
Ethernet1    0            0              0          ← Healthy
Ethernet5    1,234        523            0          ← Cable problem
Ethernet7    0            0              52,341     ← Congestion (drops)
</code></pre>
</li>
<li><p><strong>Check Interface Utilization:</strong></p>
<ul>
<li>Locate &quot;Interface Utilization %&quot; panel</li>
<li><strong>Healthy:</strong> &lt; 70%</li>
<li><strong>Warning:</strong> 70-90%</li>
<li><strong>Critical:</strong> &gt; 90%</li>
<li><strong>Action:</strong> If &gt; 90%, note for capacity planning</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ All expected interfaces up</li>
<li>✅ No growing error counters</li>
<li>✅ Utilization &lt; 90%</li>
</ul>
<h3>Task 3: Check Hardware Health (1 minute)</h3>
<p><strong>Objective:</strong> Verify switches are not experiencing hardware issues</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Access Platform Dashboard:</strong></p>
<ul>
<li>Click <strong>Dashboards</strong> → <strong>&quot;Hedgehog Platform&quot;</strong></li>
</ul>
</li>
<li><p><strong>Check PSU Status:</strong></p>
<ul>
<li>Locate &quot;PSU Status&quot; panel</li>
<li><strong>Healthy:</strong> All PSUs = OK</li>
<li><strong>Unhealthy:</strong> Any PSU = Failed → Schedule replacement</li>
</ul>
</li>
<li><p><strong>Check Fan Speeds:</strong></p>
<ul>
<li>Find &quot;Fan Speeds&quot; panel</li>
<li><strong>Healthy:</strong> All fans &gt; 0 RPM (e.g., &gt; 3000 RPM)</li>
<li><strong>Unhealthy:</strong> Any fan = 0 RPM → Thermal risk</li>
</ul>
</li>
<li><p><strong>Check Temperature:</strong></p>
<ul>
<li>Locate &quot;Temperature Sensors&quot; panel</li>
<li><strong>Healthy thresholds:</strong><ul>
<li>CPU &lt; 70°C</li>
<li>ASIC &lt; 80°C</li>
<li>Ambient &lt; 45°C</li>
</ul>
</li>
<li><strong>Unhealthy:</strong> Any sensor exceeding threshold → Investigate cooling</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ All PSUs operational</li>
<li>✅ All fans running</li>
<li>✅ Temperatures within limits</li>
</ul>
<h3>Task 4: Check for Recent Errors (1 minute)</h3>
<p><strong>Objective:</strong> Identify any error logs in last 1 hour</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Access Logs Dashboard:</strong></p>
<ul>
<li>Click <strong>Dashboards</strong> → <strong>&quot;Hedgehog Logs&quot;</strong></li>
</ul>
</li>
<li><p><strong>Check Error Count:</strong></p>
<ul>
<li>Locate &quot;Log Level Breakdown&quot; panel</li>
<li>Set time range: &quot;Last 1 hour&quot;</li>
<li><strong>Healthy:</strong> ERROR count = 0 or very low (&lt; 5)</li>
<li><strong>Unhealthy:</strong> ERROR count &gt; 10 → Investigate</li>
</ul>
</li>
<li><p><strong>Review Error Logs (if present):</strong></p>
<ul>
<li>If errors present, locate &quot;Syslog Stream&quot; panel</li>
<li>Filter by <code>level=&quot;error&quot;</code></li>
<li>Read error messages to identify issues</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>[leaf-02] ERROR: Interface Ethernet5 link down
[spine-01] ERROR: BGP neighbor 172.30.128.5 connection refused
</code></pre>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ ERROR count near zero</li>
<li>✅ No unexpected critical errors</li>
</ul>
<h3>Task 5: Optional - Check ASIC Resources (1 minute)</h3>
<p><strong>Objective:</strong> Verify no ASIC resources nearing capacity</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Access Switch Critical Resources Dashboard:</strong></p>
<ul>
<li>Click <strong>Dashboards</strong> → <strong>&quot;Switch Critical Resources&quot;</strong></li>
</ul>
</li>
<li><p><strong>Scan Resource Utilization:</strong></p>
<ul>
<li>Review all panels (Route Table, ARP Table, FDB, ACL)</li>
<li><strong>Healthy:</strong> All &lt; 80% capacity</li>
<li><strong>Warning:</strong> Any resource 80-90%</li>
<li><strong>Critical:</strong> Any resource &gt; 90%</li>
</ul>
</li>
<li><p><strong>Note Resources for Capacity Planning:</strong></p>
<ul>
<li>If any resource &gt; 70%, document for future planning</li>
<li>ASIC resources are hardware limits and cannot be increased</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ All ASIC resources &lt; 90% capacity</li>
</ul>
<h3>Lab Summary</h3>
<p><strong>What you accomplished:</strong></p>
<p>You performed a complete fabric health check using 5 Grafana dashboards in under 5 minutes:</p>
<ul>
<li>✅ Verified BGP underlay health (Fabric Dashboard)</li>
<li>✅ Checked interface state and errors (Interfaces Dashboard)</li>
<li>✅ Confirmed hardware health (Platform Dashboard)</li>
<li>✅ Scanned for recent error logs (Logs Dashboard)</li>
<li>✅ Reviewed ASIC resource usage (Critical Resources Dashboard)</li>
</ul>
<p><strong>What you learned:</strong></p>
<ul>
<li>Morning health check takes &lt; 5 minutes with dashboards</li>
<li>Each dashboard answers specific operational questions</li>
<li>&quot;Healthy&quot; has clear, measurable criteria</li>
<li>Dashboards enable proactive monitoring (catch issues before alerts)</li>
<li>Systematic workflows prevent overlooking critical issues</li>
</ul>
<p><strong>Morning Health Check Checklist:</strong></p>
<pre><code>Daily Fabric Health Check (5 minutes)
────────────────────────────────────
☐ BGP sessions all established (Fabric Dashboard)
☐ No BGP flaps in last 24h
☐ Interfaces up and error-free (Interfaces Dashboard)
☐ PSUs operational, fans running (Platform Dashboard)
☐ Temperatures within limits
☐ ERROR log count near zero (Logs Dashboard)
☐ ASIC resources &lt; 90% (Critical Resources Dashboard)
</code></pre>
<h2>Concepts &amp; Deep Dive</h2>
<p>Now that you&#39;ve performed a morning health check hands-on, let&#39;s explore each dashboard in detail. Understanding what each panel shows and what &quot;healthy&quot; looks like will deepen your operational mastery.</p>
<h3>Concept 1: Fabric Dashboard - BGP Underlay Health</h3>
<p><strong>Purpose:</strong> Monitor the BGP fabric underlay that connects all switches</p>
<p>The Fabric Dashboard is your primary tool for verifying that the BGP underlay—the foundation of your fabric—is stable and routing correctly. Without healthy BGP sessions, VPCs cannot communicate across switches.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. BGP Sessions Overview</strong></p>
<ul>
<li><strong>Metric:</strong> Count of BGP sessions (total, established, down)</li>
<li><strong>Healthy State:</strong> All sessions = Established</li>
<li><strong>Unhealthy State:</strong> Any sessions in Idle, Connect, or Active state</li>
<li><strong>Example:</strong><pre><code>Total Sessions: 40
Established: 40
Down: 0
</code></pre>
</li>
</ul>
<p><strong>2. BGP Neighbor Status Table</strong></p>
<ul>
<li><strong>Columns:</strong> Switch, Neighbor IP, State, Prefixes Received, Uptime</li>
<li><strong>Healthy Row:</strong> State = &quot;established&quot;, Prefixes &gt; 0, Uptime &gt; 1 hour</li>
<li><strong>Unhealthy Row:</strong> State ≠ &quot;established&quot;, Prefixes = 0</li>
<li><strong>Example:</strong><pre><code>Switch    Neighbor         State         Prefixes  Uptime
leaf-01   172.30.128.1    established   12        3d 5h
leaf-01   172.30.128.2    established   12        3d 5h
leaf-02   172.30.128.1    Idle          0         0s      ← PROBLEM
</code></pre>
</li>
</ul>
<p><strong>3. BGP Session Flaps</strong></p>
<ul>
<li><strong>Metric:</strong> BGP session state changes over time</li>
<li><strong>Healthy State:</strong> Flat line (no changes)</li>
<li><strong>Unhealthy State:</strong> Spikes indicate session instability</li>
<li><strong>Interpretation:</strong> Graph showing session going down and back up = flapping (investigate physical layer or BGP config)</li>
</ul>
<p><strong>4. Prefixes Received/Advertised</strong></p>
<ul>
<li><strong>Metric:</strong> Number of routes exchanged per neighbor</li>
<li><strong>Healthy State:</strong> Consistent count (e.g., 10-20 prefixes per neighbor)</li>
<li><strong>Unhealthy State:</strong> Sudden drop to 0 (neighbor lost routes)</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Daily Health Check:</strong> Verify BGP Sessions count matches expected (e.g., 40)</li>
<li><strong>Troubleshooting:</strong> Identify which switch/neighbor has session down</li>
<li><strong>Trend Analysis:</strong> Check for flapping patterns over last 24 hours</li>
<li><strong>Capacity Planning:</strong> Monitor prefix counts as VPCs scale</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>All sessions green/established</li>
<li>Stable prefix counts</li>
<li>No flaps in last 24 hours</li>
<li>Uptime &gt; last change window</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>Any session not established</li>
<li>Frequent session flaps (more than 1-2 per day)</li>
<li>Prefix count = 0 for established sessions</li>
<li>Sessions down for &gt; 5 minutes</li>
</ul>
<h3>Concept 2: Interfaces Dashboard - Traffic and Errors</h3>
<p><strong>Purpose:</strong> Monitor interface state, traffic rates, and error counters</p>
<p>The Interfaces Dashboard gives you visibility into the health of every network interface across your fabric. This is where you identify congestion, cable problems, and capacity issues.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. Interface Operational State</strong></p>
<ul>
<li><strong>Metric:</strong> Up (green) or Down (red) per interface</li>
<li><strong>Healthy State:</strong> All configured interfaces up</li>
<li><strong>Unhealthy State:</strong> Expected-up interfaces showing down</li>
<li><strong>Example:</strong><pre><code>leaf-01/Ethernet1: UP    (server connection)
leaf-01/Ethernet2: UP    (server connection)
leaf-01/Ethernet48: UP   (spine uplink)
leaf-01/Ethernet49: DOWN (unused port - OK)
</code></pre>
</li>
</ul>
<p><strong>2. Interface Traffic Rate</strong></p>
<ul>
<li><strong>Metric:</strong> Bits per second (bps) or packets per second (pps)</li>
<li><strong>Visualization:</strong> Time series graph per interface</li>
<li><strong>Healthy State:</strong> Consistent traffic matching workload</li>
<li><strong>Unhealthy State:</strong> Unexpected spikes or drops to zero</li>
<li><strong>Example:</strong><ul>
<li>Server interface: 2 Gbps steady (expected)</li>
<li>Server interface: 10 Gbps sustained (possible saturation - investigate)</li>
</ul>
</li>
</ul>
<p><strong>3. Interface Utilization</strong></p>
<ul>
<li><strong>Metric:</strong> Percentage of link capacity used</li>
<li><strong>Healthy State:</strong> &lt; 70% utilization (headroom available)</li>
<li><strong>Warning State:</strong> 70-90% (nearing capacity)</li>
<li><strong>Critical State:</strong> &gt; 90% (congestion risk)</li>
<li><strong>Example:</strong><pre><code>Ethernet1: 45% (healthy)
Ethernet2: 85% (warning - consider upgrade)
Ethernet3: 95% (critical - likely packet drops)
</code></pre>
</li>
</ul>
<p><strong>4. Error Counters</strong></p>
<ul>
<li><strong>Metric:</strong> CRC errors, frame errors, discards per interface</li>
<li><strong>Healthy State:</strong> 0 errors, or very low error rate</li>
<li><strong>Unhealthy State:</strong> Growing error count (cable issue, duplex mismatch)</li>
</ul>
<p><strong>Understanding Error Types:</strong></p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Cause</th>
<th>Action</th>
</tr>
</thead>
<tbody><tr>
<td><strong>CRC Errors</strong></td>
<td>Bad cable, dirty fiber, EMI</td>
<td>Replace cable/clean fiber</td>
</tr>
<tr>
<td><strong>Frame Errors</strong></td>
<td>Physical layer issues</td>
<td>Check cable, duplex settings</td>
</tr>
<tr>
<td><strong>Discards</strong></td>
<td>Congestion, buffer full</td>
<td>Check utilization, consider QoS</td>
</tr>
</tbody></table>
<p><strong>Example:</strong></p>
<pre><code>Interface    CRC Errors   Frame Errors   Discards
Ethernet1    0            0              0          ← Healthy
Ethernet2    1,234        523            0          ← Cable problem
Ethernet3    0            0              52,341     ← Congestion (drops)
</code></pre>
<p><strong>5. VLAN Configuration</strong></p>
<ul>
<li><strong>Metric:</strong> VLANs configured on each interface</li>
<li><strong>Use Case:</strong> Verify VPCAttachment applied correct VLANs</li>
<li><strong>Example:</strong><pre><code>Ethernet1: VLAN 1010, 1020 (expected for server-01)
Ethernet2: VLAN 1030        (expected for server-02)
</code></pre>
</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Daily Health Check:</strong> Scan for red (down) interfaces or high error rates</li>
<li><strong>Capacity Planning:</strong> Identify interfaces consistently &gt; 70% utilized</li>
<li><strong>Troubleshooting:</strong> Correlate traffic drop with VPC connectivity issues</li>
<li><strong>Validation:</strong> After VPCAttachment, verify VLAN appears on interface</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>All expected interfaces up (green)</li>
<li>Error counters flat (not increasing)</li>
<li>Utilization &lt; 70%</li>
<li>Traffic patterns match workload expectations</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>Expected interfaces down</li>
<li>Growing error counters (cable degradation)</li>
<li>Utilization &gt; 90% (congestion risk)</li>
<li>Traffic anomalies (unexpected spikes/drops)</li>
</ul>
<h3>Concept 3: Platform Dashboard - Hardware Health</h3>
<p><strong>Purpose:</strong> Monitor switch hardware (PSU, fans, temperature, optics)</p>
<p>The Platform Dashboard provides visibility into the physical health of your switches. This is preventive maintenance—catching hardware failures before they cause outages.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. PSU Status</strong></p>
<ul>
<li><strong>Metric:</strong> Operational state (OK/Failed) per PSU</li>
<li><strong>Healthy State:</strong> All PSUs = OK</li>
<li><strong>Unhealthy State:</strong> Any PSU = Failed or Not Present</li>
<li><strong>Example:</strong><pre><code>Switch    PSU1   PSU2
leaf-01   OK     OK       ← Healthy
leaf-02   OK     FAILED   ← Replace PSU2
</code></pre>
</li>
</ul>
<p><strong>2. PSU Voltage</strong></p>
<ul>
<li><strong>Metric:</strong> Input/output voltage in volts</li>
<li><strong>Healthy State:</strong> Within expected range (e.g., 11.5V - 12.5V for 12V rail)</li>
<li><strong>Unhealthy State:</strong> Voltage outside range (power supply issue)</li>
<li><strong>Example:</strong><pre><code>PSU1 Output: 12.1V (OK)
PSU2 Output: 10.2V (Low - failing PSU)
</code></pre>
</li>
</ul>
<p><strong>3. Fan Speeds</strong></p>
<ul>
<li><strong>Metric:</strong> RPM (revolutions per minute) per fan</li>
<li><strong>Healthy State:</strong> All fans &gt; minimum threshold (e.g., &gt; 3000 RPM)</li>
<li><strong>Unhealthy State:</strong> Fan = 0 RPM (failed) or very low</li>
<li><strong>Example:</strong><pre><code>Fan Tray 1: 5,200 RPM (OK)
Fan Tray 2: 5,100 RPM (OK)
Fan Tray 3: 0 RPM     (FAILED - thermal risk)
</code></pre>
</li>
</ul>
<p><strong>4. Temperature Sensors</strong></p>
<ul>
<li><strong>Metric:</strong> Celsius per sensor (CPU, ASIC, ambient, PSU)</li>
<li><strong>Healthy State:</strong> Below warning thresholds<ul>
<li>CPU: &lt; 70°C</li>
<li>ASIC: &lt; 80°C</li>
<li>Ambient: &lt; 45°C</li>
</ul>
</li>
<li><strong>Unhealthy State:</strong> Approaching or exceeding limits</li>
<li><strong>Example:</strong><pre><code>CPU Temp: 55°C (OK)
ASIC Temp: 85°C (WARNING - check cooling)
Ambient: 48°C (WARNING - data center HVAC issue)
</code></pre>
</li>
</ul>
<p><strong>5. Transceiver Optics (DOM)</strong></p>
<ul>
<li><strong>Metric:</strong> TX/RX power, temperature per optical interface</li>
<li><strong>Healthy State:</strong> Within optic specifications</li>
<li><strong>Unhealthy State:</strong> Low RX power (dirty/bad fiber), high temp (failing optic)</li>
<li><strong>Example:</strong><pre><code>Ethernet48 (Spine Uplink):
  TX Power: -2.5 dBm (OK)
  RX Power: -3.1 dBm (OK)
  Temp: 45°C (OK)

Ethernet49:
  RX Power: -15.2 dBm (CRITICAL - signal loss, check fiber)
</code></pre>
</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Daily Health Check:</strong> Scan for failed PSUs, stopped fans, high temps</li>
<li><strong>Preventive Maintenance:</strong> Schedule PSU/fan replacement before failure</li>
<li><strong>Capacity Planning:</strong> Track temperature trends (data center cooling)</li>
<li><strong>Optics Validation:</strong> After fiber installation, verify RX power in range</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>All PSUs operational</li>
<li>All fans running &gt; minimum RPM</li>
<li>Temperatures well below limits</li>
<li>Optic power levels within spec</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>Failed PSU (single point of failure)</li>
<li>Fan failure (thermal risk)</li>
<li>High temperatures (cooling insufficient)</li>
<li>Low optical RX power (fiber issue)</li>
</ul>
<h3>Concept 4: Logs Dashboard - Error Filtering</h3>
<p><strong>Purpose:</strong> Aggregate and search switch logs for troubleshooting</p>
<p>The Logs Dashboard gives you a centralized view of syslog messages from all switches. This is essential for troubleshooting—correlating metrics anomalies with log events.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. Syslog Stream</strong></p>
<ul>
<li><strong>Content:</strong> Live stream of syslog messages from all switches</li>
<li><strong>Use Case:</strong> Watch log messages in real-time</li>
<li><strong>Filter By:</strong> Log level, switch, time range</li>
<li><strong>Example:</strong><pre><code>[leaf-01] Oct 17 10:15:23 INFO: BGP neighbor 172.30.128.1 established
[leaf-02] Oct 17 10:15:45 ERROR: Interface Ethernet5 link down
[spine-01] Oct 17 10:16:02 WARNING: High CPU usage detected
</code></pre>
</li>
</ul>
<p><strong>2. Log Level Breakdown</strong></p>
<ul>
<li><strong>Metric:</strong> Count of logs by severity (ERROR, WARNING, INFO, DEBUG)</li>
<li><strong>Healthy State:</strong> Few or no ERRORs</li>
<li><strong>Unhealthy State:</strong> Spike in ERROR count</li>
<li><strong>Example:</strong><pre><code>Last 1 hour:
  ERROR: 2
  WARNING: 12
  INFO: 523
</code></pre>
</li>
</ul>
<p><strong>3. Error Rate Visualization</strong></p>
<ul>
<li><strong>Metric:</strong> Errors per minute over time</li>
<li><strong>Healthy State:</strong> Flat line near zero</li>
<li><strong>Unhealthy State:</strong> Spike indicates incident</li>
<li><strong>Example:</strong> Graph shows spike at 10:15 (correlate with interface down event)</li>
</ul>
<p><strong>4. Log Search / Filtering</strong></p>
<ul>
<li><strong>Capability:</strong> Full-text search across all logs</li>
<li><strong>Use Cases:</strong><ul>
<li>Find all BGP-related errors: <code>bgp AND error</code></li>
<li>Find logs for specific switch: <code>switch=&quot;leaf-01&quot;</code></li>
<li>Find interface events: <code>interface AND (up OR down)</code></li>
</ul>
</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Daily Health Check:</strong> Check ERROR count (should be 0 or near 0)</li>
<li><strong>Troubleshooting:</strong> Search for specific error messages</li>
<li><strong>Incident Investigation:</strong> Correlate log spike with metrics change</li>
<li><strong>Audit Trail:</strong> Review configuration change logs</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>ERROR count = 0 or very low</li>
<li>No unusual log patterns</li>
<li>Warnings are expected/known (e.g., unused ports)</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>ERROR count &gt; 10 in last hour</li>
<li>Repeating error messages</li>
<li>Unexpected critical errors</li>
</ul>
<h3>Concept 5: Node Exporter Dashboard - System Metrics</h3>
<p><strong>Purpose:</strong> Deep dive into Linux OS metrics (CPU, memory, disk, network I/O)</p>
<p>The Node Exporter Dashboard provides detailed visibility into the SONiC operating system running on each switch. This is useful for diagnosing performance issues or resource exhaustion.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. CPU Utilization</strong></p>
<ul>
<li><strong>Metric:</strong> Percentage breakdown (user, system, idle, iowait)</li>
<li><strong>Healthy State:</strong> &lt; 70% total utilization, low iowait</li>
<li><strong>Unhealthy State:</strong> &gt; 80% sustained, high iowait (disk bottleneck)</li>
<li><strong>Example:</strong><pre><code>User: 25%
System: 15%
IOWait: 5%
Idle: 55%
Total: 45% (healthy)
</code></pre>
</li>
</ul>
<p><strong>2. Load Average</strong></p>
<ul>
<li><strong>Metric:</strong> 1-min, 5-min, 15-min load average</li>
<li><strong>Healthy State:</strong> Load &lt; number of CPUs</li>
<li><strong>Unhealthy State:</strong> Load &gt; CPUs (queued processes)</li>
<li><strong>Example:</strong><pre><code>1-min: 2.5
5-min: 2.2
15-min: 1.8
(Assuming 4 CPUs: healthy - load &lt; 4)
</code></pre>
</li>
</ul>
<p><strong>3. Memory Usage</strong></p>
<ul>
<li><strong>Metric:</strong> Total, used, available, buffers/cache</li>
<li><strong>Healthy State:</strong> Available &gt; 20% of total</li>
<li><strong>Unhealthy State:</strong> Available &lt; 10% (memory pressure)</li>
<li><strong>Example:</strong><pre><code>Total: 16 GB
Used: 10 GB
Available: 5 GB (31% - healthy)
</code></pre>
</li>
</ul>
<p><strong>4. Disk Space</strong></p>
<ul>
<li><strong>Metric:</strong> Used/available per filesystem</li>
<li><strong>Healthy State:</strong> &lt; 80% used</li>
<li><strong>Unhealthy State:</strong> &gt; 90% used (risk of full disk)</li>
<li><strong>Example:</strong><pre><code>/: 45% used (OK)
/var/log: 92% used (WARNING - rotate logs)
</code></pre>
</li>
</ul>
<p><strong>5. Disk I/O</strong></p>
<ul>
<li><strong>Metric:</strong> Read/write operations per second, throughput</li>
<li><strong>Use Case:</strong> Identify disk bottlenecks</li>
<li><strong>Example:</strong><pre><code>Read: 150 IOPS, 25 MB/s
Write: 300 IOPS, 50 MB/s
</code></pre>
</li>
</ul>
<p><strong>6. Network Throughput (All Interfaces)</strong></p>
<ul>
<li><strong>Metric:</strong> Total bytes in/out across all network interfaces</li>
<li><strong>Use Case:</strong> Overall switch traffic</li>
<li><strong>Example:</strong><pre><code>In: 5 Gbps
Out: 4.8 Gbps
</code></pre>
</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Performance Troubleshooting:</strong> Identify if switch CPU/memory is bottleneck</li>
<li><strong>Capacity Planning:</strong> Track resource usage trends over time</li>
<li><strong>Disk Management:</strong> Proactively clear logs before disk fills</li>
<li><strong>Baseline Understanding:</strong> Know &quot;normal&quot; resource usage for comparison</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>CPU &lt; 70%, low iowait</li>
<li>Memory available &gt; 20%</li>
<li>Disk usage &lt; 80%</li>
<li>Load average &lt; CPU count</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>CPU sustained &gt; 80%</li>
<li>High iowait (disk bottleneck)</li>
<li>Memory available &lt; 10%</li>
<li>Disk &gt; 90% full</li>
</ul>
<h3>Concept 6: Switch Critical Resources Dashboard - ASIC Limits</h3>
<p><strong>Purpose:</strong> Monitor programmable ASIC hardware table capacity (prevents resource exhaustion)</p>
<p>The Switch Critical Resources Dashboard is unique to network switches. Unlike CPU or memory, ASIC resources are <strong>hardware limits that cannot be increased</strong>. When an ASIC table fills, the switch cannot accept new entries, causing connectivity failures.</p>
<p><strong>Key Panels:</strong></p>
<p><strong>1. IPv4 Route Table</strong></p>
<ul>
<li><strong>Metric:</strong> Routes used / routes available</li>
<li><strong>Healthy State:</strong> &lt; 80% capacity</li>
<li><strong>Critical State:</strong> &gt; 90% (risk of route installation failure)</li>
<li><strong>Example:</strong><pre><code>leaf-01: 15,000 / 32,768 routes (46% - OK)
spine-01: 28,000 / 32,768 routes (85% - WARNING)
</code></pre>
</li>
</ul>
<p><strong>2. IPv4 Nexthop Table</strong></p>
<ul>
<li><strong>Metric:</strong> Nexthops used / available</li>
<li><strong>Use Case:</strong> Tracks ECMP paths</li>
<li><strong>Example:</strong><pre><code>Used: 2,048 / 4,096 (50% - OK)
</code></pre>
</li>
</ul>
<p><strong>3. IPv4 Neighbor (ARP) Table</strong></p>
<ul>
<li><strong>Metric:</strong> ARP entries used / available</li>
<li><strong>Healthy State:</strong> &lt; 80% capacity</li>
<li><strong>Risk:</strong> Large subnets with many hosts can exhaust ARP table</li>
<li><strong>Example:</strong><pre><code>Used: 8,192 / 16,384 (50% - OK)
Used: 15,500 / 16,384 (95% - CRITICAL)
</code></pre>
</li>
</ul>
<p><strong>4. FDB (Forwarding Database) Capacity</strong></p>
<ul>
<li><strong>Metric:</strong> MAC addresses learned / capacity</li>
<li><strong>Use Case:</strong> Layer 2 forwarding table</li>
<li><strong>Example:</strong><pre><code>Used: 12,000 / 32,768 MACs (37% - OK)
</code></pre>
</li>
</ul>
<p><strong>5. ACL Table Usage</strong></p>
<ul>
<li><strong>Metric:</strong> ACL entries used / available</li>
<li><strong>Use Case:</strong> Security rules, permit lists</li>
<li><strong>Example:</strong><pre><code>Used: 512 / 2,048 (25% - OK)
</code></pre>
</li>
</ul>
<p><strong>6. IPMC (IP Multicast) Table</strong></p>
<ul>
<li><strong>Metric:</strong> Multicast entries used / available</li>
<li><strong>Use Case:</strong> Multicast routing (if enabled)</li>
</ul>
<p><strong>Critical Concept: Hardware Limits</strong></p>
<p>Unlike CPU or memory, ASIC resources are <strong>fixed at manufacture</strong>:</p>
<ul>
<li>Cannot be upgraded</li>
<li>Cannot be expanded</li>
<li>Exhaustion causes hard failures (not performance degradation)</li>
</ul>
<p><strong>When ASIC tables fill:</strong></p>
<ul>
<li>New routes rejected (connectivity lost)</li>
<li>New ARP entries fail (hosts unreachable)</li>
<li>ACL rules not applied (security gaps)</li>
</ul>
<p><strong>Dashboard Actions:</strong></p>
<ul>
<li><strong>Daily Health Check:</strong> Verify no resource &gt; 90% used</li>
<li><strong>Capacity Planning:</strong> Identify resources trending toward limits</li>
<li><strong>Scale Planning:</strong> If ARP table nearing limit, consider subnet redesign</li>
<li><strong>Alert Thresholds:</strong> Set alerts at 80% capacity</li>
</ul>
<p><strong>What &quot;Healthy&quot; Looks Like:</strong></p>
<ul>
<li>All resources &lt; 80% capacity</li>
<li>Stable usage (not rapidly growing)</li>
<li>Headroom for growth</li>
</ul>
<p><strong>What &quot;Unhealthy&quot; Looks Like:</strong></p>
<ul>
<li>Any resource &gt; 90% (immediate risk)</li>
<li>Rapidly growing usage (trend toward exhaustion)</li>
<li>No mitigation plan for full tables</li>
</ul>
<h2>Troubleshooting</h2>
<h3>Issue 1: Dashboard Shows &quot;No Data&quot;</h3>
<p><strong>Symptom:</strong> Panels empty or showing &quot;No Data&quot;</p>
<p><strong>Possible Causes:</strong></p>
<ul>
<li>Prometheus not receiving metrics</li>
<li>Time range issue</li>
<li>Data source misconfigured</li>
</ul>
<p><strong>Fix:</strong></p>
<ol>
<li><p><strong>Check Prometheus targets:</strong></p>
<ul>
<li>Navigate to <a href=""http://localhost:9090/targets"">http://localhost:9090/targets</a></li>
<li>Verify fabric-proxy target is UP</li>
<li>Check last scrape time</li>
</ul>
</li>
<li><p><strong>Adjust time range:</strong></p>
<ul>
<li>Click time range picker (top right)</li>
<li>Try &quot;Last 5 minutes&quot; or &quot;Last 1 hour&quot;</li>
<li>Ensure time range is within data retention (15 days)</li>
</ul>
</li>
<li><p><strong>Verify data source:</strong></p>
<ul>
<li>Grafana → Configuration → Data Sources</li>
<li>Ensure Prometheus is default and URL is correct</li>
</ul>
</li>
</ol>
<h3>Issue 2: Can&#39;t Find Specific Dashboard</h3>
<p><strong>Symptom:</strong> Dashboard not appearing in list</p>
<p><strong>Possible Causes:</strong></p>
<ul>
<li>Dashboard not imported</li>
<li>Search term incorrect</li>
<li>Dashboard in different folder</li>
</ul>
<p><strong>Fix:</strong></p>
<ol>
<li><p><strong>Search by name:</strong></p>
<ul>
<li>Use search box in Dashboards menu</li>
<li>Try partial names: &quot;Fabric&quot;, &quot;Interfaces&quot;, &quot;Platform&quot;</li>
</ul>
</li>
<li><p><strong>Check dashboard list:</strong></p>
<ul>
<li>Dashboards → Browse</li>
<li>Look in all folders</li>
</ul>
</li>
<li><p><strong>Import if missing:</strong></p>
<ul>
<li>Dashboards → Import</li>
<li>Upload JSON from <code>/docs/user-guide/boards/</code> directory</li>
</ul>
</li>
</ol>
<h3>Issue 3: Unsure if Metric Value is &quot;Healthy&quot;</h3>
<p><strong>Symptom:</strong> Panel shows value but unclear if it&#39;s good/bad</p>
<p><strong>Cause:</strong> Missing context about healthy ranges</p>
<p><strong>Fix:</strong></p>
<p>Reference the healthy/unhealthy criteria in this module:</p>
<table>
<thead>
<tr>
<th>Dashboard</th>
<th>Metric</th>
<th>Healthy</th>
<th>Unhealthy</th>
</tr>
</thead>
<tbody><tr>
<td><strong>BGP</strong></td>
<td>Sessions down</td>
<td>0</td>
<td>&gt; 0</td>
</tr>
<tr>
<td><strong>Interfaces</strong></td>
<td>Utilization</td>
<td>&lt; 70%</td>
<td>&gt; 90%</td>
</tr>
<tr>
<td><strong>Platform</strong></td>
<td>CPU temp</td>
<td>&lt; 70°C</td>
<td>&gt; 80°C</td>
</tr>
<tr>
<td><strong>Platform</strong></td>
<td>ASIC temp</td>
<td>&lt; 80°C</td>
<td>&gt; 90°C</td>
</tr>
<tr>
<td><strong>Platform</strong></td>
<td>Fan speed</td>
<td>&gt; 3000 RPM</td>
<td>0 RPM</td>
</tr>
<tr>
<td><strong>Logs</strong></td>
<td>ERROR count/hour</td>
<td>&lt; 5</td>
<td>&gt; 10</td>
</tr>
<tr>
<td><strong>ASIC Resources</strong></td>
<td>Capacity</td>
<td>&lt; 80%</td>
<td>&gt; 90%</td>
</tr>
</tbody></table>
<h3>Issue 4: Metric Interpretation Confusion</h3>
<p><strong>Symptom:</strong> Counter value very large (e.g., billions)</p>
<p><strong>Cause:</strong> Viewing raw counter instead of rate</p>
<p><strong>Fix:</strong></p>
<ol>
<li><p><strong>Check if panel uses rate():</strong></p>
<ul>
<li>Click panel title → Edit</li>
<li>Check PromQL query</li>
<li>Should use <code>rate(metric[5m])</code> for counters</li>
</ul>
</li>
<li><p><strong>Adjust panel query:</strong></p>
<ul>
<li>For byte counters: <code>rate(interface_bytes_out[5m]) * 8</code> (converts to bps)</li>
<li>For packet counters: <code>rate(interface_packets_out[5m])</code></li>
</ul>
</li>
<li><p><strong>Reference Module 3.1:</strong></p>
<ul>
<li>Review counter vs gauge concepts</li>
<li>Remember: counters always increase, use rate() to see change</li>
</ul>
</li>
</ol>
<h2>Resources</h2>
<h3>Grafana Documentation</h3>
<ul>
<li><a href=""https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/"">Grafana Dashboard Best Practices</a></li>
<li><a href=""https://grafana.com/docs/grafana/latest/datasources/prometheus/"">Prometheus Data Source</a></li>
<li><a href=""https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/time-series/"">Time Series Visualizations</a></li>
</ul>
<h3>Hedgehog Documentation</h3>
<ul>
<li>Hedgehog Observability Guide (OBSERVABILITY.md in research folder)</li>
<li>Hedgehog Fabric Controller Documentation</li>
<li>Grafana Dashboard JSON files (<code>/docs/user-guide/boards/</code>)</li>
</ul>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""module-3.1-fabric-telemetry-overview.md"">Module 3.1: Fabric Telemetry Overview</a></li>
<li>Next: Module 3.3: Events &amp; Status Monitoring (coming soon)</li>
<li>Pathway: Network Like a Hyperscaler</li>
</ul>
<h3>Dashboard Quick Reference</h3>
<p><strong>Dashboard Access:</strong></p>
<ul>
<li>URL: <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li>Login: admin / prom-operator</li>
<li>Location: Dashboards → Browse</li>
</ul>
<p><strong>6 Hedgehog Dashboards:</strong></p>
<ol>
<li><strong>Hedgehog Fabric</strong> - BGP underlay health</li>
<li><strong>Hedgehog Interfaces</strong> - Interface state, traffic, errors</li>
<li><strong>Hedgehog Platform</strong> - PSU, fans, temperature, optics</li>
<li><strong>Hedgehog Logs</strong> - Syslog aggregation and search</li>
<li><strong>Node Exporter</strong> - Linux system metrics (CPU, memory, disk)</li>
<li><strong>Switch Critical Resources</strong> - ASIC resource capacity</li>
</ol>
<p><strong>Common Time Ranges:</strong></p>
<ul>
<li>Last 5 minutes (real-time monitoring)</li>
<li>Last 1 hour (health checks)</li>
<li>Last 24 hours (daily trends)</li>
<li>Last 7 days (weekly patterns)</li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve learned to interpret Grafana dashboards for daily fabric health checks. Ready to correlate metrics with Kubernetes events in Module 3.3.</p>
",302,15,,,"hedgehog,fabric,grafana,dashboards,observability,monitoring"
198578603897,Decommission & Cleanup,fabric-operations-decommission-cleanup,[object Object],"Learn safe decommissioning workflows for VPCs and VPCAttachments. Understand cleanup order, validation, and lifecycle management best practices.","<h2>Introduction</h2>
<p>In Modules 2.1-2.3, you completed the provisioning workflow: creating the <code>web-app-prod</code> VPC, attaching two servers, and validating connectivity. Your infrastructure is operational and serving traffic. But what happens when it&#39;s time to decommission?</p>
<p>Every resource has a lifecycle—from creation to deletion. Proper decommissioning is as critical as proper provisioning. Delete resources in the wrong order, and you risk orphaned configurations, switch misconfigurations, or leaving resources that consume fabric capacity unnecessarily.</p>
<p>In this final module of Course 2, you&#39;ll learn safe decommissioning workflows: deleting VPCAttachments before VPCs, validating cleanup completion, and understanding when to keep versus delete resources. This completes the full Day 1 operations lifecycle: <strong>Provision → Attach → Validate → Cleanup</strong>.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Decommission VPCAttachments safely</strong> - Remove server-to-VPC connections in correct order</li>
<li><strong>Decommission VPCs safely</strong> - Delete VPCs after all attachments removed</li>
<li><strong>Validate cleanup completion</strong> - Verify resources deleted and switches reconfigured</li>
<li><strong>Understand lifecycle management</strong> - Know when to keep vs delete resources</li>
<li><strong>Apply Day 1 operations knowledge</strong> - Complete the provisioning→validation→cleanup workflow</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 2.1 completion (VPC Provisioning Essentials)</li>
<li>Module 2.2 completion (VPC Attachments)</li>
<li>Module 2.3 completion (Connectivity Validation)</li>
<li>Existing <code>web-app-prod</code> VPC with 2 VPCAttachments (from previous modules)</li>
<li>kubectl access to Hedgehog fabric</li>
</ul>
<h2>Scenario: Application Decommission</h2>
<p>The <code>web-app-prod</code> application is being decommissioned after a successful migration to a new platform. Your task: safely remove the VPCAttachments and VPC without disrupting other fabric operations. You&#39;ll follow the proper cleanup order (attachments first, then VPC), validate each step, and verify the fabric returns to a clean state. This is your opportunity to complete the full lifecycle workflow and demonstrate Day 1 operations mastery.</p>
<h2>Lab Steps</h2>
<h3>Step 1: Pre-Decommission Review</h3>
<p><strong>Objective:</strong> Identify resources to delete and understand current state</p>
<p>Before deleting anything, understand what you&#39;re removing and document the current state for validation.</p>
<p>List current VPCs:</p>
<pre><code class=""language-bash"">kubectl get vpcs
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME           AGE
web-app-prod   2h
</code></pre>
<p>List current VPCAttachments:</p>
<pre><code class=""language-bash"">kubectl get vpcattachments
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME                       AGE
server-01-web-servers      2h
server-05-worker-nodes     2h
</code></pre>
<p>Review what will be deleted:</p>
<p><strong>Resources to decommission:</strong></p>
<ul>
<li><strong>web-app-prod VPC</strong> with 2 subnets:<ul>
<li>web-servers (10.10.10.0/24, VLAN 1010)</li>
<li>worker-nodes (10.10.20.0/24, VLAN 1020, DHCP enabled)</li>
</ul>
</li>
<li><strong>server-01-web-servers VPCAttachment</strong> (MCLAG connection to leaf-01/leaf-02)</li>
<li><strong>server-05-worker-nodes VPCAttachment</strong> (ESLAG connection to leaf-03/leaf-04)</li>
</ul>
<p>Document current state for later validation:</p>
<pre><code class=""language-bash""># Save current VPC configuration
kubectl get vpc web-app-prod -o yaml &gt; web-app-prod-backup.yaml

# List attachments for documentation
kubectl get vpcattachments -o wide
</code></pre>
<p><strong>Critical decommissioning order:</strong></p>
<pre><code>1. Delete VPCAttachments FIRST (server-01-web-servers, server-05-worker-nodes)
2. Validate all attachments removed
3. Delete VPC LAST (web-app-prod)
4. Validate cleanup complete
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Identified all resources to delete (1 VPC, 2 VPCAttachments)</li>
<li>✅ Documented current state</li>
<li>✅ Understand decommissioning order: <strong>Attachments → VPC</strong></li>
</ul>
<h3>Step 2: Delete VPCAttachments (Must Delete First)</h3>
<p><strong>Objective:</strong> Remove server-to-VPC connections before deleting VPC</p>
<p><strong>CRITICAL RULE: Always delete VPCAttachments BEFORE deleting the VPC.</strong></p>
<p>VPCAttachments depend on the VPC they reference. Deleting the VPC first would leave orphaned attachments that reference a non-existent VPC, causing reconciliation errors.</p>
<p>Delete the first VPCAttachment (server-01):</p>
<pre><code class=""language-bash"">kubectl delete vpcattachment server-01-web-servers
</code></pre>
<p>Expected output:</p>
<pre><code>vpcattachment.vpc.githedgehog.com &quot;server-01-web-servers&quot; deleted
</code></pre>
<p>Delete the second VPCAttachment (server-05):</p>
<pre><code class=""language-bash"">kubectl delete vpcattachment server-05-worker-nodes
</code></pre>
<p>Expected output:</p>
<pre><code>vpcattachment.vpc.githedgehog.com &quot;server-05-worker-nodes&quot; deleted
</code></pre>
<p>Verify VPCAttachments are deleted:</p>
<pre><code class=""language-bash""># List all VPCAttachments (web-app resources should be gone)
kubectl get vpcattachments

# Specifically check for web-app attachments (should return empty)
kubectl get vpcattachments | grep web-app
</code></pre>
<p>Check cleanup reconciliation events:</p>
<pre><code class=""language-bash""># View events for attachment deletions
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20

# Look for events indicating cleanup reconciliation
</code></pre>
<p><strong>What happens during VPCAttachment deletion:</strong></p>
<ol>
<li>Fabric controller detects VPCAttachment deletion</li>
<li>Identifies affected switches (leaf-01, leaf-02 for server-01; leaf-03, leaf-04 for server-05)</li>
<li>Computes cleanup configuration:<ul>
<li>Remove VLAN from server-facing ports</li>
<li>Remove VXLAN tunnels (if no other VPCs using them)</li>
<li>Remove BGP EVPN routes</li>
</ul>
</li>
<li>Updates Agent CRDs (removes config from Agent spec)</li>
<li>Switch agents apply cleanup (unconfigure ports)</li>
<li>VPCAttachment deleted from Kubernetes</li>
</ol>
<p><strong>Wait for reconciliation to complete</strong> (typically 10-30 seconds).</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Both VPCAttachments deleted successfully</li>
<li>✅ <code>kubectl get vpcattachments</code> shows no web-app resources</li>
<li>✅ Events show successful cleanup reconciliation</li>
<li>✅ No error events</li>
</ul>
<h3>Step 3: Delete VPC (After Attachments Removed)</h3>
<p><strong>Objective:</strong> Remove VPC only after all attachments are gone</p>
<p>Before deleting the VPC, verify no attachments remain:</p>
<pre><code class=""language-bash""># Check for any remaining attachments referencing web-app-prod
kubectl get vpcattachments | grep web-app-prod

# Expected: No results (empty output)
</code></pre>
<p><strong>If any attachments remain, DO NOT proceed.</strong> Delete them first.</p>
<p>Delete the VPC:</p>
<pre><code class=""language-bash"">kubectl delete vpc web-app-prod
</code></pre>
<p>Expected output:</p>
<pre><code>vpc.vpc.githedgehog.com &quot;web-app-prod&quot; deleted
</code></pre>
<p>Verify VPC deletion:</p>
<pre><code class=""language-bash""># Attempt to get the VPC (should return NotFound error)
kubectl get vpc web-app-prod
</code></pre>
<p>Expected output:</p>
<pre><code>Error from server (NotFound): vpcs.vpc.githedgehog.com &quot;web-app-prod&quot; not found
</code></pre>
<p>This error is <strong>expected and correct</strong>—it confirms the VPC is deleted.</p>
<p>Check VPC cleanup events:</p>
<pre><code class=""language-bash""># View recent events
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20
</code></pre>
<p>List remaining VPCs to confirm:</p>
<pre><code class=""language-bash""># web-app-prod should not appear in this list
kubectl get vpcs
</code></pre>
<p><strong>What happens during VPC deletion:</strong></p>
<ol>
<li>Fabric controller verifies no attachments exist (deletion would fail if attachments remain)</li>
<li>Removes VPC configuration:<ul>
<li>VXLAN VNI released back to namespace pool</li>
<li>VLAN namespace entries removed</li>
<li>IP namespace entries removed</li>
</ul>
</li>
<li>VPC deleted from Kubernetes</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC deleted successfully</li>
<li>✅ <code>kubectl get vpc web-app-prod</code> returns NotFound error</li>
<li>✅ Events show successful VPC cleanup</li>
<li>✅ No error events</li>
<li>✅ VPC no longer appears in <code>kubectl get vpcs</code></li>
</ul>
<h3>Step 4: Validate Cleanup Completion</h3>
<p><strong>Objective:</strong> Verify all resources deleted and switches reconfigured</p>
<p>Verify VPC completely removed:</p>
<pre><code class=""language-bash""># Should return NotFound error
kubectl get vpc web-app-prod
</code></pre>
<p>Verify VPCAttachments completely removed:</p>
<pre><code class=""language-bash""># web-app resources should be gone
kubectl get vpcattachments | grep web-app
</code></pre>
<p>Check Agent CRDs for switch cleanup:</p>
<pre><code class=""language-bash""># Check leaf-01 (should no longer have VLAN 1010 for server-01)
kubectl get agent leaf-01 -n fab -o yaml | grep -A 5 &quot;1010&quot;

# Check leaf-03 (should no longer have VLAN 1020 for server-05)
kubectl get agent leaf-03 -n fab -o yaml | grep -A 5 &quot;1020&quot;
</code></pre>
<p><strong>Expected result:</strong> VLANs 1010 and 1020 should be removed from the relevant Agent CRDs, indicating switches have been reconfigured.</p>
<p>Verify no orphaned resources:</p>
<pre><code class=""language-bash""># List all VPCAttachments to ensure none reference deleted VPC
kubectl get vpcattachments -o yaml | grep &quot;web-app-prod&quot;

# Expected: No results
</code></pre>
<p>Review cleanup event timeline:</p>
<pre><code class=""language-bash""># View all recent events to see cleanup progression
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -30
</code></pre>
<p><strong>Validation checklist:</strong></p>
<ul>
<li>✅ VPC web-app-prod deleted (NotFound error when queried)</li>
<li>✅ VPCAttachments deleted (no web-app resources in list)</li>
<li>✅ Agent CRDs updated (VLANs removed from switch configurations)</li>
<li>✅ No orphaned resources</li>
<li>✅ Events show successful cleanup reconciliation</li>
<li>✅ Fabric returned to clean state</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ All validation checks passed</li>
<li>✅ Fabric state clean (no web-app resources)</li>
<li>✅ Switches reconfigured (VLANs removed)</li>
<li>✅ No errors in cleanup process</li>
</ul>
<h3>Step 5: Course 2 Completion - Full Lifecycle Review</h3>
<p><strong>Objective:</strong> Understand the complete Day 1 operations workflow</p>
<p>You&#39;ve now completed the <strong>entire Day 1 operations lifecycle</strong> for Hedgehog Fabric:</p>
<p><strong>Module 2.1: Provision VPC</strong></p>
<ul>
<li>Created web-app-prod VPC with two subnets</li>
<li>Configured IPv4 (static) and DHCPv4 subnets</li>
<li>Learned VPC CRD structure and reconciliation</li>
</ul>
<p><strong>Module 2.2: Attach Servers</strong></p>
<ul>
<li>Attached server-01 (MCLAG) to web-servers subnet</li>
<li>Attached server-05 (ESLAG) to worker-nodes subnet</li>
<li>Understood connection types and VPCAttachment workflow</li>
</ul>
<p><strong>Module 2.3: Validate Connectivity</strong></p>
<ul>
<li>Validated VPC and VPCAttachment configurations</li>
<li>Inspected Agent CRDs for switch-level state</li>
<li>Learned event-based validation and troubleshooting</li>
</ul>
<p><strong>Module 2.4: Decommission &amp; Cleanup</strong> (this module)</p>
<ul>
<li>Deleted VPCAttachments in correct order</li>
<li>Deleted VPC after attachments removed</li>
<li>Validated cleanup completion</li>
</ul>
<p><strong>Complete Lifecycle:</strong></p>
<pre><code>Provision → Attach → Validate → Operate → Decommission
</code></pre>
<p><strong>When to keep vs delete resources:</strong></p>
<p><strong>Keep resources when:</strong></p>
<ul>
<li>Application temporarily offline (maintenance, updates)</li>
<li>Troubleshooting connectivity issues (keep for debugging)</li>
<li>Resource reserved for future use</li>
<li>Testing in progress (don&#39;t delete mid-test)</li>
</ul>
<p><strong>Delete resources when:</strong></p>
<ul>
<li>Application permanently decommissioned</li>
<li>Migration to new VPC complete</li>
<li>Testing finished (dev/test environments)</li>
<li>Resource no longer needed</li>
<li>Cleaning up failed deployments</li>
</ul>
<p><strong>Deletion impact:</strong></p>
<ul>
<li><strong>VPCAttachment deletion</strong>: Server immediately loses VPC connectivity</li>
<li><strong>VPC deletion</strong>: All subnets, VLANs, and routing removed</li>
<li><strong>Recovery</strong>: Can re-create from YAML manifests (but auto-assigned VLANs may change)</li>
</ul>
<p><strong>Production best practices:</strong></p>
<ol>
<li><strong>Verify before deleting</strong>: Confirm with application team</li>
<li><strong>Plan maintenance window</strong>: Deletion causes immediate connectivity loss</li>
<li><strong>Document deletion</strong>: Record why, when, who approved</li>
<li><strong>Check dependencies</strong>: Ensure no other resources depend on VPC</li>
<li><strong>Validate after deletion</strong>: Confirm cleanup completed successfully</li>
<li><strong>Save manifests</strong>: Keep YAML backups for recovery if needed</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Understand complete Day 1 lifecycle workflow</li>
<li>✅ Know when to delete vs keep resources</li>
<li>✅ Understand deletion impact and recovery options</li>
<li>✅ Ready for production decommissioning tasks</li>
<li>✅ <strong>Course 2 Complete!</strong></li>
</ul>
<h2>Concepts &amp; Deep Dive</h2>
<h3>Decommissioning Order: Why Attachments First</h3>
<p><strong>The Golden Rule: Always delete VPCAttachments BEFORE deleting VPCs.</strong></p>
<p>This order is not a best practice—it&#39;s a <strong>requirement</strong> for safe decommissioning.</p>
<p><strong>Why this order matters:</strong></p>
<p><strong>1. Dependency chain</strong></p>
<p>VPCAttachments depend on VPCs. Each VPCAttachment references a VPC and subnet:</p>
<pre><code class=""language-yaml"">spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/web-servers  # References VPC
</code></pre>
<p>If you delete the VPC first, VPCAttachments reference a non-existent VPC, causing reconciliation errors.</p>
<p><strong>2. Switch configuration order</strong></p>
<p>Server ports need to be unconfigured (VLANs removed) before the VPC configuration is removed from the fabric. Deleting the VPC first leaves switches partially configured.</p>
<p><strong>3. Orphaned resources</strong></p>
<p>Deleting the VPC first creates orphaned VPCAttachments that no longer serve any purpose but still exist in Kubernetes, consuming resources and causing confusion.</p>
<p><strong>4. Reconciliation failures</strong></p>
<p>The fabric controller cannot properly reconcile VPCAttachments without the parent VPC definition. Events will show errors, and manual cleanup becomes necessary.</p>
<p><strong>What happens if you delete VPC first?</strong></p>
<pre><code class=""language-bash""># DON&#39;T DO THIS - Wrong order!
kubectl delete vpc web-app-prod  # VPC deleted
kubectl delete vpcattachment server-01-web-servers  # Attachment references deleted VPC
</code></pre>
<p><strong>Consequences:</strong></p>
<ul>
<li>VPCAttachments reference non-existent VPC</li>
<li>Events show reconciliation errors: &quot;VPC web-app-prod not found&quot;</li>
<li>Switch ports may not be properly unconfigured</li>
<li>Manual cleanup required</li>
<li>Agent CRDs may retain partial configuration</li>
</ul>
<p><strong>Correct decommissioning order:</strong></p>
<pre><code class=""language-bash""># Step 1: Delete ALL VPCAttachments first
kubectl delete vpcattachment server-01-web-servers
kubectl delete vpcattachment server-05-worker-nodes

# Step 2: Verify all attachments deleted
kubectl get vpcattachments | grep web-app-prod  # Should be empty

# Step 3: Delete VPC
kubectl delete vpc web-app-prod

# Step 4: Validate cleanup complete
kubectl get vpc web-app-prod  # Should return NotFound
</code></pre>
<p><strong>Kubernetes safeguards:</strong></p>
<p>Kubernetes has some protections:</p>
<ul>
<li>If you attempt to delete a VPC with active attachments, the deletion may be blocked</li>
<li>Finalizers prevent premature deletion in some cases</li>
</ul>
<p>However, <strong>don&#39;t rely on safeguards alone</strong>. Follow the correct order as a matter of operational discipline.</p>
<h3>Cleanup Reconciliation Process</h3>
<p>Understanding what happens during deletion helps troubleshoot cleanup issues.</p>
<p><strong>VPCAttachment deletion reconciliation:</strong></p>
<p><strong>1. Kubernetes receives delete request</strong></p>
<pre><code class=""language-bash"">kubectl delete vpcattachment server-01-web-servers
</code></pre>
<p>Kubernetes marks the resource for deletion.</p>
<p><strong>2. Fabric Controller detects deletion</strong></p>
<p>The fabric controller watches for deleted VPCAttachment CRDs and picks up the deletion event.</p>
<p><strong>3. Identifies affected switches</strong></p>
<p>Controller determines which switches serve this connection:</p>
<ul>
<li>server-01 (MCLAG): leaf-01 and leaf-02</li>
<li>server-05 (ESLAG): leaf-03 and leaf-04</li>
</ul>
<p><strong>4. Computes cleanup configuration</strong></p>
<p>Controller calculates what needs to be removed:</p>
<ul>
<li>Remove VLAN from server-facing ports (e.g., VLAN 1010 from port E1/5)</li>
<li>Remove VXLAN tunnels (if no other VPCs using the same VNI)</li>
<li>Remove BGP EVPN routes for this VPC/subnet</li>
<li>Remove DHCP relay configuration (if applicable)</li>
</ul>
<p><strong>5. Updates Agent CRDs</strong></p>
<p>Controller removes configuration from affected Agent CRD specs:</p>
<pre><code class=""language-yaml""># Before deletion: Agent spec contains VLAN 1010
spec:
  ports:
    E1/5:
      mode: access
      vlan: 1010

# After deletion: VLAN 1010 removed from port config
spec:
  ports:
    E1/5:
      mode: disabled  # or removed entirely
</code></pre>
<p><strong>6. Switch agents apply cleanup</strong></p>
<p>Each switch agent (running on or for the switch):</p>
<ul>
<li>Watches its Agent CRD</li>
<li>Detects spec change (VLAN removed)</li>
<li>Applies configuration to physical switch via gNMI</li>
<li>Unconfigures port, removes VLAN, updates routing</li>
</ul>
<p><strong>7. VPCAttachment deleted</strong></p>
<p>Once reconciliation completes, the VPCAttachment is fully removed from Kubernetes.</p>
<p><strong>Timeline:</strong></p>
<ul>
<li>VPCAttachment deletion request: &lt; 1 second</li>
<li>Reconciliation: 10-30 seconds (depends on fabric size)</li>
<li>Switch configuration cleanup: 10-20 seconds</li>
<li>Full cleanup: 30-60 seconds</li>
</ul>
<hr>
<p><strong>VPC deletion reconciliation:</strong></p>
<p><strong>1. Kubernetes receives delete request</strong></p>
<pre><code class=""language-bash"">kubectl delete vpc web-app-prod
</code></pre>
<p><strong>2. Fabric Controller verifies no attachments</strong></p>
<p>Controller checks if any VPCAttachments reference this VPC. If attachments exist, deletion may be blocked or delayed.</p>
<p><strong>3. Removes VPC configuration</strong></p>
<ul>
<li><strong>VXLAN VNI released</strong>: VNI returned to namespace pool for reuse</li>
<li><strong>VLAN namespace entries removed</strong>: VLANs 1010 and 1020 freed</li>
<li><strong>IP namespace entries removed</strong>: Subnet CIDRs freed</li>
</ul>
<p><strong>4. VPC deleted</strong></p>
<p>VPC removed from Kubernetes etcd.</p>
<p><strong>Timeline:</strong></p>
<ul>
<li>VPC deletion: &lt; 5 seconds (namespace cleanup is fast)</li>
</ul>
<h3>When to Keep vs Delete Resources</h3>
<p>Decommissioning is not always the right choice. Understanding when to keep versus delete resources is critical for production operations.</p>
<p><strong>Keep resources when:</strong></p>
<p><strong>1. Application temporarily offline</strong></p>
<ul>
<li>Planned maintenance windows</li>
<li>Software updates or patches</li>
<li>Database migrations</li>
<li>Temporary scaling down</li>
</ul>
<p><strong>Why:</strong> Re-creating VPCs and attachments later is more work than keeping them.</p>
<p><strong>2. Troubleshooting connectivity issues</strong></p>
<ul>
<li>Debugging network problems</li>
<li>Investigating performance issues</li>
<li>Testing configuration changes</li>
</ul>
<p><strong>Why:</strong> Deleting resources during troubleshooting eliminates evidence and makes root cause analysis harder.</p>
<p><strong>3. Resource reserved for future use</strong></p>
<ul>
<li>Pre-provisioned for upcoming deployment</li>
<li>Capacity planning (staging environment ready)</li>
<li>Reserved for specific team or project</li>
</ul>
<p><strong>Why:</strong> Reprovisioning later may result in different auto-assigned VLANs or other configuration drift.</p>
<p><strong>4. Testing in progress</strong></p>
<ul>
<li>Development environments with active work</li>
<li>Integration tests running</li>
<li>Performance benchmarks in flight</li>
</ul>
<p><strong>Why:</strong> Deleting mid-test invalidates results and wastes effort.</p>
<hr>
<p><strong>Delete resources when:</strong></p>
<p><strong>1. Application permanently decommissioned</strong></p>
<ul>
<li>Service retired, no longer needed</li>
<li>Business unit shut down</li>
<li>Product end-of-life</li>
</ul>
<p><strong>Why:</strong> Keeping unused resources wastes fabric capacity and creates confusion.</p>
<p><strong>2. Migration to new VPC complete</strong></p>
<ul>
<li>Traffic cut over to new infrastructure</li>
<li>Old VPC verified empty</li>
<li>Rollback window passed</li>
</ul>
<p><strong>Why:</strong> No reason to keep old infrastructure after successful migration.</p>
<p><strong>3. Testing finished</strong></p>
<ul>
<li>Development testing complete</li>
<li>Staging environment no longer needed</li>
<li>Temporary test infrastructure</li>
</ul>
<p><strong>Why:</strong> Test environments should be ephemeral to free resources for other tests.</p>
<p><strong>4. Resource no longer needed</strong></p>
<ul>
<li>Over-provisioned capacity being scaled down</li>
<li>Duplicate or redundant resources</li>
<li>Misconfigured resources being replaced</li>
</ul>
<p><strong>Why:</strong> Clean up reduces operational complexity.</p>
<p><strong>5. Cleaning up failed deployments</strong></p>
<ul>
<li>VPC provisioned incorrectly (wrong subnets, VLANs)</li>
<li>Attachments created in error</li>
<li>Testing mistakes</li>
</ul>
<p><strong>Why:</strong> Start fresh rather than trying to fix broken configurations.</p>
<hr>
<p><strong>Production decommissioning checklist:</strong></p>
<p>Before deleting resources in production, verify:</p>
<ul>
<li><input disabled="""" type=""checkbox""> Application team confirms decommission approved</li>
<li><input disabled="""" type=""checkbox""> No active traffic to servers in VPC</li>
<li><input disabled="""" type=""checkbox""> Maintenance window scheduled (deletion causes immediate connectivity loss)</li>
<li><input disabled="""" type=""checkbox""> Backup of YAML manifests saved (for recovery if needed)</li>
<li><input disabled="""" type=""checkbox""> Dependencies checked (no other resources depend on this VPC)</li>
<li><input disabled="""" type=""checkbox""> Decommission documented (who, what, when, why)</li>
<li><input disabled="""" type=""checkbox""> Post-deletion validation plan ready</li>
</ul>
<h3>Deletion Impact and Recovery</h3>
<p><strong>VPCAttachment deletion impact:</strong></p>
<p><strong>Immediate effects:</strong></p>
<ul>
<li><strong>Server connectivity</strong>: Server immediately loses VPC connectivity</li>
<li><strong>Active connections</strong>: All active TCP/UDP connections dropped</li>
<li><strong>Switch ports</strong>: VLANs removed from server-facing ports within 10-30 seconds</li>
<li><strong>No warning</strong>: Deletion is immediate, no graceful shutdown</li>
</ul>
<p><strong>What survives:</strong></p>
<ul>
<li><strong>Server OS configuration</strong>: Server network config (static IPs, routes) unchanged</li>
<li><strong>Server itself</strong>: Server CRD and Connection CRD remain</li>
<li><strong>VPC</strong>: VPC still exists and can be attached to other servers</li>
</ul>
<p><strong>Recovery:</strong>
Re-create the VPCAttachment from YAML manifest:</p>
<pre><code class=""language-bash"">kubectl apply -f server-01-attachment.yaml
</code></pre>
<p>Connectivity restores within 30-60 seconds after reconciliation.</p>
<hr>
<p><strong>VPC deletion impact:</strong></p>
<p><strong>Immediate effects:</strong></p>
<ul>
<li><strong>All subnets deleted</strong>: Every subnet in the VPC removed</li>
<li><strong>All VLANs released</strong>: VLANs returned to namespace pool for reuse</li>
<li><strong>All routing removed</strong>: VPC routing tables deleted from fabric</li>
<li><strong>Cannot have attachments</strong>: Deletion blocked if attachments exist</li>
</ul>
<p><strong>What survives:</strong></p>
<ul>
<li><strong>Servers</strong>: Server CRDs and Connection CRDs remain (can attach to other VPCs)</li>
<li><strong>Switches</strong>: Switch hardware unaffected, Agent CRDs updated</li>
</ul>
<p><strong>Recovery:</strong>
Re-create the VPC from YAML manifest:</p>
<pre><code class=""language-bash"">kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<p><strong>Important recovery notes:</strong></p>
<ul>
<li>If VLANs were auto-assigned, new VLANs may differ (namespace reuses freed VLANs)</li>
<li>All VPCAttachments must be re-created after VPC restored</li>
<li>Full recovery time: 1-2 minutes (VPC creation + attachment reconciliation)</li>
</ul>
<p><strong>Cannot delete VPC if attachments exist:</strong></p>
<pre><code class=""language-bash"">kubectl delete vpc web-app-prod
# Error: VPC has active attachments
</code></pre>
<p><strong>Error message (example):</strong></p>
<pre><code>Error: cannot delete VPC &quot;web-app-prod&quot;: active VPCAttachments exist
</code></pre>
<p><strong>Solution:</strong></p>
<ol>
<li>List all attachments: <code>kubectl get vpcattachments | grep web-app-prod</code></li>
<li>Delete each attachment: <code>kubectl delete vpcattachment &lt;name&gt;</code></li>
<li>Retry VPC deletion: <code>kubectl delete vpc web-app-prod</code></li>
</ol>
<h3>Orphaned Resources and Cleanup</h3>
<p><strong>What are orphaned resources?</strong></p>
<p>Orphaned resources are Kubernetes objects that no longer serve a purpose but still exist, consuming resources and causing operational confusion.</p>
<p><strong>Common causes:</strong></p>
<ol>
<li><p><strong>Deleting VPC before VPCAttachments</strong> (most common)</p>
<ul>
<li>VPCAttachments reference non-existent VPC</li>
<li>Reconciliation errors in events</li>
<li>Switch ports may retain partial configuration</li>
</ul>
</li>
<li><p><strong>Manual switch configuration without CRD cleanup</strong></p>
<ul>
<li>Direct switch CLI changes bypassing Hedgehog</li>
<li>Agent CRDs out of sync with switch reality</li>
</ul>
</li>
<li><p><strong>Failed reconciliation leaving partial config</strong></p>
<ul>
<li>Controller or agent pod crashed mid-reconciliation</li>
<li>Network issues during cleanup</li>
<li>Agent CRD spec updated but status not reflecting completion</li>
</ul>
</li>
</ol>
<p><strong>How to identify orphaned VPCAttachments:</strong></p>
<pre><code class=""language-bash""># List all VPCAttachments
kubectl get vpcattachments

# Check each attachment&#39;s VPC reference
kubectl get vpcattachment &lt;name&gt; -o yaml | grep &quot;subnet:&quot;

# Example orphaned attachment:
#   subnet: web-app-prod/web-servers  # VPC doesn&#39;t exist!
</code></pre>
<p>Verify the VPC exists:</p>
<pre><code class=""language-bash"">kubectl get vpc web-app-prod
# Error from server (NotFound): vpcs.vpc.githedgehog.com &quot;web-app-prod&quot; not found
</code></pre>
<p>If VPC doesn&#39;t exist but VPCAttachment does, it&#39;s orphaned.</p>
<p><strong>How to clean up orphaned VPCAttachments:</strong></p>
<pre><code class=""language-bash""># Delete the orphaned VPCAttachment
kubectl delete vpcattachment server-01-web-servers

# Verify deletion
kubectl get vpcattachment server-01-web-servers
# Error from server (NotFound) - expected
</code></pre>
<p>Check Agent CRDs to ensure switch cleanup occurred:</p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o yaml | grep -A 5 &quot;1010&quot;
# Should not show VLAN 1010 configuration
</code></pre>
<p><strong>Prevention strategies:</strong></p>
<ol>
<li><strong>Always follow correct deletion order</strong> (attachments → VPC)</li>
<li><strong>Validate cleanup after each deletion</strong></li>
<li><strong>Use GitOps</strong> (Git is source of truth, prevents manual errors)</li>
<li><strong>Monitor events</strong> for reconciliation errors</li>
<li><strong>Avoid manual switch configuration</strong> (use Hedgehog CRDs only)</li>
</ol>
<h2>Troubleshooting</h2>
<h3>Issue: Cannot delete VPC - &quot;VPC has active attachments&quot;</h3>
<p><strong>Symptom:</strong> <code>kubectl delete vpc web-app-prod</code> fails with error message</p>
<p><strong>Error message:</strong></p>
<pre><code>Error: cannot delete VPC &quot;web-app-prod&quot;: active VPCAttachments exist
</code></pre>
<p><strong>Cause:</strong> One or more VPCAttachments still reference the VPC</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: List all VPCAttachments
kubectl get vpcattachments

# Step 2: Identify attachments referencing this VPC
kubectl get vpcattachments -o yaml | grep &quot;web-app-prod&quot;

# Example output:
#   subnet: web-app-prod/web-servers
#   subnet: web-app-prod/worker-nodes

# Step 3: Delete each attachment
kubectl delete vpcattachment server-01-web-servers
kubectl delete vpcattachment server-05-worker-nodes

# Step 4: Verify all attachments deleted
kubectl get vpcattachments | grep web-app-prod
# Should return empty

# Step 5: Retry VPC deletion
kubectl delete vpc web-app-prod

# Should succeed now
</code></pre>
<h3>Issue: VPCAttachment deleted but switch ports not cleaned up</h3>
<p><strong>Symptom:</strong> Agent CRD still shows VLAN configuration after VPCAttachment deleted</p>
<p><strong>Example:</strong></p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o yaml | grep &quot;1010&quot;
# Still shows VLAN 1010 configuration even though attachment deleted
</code></pre>
<p><strong>Cause:</strong> Reconciliation not complete, or agent pod issue</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Wait for reconciliation to complete (30-60 seconds)
sleep 60

# Step 2: Check again
kubectl get agent leaf-01 -n fab -o yaml | grep &quot;1010&quot;

# Step 3: If still present, check agent pod status
kubectl get pods -n fab | grep agent

# Step 4: Check agent logs for errors
kubectl logs &lt;agent-pod-name&gt; -n fab | tail -50

# Step 5: Check events for reconciliation progress
kubectl get events -n fab --sort-by=&#39;.lastTimestamp&#39; | tail -20

# Step 6: If agent pod crashed, restart it
kubectl delete pod &lt;agent-pod-name&gt; -n fab
# Pod will restart and reapply configuration

# Step 7: Verify cleanup after restart
kubectl get agent leaf-01 -n fab -o yaml | grep &quot;1010&quot;
</code></pre>
<h3>Issue: Orphaned VPCAttachment after VPC accidentally deleted</h3>
<p><strong>Symptom:</strong> VPCAttachment exists but references non-existent VPC</p>
<p><strong>Example:</strong></p>
<pre><code class=""language-bash"">kubectl get vpcattachment server-01-web-servers
# NAME                      AGE
# server-01-web-servers     3h

kubectl describe vpcattachment server-01-web-servers
# Shows error: VPC &quot;web-app-prod&quot; not found
</code></pre>
<p><strong>Cause:</strong> VPC deleted before attachments (wrong order)</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Confirm VPC is gone
kubectl get vpc web-app-prod
# Error from server (NotFound) - confirms VPC deleted

# Step 2: Delete orphaned VPCAttachment
kubectl delete vpcattachment server-01-web-servers

# Step 3: Verify deletion
kubectl get vpcattachment server-01-web-servers
# Error from server (NotFound) - expected

# Step 4: Check for other orphaned attachments
kubectl get vpcattachments -o yaml | grep &quot;web-app-prod&quot;
# Should return nothing

# Step 5: Verify switch cleanup
kubectl get agent leaf-01 -n fab -o yaml | grep &quot;1010&quot;
# VLAN 1010 should be removed
</code></pre>
<p><strong>Prevention:</strong> Always delete VPCAttachments BEFORE VPC.</p>
<h3>Issue: VPC deletion stuck in &quot;Terminating&quot; state</h3>
<p><strong>Symptom:</strong> <code>kubectl get vpc</code> shows VPC in Terminating state for extended time</p>
<p><strong>Example:</strong></p>
<pre><code class=""language-bash"">kubectl get vpcs
# NAME           STATUS        AGE
# web-app-prod   Terminating   5m
</code></pre>
<p><strong>Cause:</strong> Finalizers preventing deletion, or controller issue</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Check VPC for finalizers
kubectl get vpc web-app-prod -o yaml | grep finalizers -A 5

# Step 2: Check events for errors
kubectl get events --field-selector involvedObject.name=web-app-prod --sort-by=&#39;.lastTimestamp&#39;

# Step 3: Verify no attachments exist
kubectl get vpcattachments | grep web-app-prod
# Should be empty - if not, delete attachments

# Step 4: Check fabric controller pod
kubectl get pods -n fab | grep controller
kubectl logs &lt;controller-pod-name&gt; -n fab | tail -50

# Step 5: If finalizers blocking, remove them (advanced - use caution)
kubectl patch vpc web-app-prod -p &#39;{&quot;metadata&quot;:{&quot;finalizers&quot;:[]}}&#39; --type=merge

# Step 6: Verify deletion completes
kubectl get vpc web-app-prod
# Should return NotFound
</code></pre>
<p><strong>Note:</strong> Removing finalizers manually should be a last resort. Usually waiting or restarting the controller resolves the issue.</p>
<h3>Issue: Accidentally deleted VPC - can I recover it?</h3>
<p><strong>Symptom:</strong> VPC deleted by mistake, need to restore</p>
<p><strong>Cause:</strong> Human error, wrong VPC name, accidental command</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Check if you have a backup YAML manifest
ls -la web-app-prod-vpc.yaml

# If you saved the YAML earlier:
kubectl apply -f web-app-prod-vpc.yaml

# If you don&#39;t have the YAML, you&#39;ll need to recreate from scratch

# Step 2: Recreate VPC manually
cat &gt; web-app-prod-vpc.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
  name: web-app-prod
  namespace: default
spec:
  ipv4Namespace: default
  vlanNamespace: default
  subnets:
    web-servers:
      subnet: 10.10.10.0/24
      gateway: 10.10.10.1
      vlan: 1010
    worker-nodes:
      subnet: 10.10.20.0/24
      gateway: 10.10.20.1
      vlan: 1020
      dhcp:
        enable: true
        range:
          start: 10.10.20.10
          end: 10.10.20.250
EOF

kubectl apply -f web-app-prod-vpc.yaml

# Step 3: Verify VPC created
kubectl get vpc web-app-prod

# Step 4: Recreate VPCAttachments
kubectl apply -f server-01-attachment.yaml
kubectl apply -f server-05-attachment.yaml

# Step 5: Validate connectivity (Module 2.3)
kubectl describe vpc web-app-prod
kubectl describe vpcattachment server-01-web-servers
</code></pre>
<p><strong>Important notes:</strong></p>
<ul>
<li>If VLANs were auto-assigned originally, new assignment may differ</li>
<li>All VPCAttachments must be recreated after VPC restored</li>
<li>Recovery time: 1-2 minutes for full reconciliation</li>
<li><strong>Prevention:</strong> Always save YAML manifests before deletion (or use GitOps)</li>
</ul>
<h3>Issue: Server still has connectivity after VPCAttachment deleted</h3>
<p><strong>Symptom:</strong> Server can still reach network after VPCAttachment deleted</p>
<p><strong>Example:</strong></p>
<pre><code class=""language-bash""># VPCAttachment deleted
kubectl get vpcattachment server-01-web-servers
# Error from server (NotFound)

# But server still has network connectivity
# SSH to server: ping 10.10.10.1 works
</code></pre>
<p><strong>Cause:</strong> Server OS network configuration not changed (static IP remains)</p>
<p><strong>Explanation:</strong></p>
<p>VPCAttachment deletion removes <strong>fabric-side configuration</strong> (VLANs on switch ports, VXLAN tunnels, routing). It does NOT change <strong>server-side configuration</strong> (IP addresses, routes in server OS).</p>
<p>The server still has the IP address and routes configured in its operating system, but:</p>
<ul>
<li>The fabric is no longer forwarding traffic for that VLAN</li>
<li>Server cannot reach other servers in the VPC</li>
<li>Server cannot reach resources outside the VPC</li>
</ul>
<p><strong>This is expected behavior.</strong></p>
<p><strong>Fix (if you want to remove server network config):</strong></p>
<pre><code class=""language-bash""># SSH to server
ssh server-01

# Remove IP address
sudo ip addr del 10.10.10.10/24 dev eth0

# Remove default route
sudo ip route del default via 10.10.10.1

# Verify connectivity gone
ping 10.10.10.1
# Should fail: Network is unreachable
</code></pre>
<p><strong>In production:</strong> Server OS configuration is typically managed separately (Ansible, configuration management tools). VPCAttachment manages fabric config only.</p>
<h2>Resources</h2>
<h3>Hedgehog CRDs</h3>
<p><strong>VPC</strong> - Virtual Private Cloud definition</p>
<ul>
<li>View all: <code>kubectl get vpcs</code></li>
<li>Delete: <code>kubectl delete vpc &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpc &lt;name&gt; -o yaml</code></li>
</ul>
<p><strong>VPCAttachment</strong> - Binds server connection to VPC subnet</p>
<ul>
<li>View all: <code>kubectl get vpcattachments</code></li>
<li>Delete: <code>kubectl delete vpcattachment &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpcattachment &lt;name&gt; -o yaml</code></li>
</ul>
<p><strong>Agent</strong> - Per-switch operational state (for cleanup validation)</p>
<ul>
<li>View all: <code>kubectl get agents -n fab</code></li>
<li>View specific: <code>kubectl get agent &lt;switch-name&gt; -n fab -o yaml</code></li>
<li>Validate cleanup: Check for removed VLANs after deletion</li>
</ul>
<p><strong>Connection</strong> - Server-to-switch wiring (not deleted in this module)</p>
<ul>
<li>View all: <code>kubectl get connections -n fab</code></li>
<li>Note: Connections persist after VPCAttachment deletion</li>
</ul>
<h3>kubectl Commands Reference</h3>
<p><strong>Decommissioning workflow:</strong></p>
<pre><code class=""language-bash""># Step 1: Pre-decommission review
kubectl get vpcs
kubectl get vpcattachments
kubectl get vpc &lt;name&gt; -o yaml &gt; backup.yaml  # Save backup

# Step 2: Delete VPCAttachments FIRST
kubectl delete vpcattachment &lt;name&gt;

# Step 3: Verify attachments deleted
kubectl get vpcattachments | grep &lt;vpc-name&gt;  # Should be empty

# Step 4: Delete VPC
kubectl delete vpc &lt;name&gt;

# Step 5: Verify VPC deleted
kubectl get vpc &lt;name&gt;  # Should return NotFound

# Step 6: Validate cleanup
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20
kubectl get agent &lt;switch-name&gt; -n fab -o yaml | grep &lt;vlan&gt;
</code></pre>
<p><strong>Event monitoring during cleanup:</strong></p>
<pre><code class=""language-bash""># View recent events
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20

# Watch events in real-time
kubectl get events --watch

# View events for specific resource
kubectl get events --field-selector involvedObject.name=&lt;resource-name&gt;

# View events in fab namespace (agents)
kubectl get events -n fab --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p><strong>Validation commands:</strong></p>
<pre><code class=""language-bash""># Verify resource deleted (should return NotFound)
kubectl get vpc &lt;name&gt;
kubectl get vpcattachment &lt;name&gt;

# List all resources to confirm deletion
kubectl get vpcs
kubectl get vpcattachments

# Check Agent CRD for cleanup
kubectl get agent &lt;switch-name&gt; -n fab -o yaml | grep -A 5 &quot;&lt;vlan-id&gt;&quot;
</code></pre>
<p><strong>Recovery commands:</strong></p>
<pre><code class=""language-bash""># Restore from backup YAML
kubectl apply -f backup.yaml

# Recreate VPCAttachments
kubectl apply -f attachment.yaml
</code></pre>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""./module-2.3-connectivity-validation.md"">Module 2.3: Connectivity Validation</a></li>
<li>Module 2.1: <a href=""./module-2.1-vpc-provisioning.md"">VPC Provisioning Essentials</a></li>
<li>Module 2.2: <a href=""./module-2.2-vpc-attachments.md"">VPC Attachments</a></li>
<li><strong>Course 2 Complete!</strong> Preview Course 3: Observability &amp; Fabric Health</li>
</ul>
<h3>External Documentation</h3>
<ul>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog VPC Documentation</a></li>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog Lifecycle Management</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/architecture/garbage-collection/"">Kubernetes Resource Deletion</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubectl/generated/kubectl_delete/"">kubectl delete Command Reference</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve successfully learned safe decommissioning workflows and completed Course 2: Provisioning &amp; Day 1 Operations. You now understand the full lifecycle: Provision → Attach → Validate → Cleanup. Ready to move on to Course 3: Observability &amp; Fabric Health!</p>
<p><strong>Course 2 Achievement Unlocked:</strong> Day 1 Operations Master</p>
",204,15,,,"hedgehog,fabric,decommission,cleanup,lifecycle,operations"
198562386861,Events & Status Monitoring,fabric-operations-events-status,[object Object],"Monitor fabric health using kubectl events and Agent CRD status, correlating with Grafana metrics for complete troubleshooting visibility.","<h2>Introduction</h2>
<p>In Module 3.2, you learned to interpret Grafana dashboards—answering questions like &quot;Are BGP sessions up?&quot; and &quot;Are there interface errors?&quot;</p>
<p>Dashboards show <strong>what</strong> is happening (metrics over time).</p>
<p>But when something goes wrong, you need to know <strong>why</strong> it&#39;s happening.</p>
<p>That&#39;s where <strong>kubectl events and Agent CRD status</strong> come in—they provide:</p>
<ul>
<li><strong>Configuration errors:</strong> &quot;VLAN 1010 already in use&quot;</li>
<li><strong>Reconciliation status:</strong> &quot;VPC successfully created&quot;</li>
<li><strong>Dependency issues:</strong> &quot;Connection server-01--mclag not found&quot;</li>
<li><strong>Switch state details:</strong> BGP neighbor state, interface status, platform health</li>
</ul>
<h3>The Troubleshooting Question</h3>
<p>A user reports: &quot;My server can&#39;t reach the VPC.&quot;</p>
<p><strong>Grafana tells you:</strong></p>
<ul>
<li>Interface is up</li>
<li>No packet errors</li>
<li>VLAN is configured</li>
</ul>
<p><strong>But that&#39;s not enough. You need to know:</strong></p>
<ul>
<li>Did VPCAttachment reconcile successfully? <em>(kubectl events)</em></li>
<li>Is the switch Agent ready? <em>(Agent CRD status)</em></li>
<li>Was there a configuration error? <em>(kubectl events)</em></li>
<li>What&#39;s the exact VLAN and interface? <em>(Agent CRD state)</em></li>
</ul>
<h3>What You&#39;ll Learn</h3>
<ul>
<li>How to monitor Kubernetes events for fabric resources</li>
<li>How to interpret Agent CRD status fields</li>
<li>How to track VPC lifecycle through events</li>
<li>Common error event patterns and their meanings</li>
<li>How to correlate kubectl data with Grafana metrics</li>
</ul>
<h3>The Integration</h3>
<pre><code>Grafana:    &quot;Interface Ethernet5 has no traffic&quot;
                        ↓
kubectl events: &quot;VPCAttachment failed: VLAN conflict&quot;
                        ↓
Agent CRD:  &quot;VLAN 1010 already used by VPC other-vpc&quot;
                        ↓
Solution:   Change VLAN in VPC spec
</code></pre>
<p>This module teaches you to use kubectl and Grafana <strong>together</strong> for complete observability.</p>
<p><strong>The Complete Observability Picture:</strong></p>
<p>Grafana dashboards are powerful—they show you metrics, trends, and anomalies. But they can&#39;t show you why a VPC failed to create or why a VPCAttachment didn&#39;t configure a VLAN. That requires Kubernetes events and the Agent CRD.</p>
<p>Think of it this way:</p>
<ul>
<li><strong>Grafana</strong> = Your speedometer and dashboard lights (symptoms)</li>
<li><strong>kubectl events</strong> = Your engine diagnostic codes (what went wrong)</li>
<li><strong>Agent CRD</strong> = Your engine control unit data (exact state)</li>
</ul>
<p>In production operations, you&#39;ll use all three together. This integrated troubleshooting approach is what separates novice operators from experienced ones. You&#39;ll start with a symptom in Grafana, use kubectl events to find configuration errors, check Agent CRD for exact switch state, and correlate all three to identify root cause.</p>
<p><strong>Why This Matters:</strong></p>
<p>Modern fabric operations require correlation across multiple data sources. A single data source rarely tells the complete story. Learning to seamlessly move between Grafana, kubectl events, and Agent CRD queries will make you significantly more effective at troubleshooting and will prepare you for the diagnostic collection workflow in Module 3.4.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Monitor kubectl events</strong> - Track VPC lifecycle events and reconciliation status</li>
<li><strong>Interpret Agent CRD status</strong> - Read switch operational state from Agent CRD status fields</li>
<li><strong>Track VPC lifecycle</strong> - Follow VPC creation, attachment, and deletion through events</li>
<li><strong>Identify error patterns</strong> - Recognize common failure events (VLAN conflicts, subnet overlaps, missing connections)</li>
<li><strong>Correlate kubectl and Grafana</strong> - Cross-reference events with metrics for complete troubleshooting picture</li>
<li><strong>Apply integrated troubleshooting</strong> - Use both kubectl events and Grafana dashboards together</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 3.1 completion (Fabric Telemetry Overview)</li>
<li>Module 3.2 completion (Dashboard Interpretation)</li>
<li>Module 1.3 completion (kubectl basics from Course 1)</li>
<li>Understanding of Kubernetes events</li>
<li>Familiarity with VPC and VPCAttachment resources</li>
</ul>
<h2>Scenario: Integrated Troubleshooting with kubectl and Grafana</h2>
<p>You provisioned a new VPC called <code>testapp-vpc</code> with a VPCAttachment for <code>server-03</code>. The server reports it&#39;s not getting DHCP. You&#39;ll use both Grafana and kubectl to diagnose the issue.</p>
<p><strong>Environment Access:</strong></p>
<ul>
<li><strong>Grafana:</strong> <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li><strong>kubectl:</strong> Already configured</li>
</ul>
<h3>Task 1: Identify Symptom in Grafana (1 minute)</h3>
<p><strong>Objective:</strong> Use Grafana to observe the issue</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Open Grafana Interfaces Dashboard:</strong></p>
<ul>
<li>Navigate to <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li>Dashboards → &quot;Hedgehog Interfaces&quot;</li>
</ul>
</li>
<li><p><strong>Find server-03 interface:</strong></p>
<ul>
<li>Look for the leaf switch interface connected to server-03</li>
<li>In vlab, you can check Connection CRDs to identify the interface:<pre><code class=""language-bash"">kubectl get connections -n fab | grep server-03
</code></pre>
</li>
</ul>
</li>
<li><p><strong>Observe symptoms:</strong></p>
<ul>
<li><strong>Expected:</strong> Interface UP, VLAN configured, traffic flowing</li>
<li><strong>Actual:</strong> What do you see?<ul>
<li>Is interface up or down?</li>
<li>Is VLAN configured?</li>
<li>Is there traffic?</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Expected Finding:</strong> Interface UP, but no traffic or VLAN not visible</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Located server-03&#39;s interface in Grafana</li>
<li>✅ Identified symptom (e.g., interface UP but no traffic)</li>
<li>✅ Noted which switch and interface</li>
</ul>
<h3>Task 2: Check VPC and VPCAttachment Events (2 minutes)</h3>
<p><strong>Objective:</strong> Use kubectl to check resource reconciliation status</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>List VPCs:</strong></p>
<pre><code class=""language-bash"">kubectl get vpc testapp-vpc
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAME           AGE
testapp-vpc    5m
</code></pre>
</li>
<li><p><strong>Check VPC events:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=testapp-vpc --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p><strong>Expected output (healthy):</strong></p>
<pre><code>LAST SEEN   TYPE     REASON              OBJECT            MESSAGE
5m          Normal   Created             vpc/testapp-vpc   VPC created successfully
5m          Normal   VNIAllocated        vpc/testapp-vpc   Allocated VNI 100010
5m          Normal   SubnetsConfigured   vpc/testapp-vpc   3 subnets configured
5m          Normal   Ready               vpc/testapp-vpc   VPC is ready
</code></pre>
<p><strong>Or use describe for combined view:</strong></p>
<pre><code class=""language-bash"">kubectl describe vpc testapp-vpc
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li>✅ Normal events: Created, VNIAllocated, SubnetsConfigured, Ready</li>
<li>❌ Warning events: ValidationFailed, VLANConflict, SubnetOverlap</li>
</ul>
</li>
<li><p><strong>List VPCAttachments:</strong></p>
<pre><code class=""language-bash"">kubectl get vpcattachment -A | grep server-03
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAMESPACE   NAME                    AGE
default     testapp-vpc-server-03   4m
</code></pre>
</li>
<li><p><strong>Check VPCAttachment events:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.kind=VPCAttachment --sort-by=&#39;.lastTimestamp&#39; | grep server-03
</code></pre>
<p><strong>Or describe the attachment:</strong></p>
<pre><code class=""language-bash"">kubectl describe vpcattachment testapp-vpc-server-03
</code></pre>
<p><strong>Expected output (healthy):</strong></p>
<pre><code>LAST SEEN   TYPE     REASON              OBJECT                          MESSAGE
4m          Normal   Created             vpcattachment/...server-03      VPCAttachment created
4m          Normal   ConnectionFound     vpcattachment/...server-03      Connection server-03--mclag found
4m          Normal   VLANConfigured      vpcattachment/...server-03      VLAN 1010 configured on leaf-05/Ethernet7
4m          Normal   Ready               vpcattachment/...server-03      VPCAttachment ready
</code></pre>
<p><strong>Example problem (VLAN conflict):</strong></p>
<pre><code>LAST SEEN   TYPE      REASON          OBJECT                          MESSAGE
4m          Normal    Created         vpcattachment/...server-03      VPCAttachment created
3m          Warning   VLANConflict    vpcattachment/...server-03      VLAN 1010 already in use by vpc-1/default
</code></pre>
<p><strong>Example problem (connection not found):</strong></p>
<pre><code>LAST SEEN   TYPE      REASON              OBJECT                          MESSAGE
4m          Normal    Created             vpcattachment/...server-03      VPCAttachment created
3m          Warning   DependencyMissing   vpcattachment/...server-03      Connection server-03--mclag not found
</code></pre>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Identified if VPC or VPCAttachment has errors</li>
<li>✅ Found specific error event message</li>
<li>✅ Determined next troubleshooting step</li>
</ul>
<p><strong>Common Issues to Find:</strong></p>
<ul>
<li>VLAN conflict (VLAN already in use)</li>
<li>Connection not found (wrong connection name)</li>
<li>Subnet overlap (IP address conflict)</li>
</ul>
<h3>Task 3: Check Agent CRD for Switch State (2 minutes)</h3>
<p><strong>Objective:</strong> Verify switch-level configuration using Agent CRD</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Identify which switch has server-03:</strong></p>
<pre><code class=""language-bash""># List connections to find server-03
kubectl get connections -n fab | grep server-03
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code>server-03--unbundled--leaf-05
</code></pre>
<p><strong>Result:</strong> server-03 connected to leaf-05, Ethernet7 (example)</p>
</li>
<li><p><strong>Check Agent status for leaf-05:</strong></p>
<pre><code class=""language-bash"">kubectl get agent leaf-05 -n fab
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAME       ROLE          DESCR           APPLIED   VERSION
leaf-05    server-leaf   VS-05           2m        v0.87.4
</code></pre>
<p><strong>Success:</strong> Agent is present and has recent APPLIED time</p>
</li>
<li><p><strong>Check interface state in Agent CRD:</strong></p>
<pre><code class=""language-bash""># View interface Ethernet7 state
kubectl get agent leaf-05 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet7}&#39; | jq
</code></pre>
<p><strong>Expected output (healthy):</strong></p>
<pre><code class=""language-json"">{
  &quot;enabled&quot;: true,
  &quot;admin&quot;: &quot;up&quot;,
  &quot;oper&quot;: &quot;up&quot;,
  &quot;mac&quot;: &quot;00:11:22:33:44:55&quot;,
  &quot;speed&quot;: &quot;25G&quot;,
  &quot;counters&quot;: {
    &quot;inb&quot;: 123456789,
    &quot;outb&quot;: 987654321,
    &quot;ine&quot;: 0,
    &quot;oute&quot;: 0
  }
}
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li>Is <code>oper</code> = &quot;up&quot;? (Interface operational)</li>
<li>Are there VLANs listed? (VLAN configuration)</li>
<li>Are there any error counters? (Hardware issues)</li>
</ul>
</li>
<li><p><strong>Check all interfaces (optional):</strong></p>
<pre><code class=""language-bash""># View all interfaces
kubectl get agent leaf-05 -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq
</code></pre>
</li>
<li><p><strong>Check configuration application status:</strong></p>
<pre><code class=""language-bash""># When was config last applied?
kubectl get agent leaf-05 -n fab -o jsonpath=&#39;{.status.lastAppliedTime}&#39;
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>2025-10-17T10:29:55Z
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li>Recent timestamp (within last few minutes) = good (config applying)</li>
<li>Old timestamp (hours/days old) = problem (config not applying)</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Verified interface operational state</li>
<li>✅ Checked if VLAN configured at switch level</li>
<li>✅ Confirmed agent applied config recently</li>
</ul>
<h3>Task 4: Correlate kubectl and Grafana Findings (1 minute)</h3>
<p><strong>Objective:</strong> Combine kubectl and Grafana data to identify root cause</p>
<p><strong>Analysis Template:</strong></p>
<p><strong>Grafana Symptom:</strong></p>
<ul>
<li>Interface: leaf-05/Ethernet7</li>
<li>State: UP</li>
<li>VLAN: Not visible or wrong VLAN</li>
<li>Traffic: 0 bps</li>
</ul>
<p><strong>kubectl Events:</strong></p>
<ul>
<li>VPC Event: [Normal/Warning] [Reason] [Message]</li>
<li>VPCAttachment Event: [Normal/Warning] [Reason] [Message]</li>
</ul>
<p><strong>Agent CRD Status:</strong></p>
<ul>
<li>Interface oper: [up/down]</li>
<li>Config applied: [timestamp]</li>
<li>VLANs: [list]</li>
</ul>
<p><strong>Root Cause:</strong></p>
<ul>
<li>Example: VPCAttachment failed due to VLAN conflict → VLAN never configured → No DHCP</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>Example: Change VPC VLAN from 1010 to 1011, commit to Gitea</li>
</ul>
<p><strong>Example Correlation Scenario 1:</strong></p>
<p><strong>Grafana:</strong> Interface Ethernet7 UP, 0 bps traffic
<strong>kubectl events:</strong> Warning: VLANConflict - VLAN 1010 already in use by vpc-1/default
<strong>Agent CRD:</strong> Interface operational, no VLANs configured
<strong>Root Cause:</strong> VPCAttachment failed due to VLAN conflict, so VLAN never applied to interface
<strong>Solution:</strong> Change testapp-vpc VLAN to unused VLAN (check: <code>kubectl get vpc -o yaml | grep vlan:</code>)</p>
<p><strong>Example Correlation Scenario 2:</strong></p>
<p><strong>Grafana:</strong> Interface Ethernet7 UP, 0 bps traffic
<strong>kubectl events:</strong> Warning: DependencyMissing - Connection server-03--mclag not found
<strong>Agent CRD:</strong> Interface operational, no VLANs configured
<strong>Root Cause:</strong> VPCAttachment references wrong connection name (server-03--mclag doesn&#39;t exist)
<strong>Solution:</strong> Fix connection name in VPCAttachment (check: <code>kubectl get connections -n fab | grep server-03</code>)</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Identified root cause from event messages</li>
<li>✅ Correlated kubectl findings with Grafana symptom</li>
<li>✅ Proposed solution based on error type</li>
</ul>
<h3>Task 5: Monitor Resolution (Optional, 1 minute)</h3>
<p><strong>Objective:</strong> Verify fix by monitoring events and Grafana</p>
<p><strong>Steps:</strong></p>
<ol>
<li><p><strong>Watch VPC/VPCAttachment events:</strong></p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.kind=VPCAttachment --watch
</code></pre>
</li>
<li><p><strong>Apply fix:</strong></p>
<ul>
<li>If VLAN conflict: Change VLAN in VPC YAML in Gitea, commit</li>
<li>If connection not found: Fix connection name in VPCAttachment YAML in Gitea, commit</li>
<li>ArgoCD will sync changes automatically</li>
</ul>
</li>
<li><p><strong>Observe events:</strong></p>
<ul>
<li>Wait for ArgoCD sync (30-60 seconds)</li>
<li>Watch for Normal events appearing:<pre><code>TYPE     REASON              MESSAGE
Normal   ConnectionFound     Connection server-03--unbundled--leaf-05 found
Normal   VLANConfigured      VLAN 1011 configured on leaf-05/Ethernet7
Normal   Ready               VPCAttachment ready
</code></pre>
</li>
</ul>
</li>
<li><p><strong>Check Grafana Interfaces Dashboard:</strong></p>
<ul>
<li>VLAN now visible on interface</li>
<li>Traffic appearing (server gets DHCP and starts communicating)</li>
</ul>
</li>
<li><p><strong>Verify on server (optional):</strong></p>
<pre><code class=""language-bash"">hhfab vlab ssh server-03
ip addr show  # Should show DHCP IP address
</code></pre>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Events show successful reconciliation</li>
<li>✅ Grafana shows VLAN configured and traffic</li>
<li>✅ Server receives DHCP address</li>
</ul>
<h3>Lab Summary</h3>
<p><strong>What You Accomplished:</strong></p>
<p>You performed integrated troubleshooting using kubectl and Grafana:</p>
<ul>
<li>✅ Identified symptom in Grafana (interface no traffic)</li>
<li>✅ Checked kubectl events for configuration errors</li>
<li>✅ Reviewed Agent CRD status for switch state</li>
<li>✅ Correlated kubectl and Grafana data to find root cause</li>
<li>✅ Proposed solution based on event messages</li>
</ul>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Grafana shows &quot;what&quot;</strong> (metrics, symptoms)</li>
<li><strong>kubectl events show &quot;why&quot;</strong> (configuration errors, reconciliation failures)</li>
<li><strong>Agent CRD shows &quot;exactly&quot;</strong> (precise switch state)</li>
<li><strong>Integrated approach</strong> (Grafana + kubectl) enables root cause analysis</li>
<li><strong>Events expire after 1 hour</strong> - check soon after operations</li>
<li><strong>Event patterns are recognizable</strong> (VLAN conflict, connection not found, etc.)</li>
</ol>
<p><strong>Troubleshooting Workflow:</strong></p>
<pre><code>Symptom (Grafana)
      ↓
Events (kubectl get events)
      ↓
Agent CRD (kubectl get agent ... -o jsonpath)
      ↓
Controller Logs (if needed)
      ↓
Root Cause → Solution
</code></pre>
<h2>Concepts &amp; Deep Dive</h2>
<h3>Concept 1: Kubernetes Event Monitoring</h3>
<p><strong>What Are Kubernetes Events?</strong></p>
<p>Events are timestamped records of actions taken by Kubernetes controllers. In Hedgehog, events track:</p>
<ul>
<li>Resource creation/updates</li>
<li>Reconciliation success/failure</li>
<li>Configuration validation errors</li>
<li>Dependency resolution</li>
</ul>
<p><strong>Event Types:</strong></p>
<p><strong>1. Normal Events (Informational)</strong></p>
<ul>
<li>Successful operations</li>
<li>Reconciliation progress</li>
<li>State transitions</li>
</ul>
<p><strong>2. Warning Events (Errors/Issues)</strong></p>
<ul>
<li>Validation failures</li>
<li>Dependency problems</li>
<li>Reconciliation errors</li>
</ul>
<p><strong>Viewing All Events:</strong></p>
<pre><code class=""language-bash""># All events, recent first
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39;

# Events in specific namespace
kubectl get events -n default --sort-by=&#39;.lastTimestamp&#39;

# Events in fab namespace (switches, agents)
kubectl get events -n fab --sort-by=&#39;.lastTimestamp&#39;

# Watch events live
kubectl get events --all-namespaces --watch
</code></pre>
<p><strong>Filtering Events:</strong></p>
<pre><code class=""language-bash""># Only Warning events
kubectl get events --all-namespaces --field-selector type=Warning

# Events for specific resource
kubectl get events --field-selector involvedObject.name=myvpc

# Events for specific resource type
kubectl get events --field-selector involvedObject.kind=VPC

# Combine filters
kubectl get events --field-selector involvedObject.kind=VPC,type=Warning
</code></pre>
<p><strong>Event Fields:</strong></p>
<pre><code>LAST SEEN   TYPE      REASON              OBJECT           MESSAGE
2m          Normal    Created             vpc/myvpc        VPC created successfully
1m          Normal    VNIAllocated        vpc/myvpc        Allocated VNI 100010
30s         Warning   VLANConflict        vpc/test-vpc     VLAN 1010 already in use by vpc-1
</code></pre>
<ul>
<li><strong>LAST SEEN:</strong> How long ago event occurred</li>
<li><strong>TYPE:</strong> Normal or Warning</li>
<li><strong>REASON:</strong> Short event reason code (VLANConflict, DependencyMissing, etc.)</li>
<li><strong>OBJECT:</strong> Resource that triggered event</li>
<li><strong>MESSAGE:</strong> Detailed human-readable message</li>
</ul>
<p><strong>Event Retention:</strong></p>
<p>Kubernetes retains events for <strong>1 hour by default</strong>. After 1 hour, events are deleted.</p>
<p><strong>Best Practice:</strong> Check events soon after operations. For long-term audit, export events or use logging system.</p>
<p><strong>Why Events Matter:</strong></p>
<p>Events provide immediate feedback on what the controller is doing with your resources. When you create a VPC, events tell you:</p>
<ul>
<li>Was the VPC created successfully?</li>
<li>Was a VNI allocated?</li>
<li>Did any validation fail?</li>
<li>Are there any conflicts?</li>
</ul>
<p>Without events, you&#39;d have to guess why a resource isn&#39;t working. With events, the controller tells you exactly what happened.</p>
<p><strong>Event Workflow Example:</strong></p>
<pre><code class=""language-bash""># Create a VPC
kubectl apply -f vpc.yaml

# Immediately check events
kubectl get events --field-selector involvedObject.name=myvpc --watch

# See real-time feedback:
# 0s    Normal   Created        VPC created successfully
# 2s    Normal   VNIAllocated   Allocated VNI 100010
# 5s    Normal   Ready          VPC is ready
</code></pre>
<h3>Concept 2: Agent CRD Status - Switch State</h3>
<p><strong>What is Agent CRD?</strong></p>
<p>The <strong>Agent CRD</strong> is Hedgehog&#39;s internal representation of each switch. It contains <strong>comprehensive switch operational state</strong>—everything happening on the switch right now.</p>
<p><strong>Why Agent CRD Matters:</strong></p>
<p>While VPC, VPCAttachment, and Connection CRDs have minimal status (<code>status: {}</code>), the <strong>Agent CRD has detailed status fields</strong>:</p>
<ul>
<li>Switch version and uptime</li>
<li>Interface operational state (up/down, counters)</li>
<li>BGP neighbor state</li>
<li>Platform health (PSU, fans, temperature)</li>
<li>ASIC resource usage</li>
<li>Configuration application status</li>
</ul>
<p><strong>Accessing Agent CRD:</strong></p>
<pre><code class=""language-bash""># List all switch agents
kubectl get agents -n fab
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>NAME       ROLE          DESCR           APPLIED   VERSION
leaf-01    server-leaf   VS-01 MCLAG 1   2m        v0.87.4
leaf-02    server-leaf   VS-02 MCLAG 1   3m        v0.87.4
spine-01   spine         VS-06           4m        v0.87.4
</code></pre>
<p><strong>Get Full Agent Status:</strong></p>
<pre><code class=""language-bash""># Full YAML output
kubectl get agent leaf-01 -n fab -o yaml

# Human-readable description
kubectl describe agent leaf-01 -n fab
</code></pre>
<p><strong>Key Status Fields:</strong></p>
<p><strong>1. Agent Heartbeat and Version:</strong></p>
<pre><code class=""language-bash""># Last heartbeat timestamp
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.lastHeartbeat}&#39;
# Output: 2025-10-17T10:30:00Z

# Agent version
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.version}&#39;
# Output: v0.87.4
</code></pre>
<p><strong>Interpretation:</strong> Recent heartbeat (within last 2-3 minutes) means agent is connected and healthy.</p>
<p><strong>2. Configuration Application Status:</strong></p>
<pre><code class=""language-bash""># When was config last applied?
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.lastAppliedTime}&#39;
# Output: 2025-10-17T10:29:55Z

# What generation was applied?
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.lastAppliedGen}&#39;
# Output: 15
</code></pre>
<p><strong>Interpretation:</strong> Recent <code>lastAppliedTime</code> means controller is actively configuring switch.</p>
<p><strong>3. Interface State:</strong></p>
<pre><code class=""language-bash""># All interfaces
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq

# Specific interface
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code class=""language-json"">{
  &quot;enabled&quot;: true,
  &quot;admin&quot;: &quot;up&quot;,
  &quot;oper&quot;: &quot;up&quot;,
  &quot;mac&quot;: &quot;00:11:22:33:44:55&quot;,
  &quot;speed&quot;: &quot;25G&quot;,
  &quot;counters&quot;: {
    &quot;inb&quot;: 123456789,
    &quot;outb&quot;: 987654321,
    &quot;ine&quot;: 0,
    &quot;oute&quot;: 0
  }
}
</code></pre>
<p><strong>Field Meanings:</strong></p>
<ul>
<li><code>enabled</code>: Port enabled in configuration</li>
<li><code>admin</code>: Administrative state (up/down)</li>
<li><code>oper</code>: Operational state (actual link state)</li>
<li><code>speed</code>: Link speed</li>
<li><code>counters.inb</code>: Input bytes</li>
<li><code>counters.outb</code>: Output bytes</li>
<li><code>counters.ine</code>: Input errors</li>
<li><code>counters.oute</code>: Output errors</li>
</ul>
<p><strong>4. BGP Neighbor State:</strong></p>
<pre><code class=""language-bash""># All BGP neighbors
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq

# Check specific neighbor state
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors.default[&quot;172.30.128.10&quot;].state}&#39;
# Output: established
</code></pre>
<p><strong>Expected BGP states:</strong></p>
<ul>
<li><code>established</code> = Session up, routes exchanged</li>
<li><code>idle</code> = Session down</li>
<li><code>active</code>, <code>connect</code> = Attempting connection</li>
<li><code>opensent</code>, <code>openconfirm</code> = Establishing session</li>
</ul>
<p><strong>5. Platform Health:</strong></p>
<pre><code class=""language-bash""># PSU status
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.platform.psus}&#39; | jq

# Fan status
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.platform.fans}&#39; | jq

# Temperature
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.platform.temps}&#39; | jq
</code></pre>
<p><strong>Example PSU output:</strong></p>
<pre><code class=""language-json"">{
  &quot;PSU1&quot;: {
    &quot;presence&quot;: true,
    &quot;status&quot;: true,
    &quot;inVoltage&quot;: 120.0,
    &quot;outVoltage&quot;: 12.0
  }
}
</code></pre>
<p><strong>6. ASIC Critical Resources:</strong></p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.criticalResources}&#39; | jq
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code class=""language-json"">{
  &quot;stats&quot;: {
    &quot;ipv4RoutesUsed&quot;: 150,
    &quot;ipv4RoutesAvailable&quot;: 32000,
    &quot;ipv4NexthopsUsed&quot;: 200,
    &quot;ipv4NexthopsAvailable&quot;: 16000
  }
}
</code></pre>
<p><strong>Interpretation:</strong> Watch for resources approaching capacity (&gt; 80% used).</p>
<p><strong>Agent Conditions:</strong></p>
<pre><code class=""language-bash""># Check if agent is Ready
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.conditions[?(@.type==&quot;Ready&quot;)].status}&#39;
# Output: True
</code></pre>
<p><strong>Why This Matters:</strong></p>
<p>Agent CRD status provides <strong>source of truth for switch state</strong> at Kubernetes level. When Grafana shows an issue, Agent CRD tells you the exact switch state—interface operational status, BGP neighbor state, VLAN configuration, and more.</p>
<p><strong>Practical Example:</strong></p>
<p><strong>Problem:</strong> Server can&#39;t reach VPC
<strong>Grafana:</strong> Interface up, no traffic
<strong>Agent CRD Check:</strong></p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq
</code></pre>
<p><strong>Finding:</strong> Interface operational but no VLANs configured
<strong>Next Step:</strong> Check VPCAttachment events for why VLAN not configured</p>
<h3>Concept 3: VPC Lifecycle Event Tracking</h3>
<p><strong>VPC Creation Event Sequence</strong></p>
<p>When you create a VPC via GitOps (Gitea commit → ArgoCD sync), events track the lifecycle:</p>
<p><strong>Successful VPC Creation:</strong></p>
<pre><code>Time    Type    Reason              Object          Message
0s      Normal  Created             vpc/myvpc       VPC created successfully
2s      Normal  VNIAllocated        vpc/myvpc       Allocated VNI 100010
5s      Normal  SubnetsConfigured   vpc/myvpc       3 subnets configured
10s     Normal  Ready               vpc/myvpc       VPC is ready
</code></pre>
<p><strong>How to watch this in real-time:</strong></p>
<pre><code class=""language-bash""># Create VPC
kubectl apply -f vpc.yaml

# Watch events
kubectl get events --field-selector involvedObject.name=myvpc --watch
</code></pre>
<p><strong>Failed VPC Creation (VLAN Conflict):</strong></p>
<pre><code>Time    Type     Reason              Object          Message
0s      Normal   Created             vpc/badVPC      VPC created successfully
2s      Warning  ValidationFailed    vpc/badVPC      VLAN 1010 already in use by vpc-1/default
</code></pre>
<p><strong>What happens:</strong> Controller creates VPC object but validation fails during reconciliation. VPC exists in Kubernetes but isn&#39;t operational.</p>
<p><strong>VPCAttachment Event Sequence</strong></p>
<p><strong>Successful Attachment:</strong></p>
<pre><code>Time    Type    Reason              Object                      Message
0s      Normal  Created             vpcattachment/vpc1-srv01    VPCAttachment created
3s      Normal  ConnectionFound     vpcattachment/vpc1-srv01    Connection server-01--mclag found
5s      Normal  VLANConfigured      vpcattachment/vpc1-srv01    VLAN 1010 configured on leaf-01/Ethernet5
8s      Normal  Ready               vpcattachment/vpc1-srv01    VPCAttachment ready
</code></pre>
<p><strong>What this tells you:</strong></p>
<ul>
<li>Controller found the Connection resource</li>
<li>VLAN was successfully configured on the switch interface</li>
<li>VPCAttachment is fully operational</li>
</ul>
<p><strong>Failed Attachment (Connection Not Found):</strong></p>
<pre><code>Time    Type     Reason                  Object                      Message
0s      Normal   Created                 vpcattachment/vpc1-srv05    VPCAttachment created
2s      Warning  DependencyMissing       vpcattachment/vpc1-srv05    Connection server-05--mclag not found
</code></pre>
<p><strong>What this tells you:</strong> VPCAttachment references a Connection that doesn&#39;t exist. Check Connection name and fix VPCAttachment spec.</p>
<p><strong>VPC Deletion Event Sequence</strong></p>
<p><strong>Successful Deletion:</strong></p>
<pre><code>Time    Type    Reason              Object          Message
0s      Normal  Deleting            vpc/myvpc       VPC deletion initiated
2s      Normal  DetachingServers    vpc/myvpc       Removing VPCAttachments
5s      Normal  ReleasingVNI        vpc/myvpc       VNI 100010 released
8s      Normal  Deleted             vpc/myvpc       VPC deleted successfully
</code></pre>
<p><strong>Failed Deletion (Active Attachments):</strong></p>
<pre><code>Time    Type     Reason                  Object          Message
0s      Normal   Deleting                vpc/myvpc       VPC deletion initiated
2s      Warning  DependencyExists        vpc/myvpc       Cannot delete: 2 VPCAttachments still active
</code></pre>
<p><strong>What this tells you:</strong> You must delete all VPCAttachments before deleting the VPC.</p>
<p><strong>Tracking VPC Lifecycle:</strong></p>
<pre><code class=""language-bash""># Watch VPC events live
kubectl get events --field-selector involvedObject.name=myvpc --watch

# Check VPC and related resources
kubectl get events --field-selector involvedObject.kind=VPC
kubectl get events --field-selector involvedObject.kind=VPCAttachment
kubectl get events --field-selector involvedObject.kind=DHCPSubnet
</code></pre>
<p><strong>Best Practice:</strong> Always watch events when creating/deleting VPCs to see immediate feedback on success or failure.</p>
<h3>Concept 4: Common Error Event Patterns</h3>
<p>Understanding common error patterns helps you quickly diagnose issues without guessing.</p>
<p><strong>Error Pattern 1: VLAN Conflict</strong></p>
<p><strong>Event:</strong></p>
<pre><code>Type: Warning
Reason: VLANConflict
Message: VLAN 1010 already in use by vpc-1/default
</code></pre>
<p><strong>Cause:</strong> VPC subnet specifies VLAN already used by another VPC in same VLANNamespace</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Check existing VLANs: <code>kubectl get vpc -o yaml | grep vlan:</code></li>
<li>Choose unused VLAN (e.g., 1011)</li>
<li>Update VPC YAML in Gitea</li>
<li>Commit change</li>
</ol>
<p><strong>Grafana Correlation:</strong> Interface shows no VLAN configured (VPC reconciliation failed)</p>
<p><strong>Example workflow:</strong></p>
<pre><code class=""language-bash""># Find all used VLANs
kubectl get vpc -o yaml | grep &quot;vlan:&quot; | sort -u

# Output:
# vlan: 1001
# vlan: 1010
# vlan: 1020

# Choose unused VLAN: 1011
# Edit VPC in Gitea, change vlan: 1010 to vlan: 1011
# Commit → ArgoCD syncs → VPC reconciles successfully
</code></pre>
<hr>
<p><strong>Error Pattern 2: Subnet Overlap</strong></p>
<p><strong>Event:</strong></p>
<pre><code>Type: Warning
Reason: SubnetOverlap
Message: Subnet 10.0.10.0/24 overlaps with existing VPC vpc-2/backend
</code></pre>
<p><strong>Cause:</strong> VPC subnet overlaps with another VPC in same IPv4Namespace</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Check existing subnets: <code>kubectl get vpc -o yaml | grep subnet:</code></li>
<li>Choose non-overlapping subnet (e.g., 10.0.20.0/24)</li>
<li>Update VPC YAML in Gitea</li>
</ol>
<p><strong>Grafana Correlation:</strong> No DHCP leases visible (VPC reconciliation failed)</p>
<p><strong>IPv4Namespace Example:</strong></p>
<pre><code class=""language-yaml""># IPv4Namespace defines allowed ranges
spec:
  subnets:
    - 10.0.0.0/16  # VPCs must use non-overlapping subnets within this range
</code></pre>
<p><strong>Finding available subnets:</strong></p>
<pre><code class=""language-bash""># List all VPC subnets
kubectl get vpc -o yaml | grep &quot;subnet:&quot; | sort

# Choose a /24 that&#39;s not in use
</code></pre>
<hr>
<p><strong>Error Pattern 3: Connection Not Found</strong></p>
<p><strong>Event:</strong></p>
<pre><code>Type: Warning
Reason: DependencyMissing
Message: Connection server-05--mclag--leaf-01--leaf-02 not found
</code></pre>
<p><strong>Cause:</strong> VPCAttachment references a Connection that doesn&#39;t exist</p>
<p><strong>Solution:</strong></p>
<ol>
<li>List available connections: <code>kubectl get connections -n fab</code></li>
<li>Find correct connection name for server-05</li>
<li>Update VPCAttachment YAML with correct connection name</li>
</ol>
<p><strong>Grafana Correlation:</strong> Interface shows as down or no VLAN (attachment failed)</p>
<p><strong>Connection naming patterns:</strong></p>
<ul>
<li>MCLAG: <code>server-01--mclag--leaf-01--leaf-02</code></li>
<li>ESLAG: <code>server-05--eslag--leaf-03--leaf-04</code></li>
<li>Bundled: <code>server-10--bundled--leaf-05</code></li>
<li>Unbundled: <code>server-09--unbundled--leaf-05</code></li>
</ul>
<p><strong>Example workflow:</strong></p>
<pre><code class=""language-bash""># Find connections for server-05
kubectl get connections -n fab | grep server-05

# Output:
# server-05--unbundled--leaf-05

# Fix VPCAttachment spec:
# connection: server-05--unbundled--leaf-05  # (not mclag)
</code></pre>
<hr>
<p><strong>Error Pattern 4: Invalid CIDR</strong></p>
<p><strong>Event:</strong></p>
<pre><code>Type: Warning
Reason: ValidationFailed
Message: Invalid subnet CIDR: 10.0.10.0/33
</code></pre>
<p><strong>Cause:</strong> Invalid CIDR notation (prefix length &gt; 32 for IPv4)</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Fix CIDR notation (e.g., /24 instead of /33)</li>
<li>Update VPC YAML in Gitea</li>
</ol>
<p><strong>Common CIDR mistakes:</strong></p>
<ul>
<li><code>/33</code> or higher (max is /32 for IPv4)</li>
<li>Missing prefix length (e.g., <code>10.0.10.0</code>)</li>
<li>Invalid IP (e.g., <code>10.0.256.0/24</code>)</li>
</ul>
<hr>
<p><strong>Error Pattern 5: Agent Not Ready</strong></p>
<p><strong>Event:</strong></p>
<pre><code>Type: Warning
Reason: AgentNotReady
Message: Switch leaf-03 agent not ready, configuration pending
</code></pre>
<p><strong>Cause:</strong> Switch agent disconnected or switch offline</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Check agent status: <code>kubectl get agents -n fab</code></li>
<li>Verify switch reachable: <code>ping leaf-03.fabric.local</code></li>
<li>Check agent logs: <code>kubectl logs -n fab agent-leaf-03</code></li>
<li>If switch down, investigate switch console/power</li>
</ol>
<p><strong>Grafana Correlation:</strong> Fabric Dashboard shows switch missing metrics</p>
<p><strong>Agent troubleshooting:</strong></p>
<pre><code class=""language-bash""># Check agent pod
kubectl get pods -n fab | grep agent-leaf-03

# If pod is missing, switch hasn&#39;t registered
# Check switch serial console:
hhfab vlab serial leaf-03

# Check fabric-boot logs:
kubectl logs -n fab deployment/fabric-boot
</code></pre>
<h3>Concept 5: Event-Metric Correlation</h3>
<p><strong>Using kubectl Events and Grafana Dashboards Together</strong></p>
<p>The most powerful troubleshooting approach combines multiple data sources. Here are realistic scenarios showing how to correlate events with metrics.</p>
<p><strong>Scenario 1: Interface No Traffic</strong></p>
<p><strong>Grafana shows:</strong></p>
<ul>
<li>Interfaces Dashboard: leaf-01/Ethernet5 has 0 bps traffic</li>
<li>Interface state: UP</li>
</ul>
<p><strong>kubectl investigation:</strong></p>
<pre><code class=""language-bash""># Check if VPCAttachment exists for this interface
kubectl get vpcattachment -A

# Check events for VPCAttachment
kubectl get events --field-selector involvedObject.kind=VPCAttachment

# Found event:
# Warning  VLANConflict  VPCAttachment/vpc1-srv01  VLAN 1010 conflict
</code></pre>
<p><strong>Agent CRD investigation:</strong></p>
<pre><code class=""language-bash""># Check interface state
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq

# Output shows:
# &quot;oper&quot;: &quot;up&quot;  (interface operational)
# No VLANs configured (missing in output)
</code></pre>
<p><strong>Root Cause:</strong> VPCAttachment failed due to VLAN conflict, so VLAN never configured on interface</p>
<p><strong>Correlation:</strong></p>
<ul>
<li>Grafana symptom: Interface UP, no traffic</li>
<li>kubectl events: VLAN conflict error</li>
<li>Agent CRD: No VLAN configured</li>
<li>Root cause: VLAN conflict prevented configuration</li>
</ul>
<p><strong>Solution:</strong> Fix VLAN conflict in VPC YAML</p>
<hr>
<p><strong>Scenario 2: BGP Session Down</strong></p>
<p><strong>Grafana shows:</strong></p>
<ul>
<li>Fabric Dashboard: BGP session leaf-01 ↔ spine-01 down</li>
</ul>
<p><strong>kubectl investigation:</strong></p>
<pre><code class=""language-bash""># Check Agent CRD for BGP state
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors.default[&quot;172.30.128.1&quot;]}&#39; | jq
</code></pre>
<p><strong>Output shows:</strong></p>
<pre><code class=""language-json"">{
  &quot;state&quot;: &quot;idle&quot;,
  &quot;enabled&quot;: true,
  &quot;localAS&quot;: 65101,
  &quot;peerAS&quot;: 65100
}
</code></pre>
<p><strong>Check switch events:</strong></p>
<pre><code class=""language-bash"">kubectl get events -n fab --field-selector involvedObject.name=leaf-01

# Found event:
# Warning  ConfigApplyFailed  switch/leaf-01  BGP peer 172.30.128.1 not reachable
</code></pre>
<p><strong>Root Cause:</strong> Spine-01 not reachable from leaf-01 (network issue)</p>
<p><strong>Correlation:</strong></p>
<ul>
<li>Grafana symptom: BGP session down</li>
<li>Agent CRD: BGP neighbor state = &quot;idle&quot;</li>
<li>kubectl events: BGP peer not reachable</li>
<li>Root cause: Network connectivity issue between leaf and spine</li>
</ul>
<p><strong>Solution:</strong> Investigate spine-01 connectivity (check spine-01 agent, physical links)</p>
<hr>
<p><strong>Scenario 3: Server Can&#39;t Get DHCP</strong></p>
<p><strong>Grafana shows:</strong></p>
<ul>
<li>Interfaces Dashboard: leaf-02/Ethernet10 UP, traffic minimal</li>
<li>Logs Dashboard: No DHCP errors</li>
</ul>
<p><strong>kubectl investigation:</strong></p>
<pre><code class=""language-bash""># Check DHCPSubnet resources
kubectl get dhcpsubnet -A

# Expected: myvpc--default
# Found: Missing!

# Check VPC events
kubectl get events --field-selector involvedObject.name=myvpc

# Found event:
# Warning  SubnetOverlap  vpc/myvpc  Subnet 10.0.10.0/24 overlaps with vpc-2
</code></pre>
<p><strong>Root Cause:</strong> VPC reconciliation failed due to subnet overlap, so DHCPSubnet never created</p>
<p><strong>Correlation:</strong></p>
<ul>
<li>Grafana symptom: Server not getting DHCP (no traffic)</li>
<li>kubectl events: Subnet overlap error</li>
<li>kubectl resources: DHCPSubnet missing</li>
<li>Root cause: Subnet overlap prevented VPC creation</li>
</ul>
<p><strong>Solution:</strong> Fix subnet overlap in VPC YAML</p>
<hr>
<p><strong>Integration Workflow:</strong></p>
<pre><code>1. Grafana: Identify symptom (no traffic, session down, etc.)
           ↓
2. kubectl events: Check for configuration errors
           ↓
3. Agent CRD status: Verify switch state
           ↓
4. Controller logs: Detailed reconciliation info (if needed)
           ↓
5. Solution: Fix configuration, verify in Grafana
</code></pre>
<p><strong>Best Practice:</strong> Start with Grafana for symptoms, use kubectl events for diagnosis, check Agent CRD for confirmation, return to Grafana for verification after fix.</p>
<h2>Troubleshooting</h2>
<h3>Issue: Events are Missing / Expired</h3>
<p><strong>Symptom:</strong> <code>kubectl get events</code> returns no results or only recent events</p>
<p><strong>Cause:</strong> Kubernetes events expire after 1 hour</p>
<p><strong>Solution:</strong></p>
<ol>
<li><p><strong>Check event age:</strong></p>
<pre><code class=""language-bash"">kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39; | head -20
</code></pre>
</li>
<li><p><strong>If events expired:</strong></p>
<ul>
<li>Reproduce issue to generate new events</li>
<li>Check controller logs for historical data:<pre><code class=""language-bash"">kubectl logs -n fab deployment/fabric-controller-manager
</code></pre>
</li>
</ul>
</li>
<li><p><strong>For long-term audit:</strong></p>
<ul>
<li>Configure event export to external logging system</li>
<li>Use Loki/Elasticsearch for long-term event storage</li>
<li>Take screenshots/copies of events during incidents</li>
</ul>
</li>
</ol>
<p><strong>Prevention:</strong> Check events soon after operations, don&#39;t wait hours to troubleshoot.</p>
<hr>
<h3>Issue: Agent CRD Has No Status Data</h3>
<p><strong>Symptom:</strong> <code>kubectl get agent leaf-01 -n fab -o yaml</code> shows <code>status: {}</code> or minimal data</p>
<p><strong>Possible Causes:</strong></p>
<ol>
<li>Agent not connected to switch</li>
<li>Switch not fully registered</li>
<li>Agent pod not running</li>
</ol>
<p><strong>Solution:</strong></p>
<ol>
<li><p><strong>Check agent pod:</strong></p>
<pre><code class=""language-bash"">kubectl get pods -n fab | grep agent-leaf-01
</code></pre>
<p><strong>Expected:</strong> Pod Running</p>
</li>
<li><p><strong>Check agent logs:</strong></p>
<pre><code class=""language-bash"">kubectl logs -n fab agent-leaf-01
</code></pre>
<p><strong>Look for:</strong> Connection errors, gNMI errors</p>
</li>
<li><p><strong>Verify switch registration:</strong></p>
<pre><code class=""language-bash"">kubectl get switch leaf-01 -n fab
</code></pre>
</li>
<li><p><strong>Check switch reachability:</strong></p>
<pre><code class=""language-bash"">ping leaf-01.fabric.local
</code></pre>
</li>
<li><p><strong>If switch not registered:</strong></p>
<ul>
<li>Check fabric-boot logs:<pre><code class=""language-bash"">kubectl logs -n fab deployment/fabric-boot
</code></pre>
</li>
<li>Check switch serial console:<pre><code class=""language-bash"">hhfab vlab serial leaf-01
</code></pre>
</li>
</ul>
</li>
</ol>
<hr>
<h3>Issue: kubectl get events Returns No Results</h3>
<p><strong>Symptom:</strong> Command succeeds but shows no events</p>
<p><strong>Possible Causes:</strong></p>
<ol>
<li>Events expired (&gt; 1 hour old)</li>
<li>Wrong namespace</li>
<li>Resource name mismatch</li>
</ol>
<p><strong>Solution:</strong></p>
<ol>
<li><p><strong>Check all namespaces:</strong></p>
<pre><code class=""language-bash"">kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
</li>
<li><p><strong>Check specific namespace:</strong></p>
<pre><code class=""language-bash"">kubectl get events -n fab
kubectl get events -n default
</code></pre>
</li>
<li><p><strong>Verify resource name:</strong></p>
<pre><code class=""language-bash""># List resources first
kubectl get vpc
kubectl get vpcattachment

# Then query events with correct name
kubectl get events --field-selector involvedObject.name=&lt;exact-name&gt;
</code></pre>
</li>
<li><p><strong>Use describe for events:</strong></p>
<pre><code class=""language-bash"">kubectl describe vpc myvpc
# Events appear at bottom of output
</code></pre>
</li>
</ol>
<hr>
<h3>Issue: Agent CRD jsonpath Queries Return Empty</h3>
<p><strong>Symptom:</strong> <code>kubectl get agent ... -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39;</code> returns nothing</p>
<p><strong>Possible Causes:</strong></p>
<ol>
<li>Interface name incorrect</li>
<li>Agent status not populated</li>
<li>jsonpath syntax error</li>
</ol>
<p><strong>Solution:</strong></p>
<ol>
<li><p><strong>Verify interface exists:</strong></p>
<pre><code class=""language-bash""># List all interfaces
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq &#39;keys&#39;
</code></pre>
<p><strong>Output:</strong> Array of interface names (e.g., <code>[&quot;Ethernet0&quot;, &quot;Ethernet1&quot;, ...]</code>)</p>
</li>
<li><p><strong>Check status exists:</strong></p>
<pre><code class=""language-bash"">kubectl get agent leaf-01 -n fab -o yaml | grep -A 10 &quot;state:&quot;
</code></pre>
</li>
<li><p><strong>Use correct interface name:</strong></p>
<ul>
<li>SONiC interfaces: <code>Ethernet0</code>, <code>Ethernet1</code>, etc. (not <code>Ethernet5</code> if switch only has 0-3)</li>
<li>Check Connection CRDs for actual port names</li>
</ul>
</li>
<li><p><strong>Test jsonpath syntax:</strong></p>
<pre><code class=""language-bash""># Simple test
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.lastHeartbeat}&#39;
</code></pre>
</li>
</ol>
<hr>
<h3>Issue: How to Export Events for Long-Term Audit</h3>
<p><strong>Symptom:</strong> Need to retain events beyond 1-hour Kubernetes retention</p>
<p><strong>Solution:</strong></p>
<p><strong>Option 1: Export to file</strong></p>
<pre><code class=""language-bash""># Export all events
kubectl get events --all-namespaces -o yaml &gt; events-$(date +%Y%m%d-%H%M%S).yaml

# Export specific resource events
kubectl get events --field-selector involvedObject.name=myvpc -o yaml &gt; vpc-events.yaml
</code></pre>
<p><strong>Option 2: Continuous export to logging system</strong></p>
<pre><code class=""language-bash""># Watch events and pipe to logger
kubectl get events --all-namespaces --watch -o json | \
  while read line; do
    echo &quot;$line&quot; | jq -r &#39;[.lastTimestamp, .type, .reason, .involvedObject.name, .message] | @tsv&#39;
  done | logger -t k8s-events
</code></pre>
<p><strong>Option 3: Use event exporter</strong></p>
<ul>
<li>Deploy Kubernetes event exporter to push events to Elasticsearch/Loki</li>
<li>Example: <a href=""https://github.com/opsgenie/kubernetes-event-exporter"">opsgenie/kubernetes-event-exporter</a></li>
</ul>
<p><strong>Option 4: Grafana Loki integration</strong></p>
<ul>
<li>Configure Alloy to collect Kubernetes events</li>
<li>Query historical events in Grafana Logs Dashboard</li>
</ul>
<p><strong>Best Practice:</strong> For production, configure event export to external logging. For labs/dev, export to file when troubleshooting.</p>
<h2>Resources</h2>
<h3>Kubernetes Documentation</h3>
<ul>
<li><a href=""https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/"">Kubernetes Events</a></li>
<li><a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-events-em-"">kubectl get events</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/"">Field Selectors</a></li>
</ul>
<h3>Hedgehog Documentation</h3>
<ul>
<li>Hedgehog CRD Reference (CRD_REFERENCE.md in research folder)</li>
<li>Hedgehog Observability Guide (OBSERVABILITY.md in research folder)</li>
<li>Hedgehog Fabric Controller Documentation</li>
</ul>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""module-3.2-dashboard-interpretation.md"">Module 3.2: Dashboard Interpretation</a></li>
<li>Next: Module 3.4: Pre-Support Diagnostic Checklist (coming soon)</li>
<li>Pathway: Network Like a Hyperscaler</li>
</ul>
<h3>kubectl Commands Quick Reference</h3>
<p><strong>Event Monitoring:</strong></p>
<pre><code class=""language-bash""># All events, recent first
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39;

# Warning events only
kubectl get events --all-namespaces --field-selector type=Warning

# Events for specific resource
kubectl get events --field-selector involvedObject.name=myvpc

# Events for resource type
kubectl get events --field-selector involvedObject.kind=VPC

# Watch events live
kubectl get events --all-namespaces --watch

# Events in last hour (all events are &lt; 1 hour due to retention)
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39; | head -20
</code></pre>
<p><strong>Agent CRD Queries:</strong></p>
<pre><code class=""language-bash""># List all agents
kubectl get agents -n fab

# Get agent full status
kubectl get agent leaf-01 -n fab -o yaml

# Check agent heartbeat
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.lastHeartbeat}&#39;

# Check agent version
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.version}&#39;

# View all interfaces
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq

# View specific interface
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq

# View BGP neighbors
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq

# View specific BGP neighbor state
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors.default[&quot;172.30.128.10&quot;].state}&#39;

# View platform health
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.platform}&#39; | jq

# View ASIC resources
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.criticalResources}&#39; | jq

# Check agent conditions (Ready)
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.conditions[?(@.type==&quot;Ready&quot;)].status}&#39;
</code></pre>
<p><strong>VPC/VPCAttachment Queries:</strong></p>
<pre><code class=""language-bash""># List VPCs with events
kubectl describe vpc myvpc

# Get VPCAttachment events
kubectl describe vpcattachment vpc1-srv01

# List DHCPSubnets
kubectl get dhcpsubnet -A

# Check Connection exists
kubectl get connection -n fab | grep server-01
</code></pre>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve learned to use kubectl events and Agent CRD status for integrated troubleshooting with Grafana. Ready for diagnostic collection in Module 3.4.</p>
",303,15,,,"hedgehog,fabric,kubernetes,events,troubleshooting,observability"
198578604063,Fabric Telemetry Overview,fabric-operations-telemetry-overview,[object Object],"Learn how Hedgehog collects and stores fabric telemetry. Explore Prometheus queries, metric types, and data retention for network observability.","<h2>Introduction</h2>
<p>In Course 2, you became proficient at provisioning VPCs, attaching servers, and validating connectivity. You created resources and verified they worked. But in production operations, provisioning is just the beginning. The real questions become:</p>
<ul>
<li><strong>Is my fabric healthy right now?</strong></li>
<li><strong>How much traffic is flowing through each interface?</strong></li>
<li><strong>Are my switches experiencing errors?</strong></li>
<li><strong>Is my network performing as expected?</strong></li>
</ul>
<p>This is where <strong>observability</strong> comes in—the ability to see what&#39;s happening inside your fabric over time. Course 2 was about <strong>creating resources</strong> (VPCs, attachments, validation). Course 3 is about <strong>observing resources</strong> (metrics, health, trends, diagnostics).</p>
<p>In this module, you&#39;ll learn how Hedgehog collects telemetry from switches, the path metrics take from switch to dashboard, different types of metrics (counters vs gauges), how to query raw metrics in Prometheus, and how long metrics are retained and why. You&#39;ll access the Prometheus UI directly and run queries to see the raw metrics that power Grafana dashboards.</p>
<p>Hedgehog uses the <strong>LGTM stack</strong> (Loki + Grafana + Tempo + Mimir) for observability:</p>
<ul>
<li><strong>Prometheus</strong>: Time-series metrics database</li>
<li><strong>Grafana</strong>: Visualization and dashboards</li>
<li><strong>Loki</strong>: Log aggregation</li>
<li><strong>Alloy</strong>: Telemetry collector (runs on switches)</li>
</ul>
<p>In your lab environment, these tools run in the External Management K3s Cluster (EMKC) alongside ArgoCD and Gitea.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Explain telemetry architecture</strong> - Describe how metrics flow from switches to Grafana</li>
<li><strong>Identify metric sources</strong> - List where metrics originate (Alloy agents, fabric-proxy, Prometheus)</li>
<li><strong>Distinguish metric types</strong> - Differentiate between counters, gauges, and their use cases</li>
<li><strong>Navigate Prometheus UI</strong> - Query basic fabric metrics using PromQL</li>
<li><strong>Understand data retention</strong> - Explain how long metrics are stored and why</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Course 1 completion (Foundations &amp; Interfaces)</li>
<li>Course 2 completion (Provisioning &amp; Day 1 Operations)</li>
<li>Understanding of GitOps workflow and three interfaces</li>
<li>VPC provisioning experience</li>
<li>Basic understanding of metrics and time-series data (helpful but not required)</li>
</ul>
<h2>Scenario: Exploring Fabric Telemetry</h2>
<p>You&#39;ve been operating Hedgehog Fabric for several weeks. VPCs are provisioned, servers are attached, and traffic is flowing. Now you want to understand the telemetry system that monitors your fabric. In this hands-on exploration, you&#39;ll access Prometheus directly and run queries to see the raw metrics that power your observability stack.</p>
<p><strong>Environment Access:</strong></p>
<ul>
<li><strong>Prometheus:</strong> <a href=""http://localhost:9090"">http://localhost:9090</a></li>
<li><strong>Grafana:</strong> <a href=""http://localhost:3000"">http://localhost:3000</a> (we&#39;ll use in Module 3.2)</li>
<li><strong>kubectl:</strong> Already configured</li>
</ul>
<h3>Task 1: Access Prometheus UI</h3>
<p><strong>Objective:</strong> Navigate to Prometheus and understand the interface</p>
<ol>
<li><p><strong>Open Prometheus in your browser:</strong></p>
<ul>
<li>Navigate to <a href=""http://localhost:9090"">http://localhost:9090</a></li>
<li>You should see the Prometheus query interface</li>
</ul>
</li>
<li><p><strong>Explore the UI sections:</strong></p>
<ul>
<li><strong>Graph tab</strong>: Query and visualize metrics</li>
<li><strong>Alerts tab</strong>: Active alerts (if configured)</li>
<li><strong>Status dropdown</strong>:<ul>
<li><strong>Targets</strong>: View scrape targets (switches)</li>
<li><strong>Configuration</strong>: Prometheus config</li>
<li><strong>Service Discovery</strong>: How Prometheus finds targets</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Check scrape targets:</strong></p>
<ul>
<li>Click <strong>Status → Targets</strong></li>
<li>Look for fabric-proxy and other Hedgehog targets</li>
<li>Verify state = &quot;UP&quot; (healthy scraping)</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Prometheus UI loads successfully</li>
<li>✅ Targets page shows fabric-related endpoints</li>
<li>✅ All targets show state = UP</li>
</ul>
<h3>Task 2: Query Switch Metrics</h3>
<p><strong>Objective:</strong> Run basic PromQL queries to explore switch metrics</p>
<ol>
<li><p><strong>Query CPU usage for all switches:</strong></p>
<p>In the Prometheus query box, enter:</p>
<pre><code class=""language-promql"">cpu_usage_percent
</code></pre>
<p>Click <strong>Execute</strong></p>
<p><strong>Expected result:</strong> List of switches with CPU percentages:</p>
<pre><code>cpu_usage_percent{switch=&quot;leaf-01&quot;} 18.5
cpu_usage_percent{switch=&quot;leaf-02&quot;} 22.3
cpu_usage_percent{switch=&quot;spine-01&quot;} 12.1
...
</code></pre>
<p>Click <strong>Graph</strong> tab to see time series visualization</p>
</li>
<li><p><strong>Query CPU for a specific switch:</strong></p>
<pre><code class=""language-promql"">cpu_usage_percent{switch=&quot;leaf-01&quot;}
</code></pre>
<p><strong>Expected result:</strong> Single time series for leaf-01</p>
</li>
<li><p><strong>Query interface byte counters:</strong></p>
<pre><code class=""language-promql"">interface_bytes_out{switch=&quot;leaf-01&quot;}
</code></pre>
<p><strong>Expected result:</strong> Counter values for all interfaces on leaf-01:</p>
<pre><code>interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;} 152345678901
interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet2&quot;} 98234567890
...
</code></pre>
<p><strong>Note:</strong> These are counters (large numbers that keep increasing)</p>
</li>
<li><p><strong>Calculate bandwidth utilization:</strong></p>
<pre><code class=""language-promql"">rate(interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;}[5m]) * 8
</code></pre>
<p><strong>Expected result:</strong> Bits per second over last 5 minutes:</p>
<pre><code>{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;} 125600000  # ~125 Mbps
</code></pre>
<p><strong>Explanation:</strong></p>
<ul>
<li><code>rate([5m])</code> calculates bytes/sec over 5 minutes</li>
<li><code>* 8</code> converts bytes to bits</li>
<li>Result is bandwidth in bits per second</li>
</ul>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ CPU metrics display for all switches</li>
<li>✅ Interface counters visible</li>
<li>✅ Bandwidth calculation returns reasonable values</li>
</ul>
<h3>Task 3: Explore BGP Metrics</h3>
<p><strong>Objective:</strong> Query BGP neighbor status to verify fabric underlay</p>
<ol>
<li><p><strong>Query all BGP neighbors:</strong></p>
<pre><code class=""language-promql"">bgp_neighbor_state
</code></pre>
<p><strong>Expected result:</strong> BGP neighbors with state labels:</p>
<pre><code>bgp_neighbor_state{switch=&quot;leaf-01&quot;,neighbor=&quot;172.30.128.1&quot;,state=&quot;established&quot;} 1
bgp_neighbor_state{switch=&quot;leaf-01&quot;,neighbor=&quot;172.30.128.2&quot;,state=&quot;established&quot;} 1
...
</code></pre>
</li>
<li><p><strong>Filter for non-established neighbors (find problems):</strong></p>
<pre><code class=""language-promql"">bgp_neighbor_state{state!=&quot;established&quot;}
</code></pre>
<p><strong>Expected result:</strong> Empty (all neighbors healthy) or list of down neighbors</p>
</li>
<li><p><strong>Count total BGP sessions:</strong></p>
<pre><code class=""language-promql"">count(bgp_neighbor_state)
</code></pre>
<p><strong>Expected result:</strong> Total number of BGP sessions in fabric (e.g., 40)</p>
</li>
<li><p><strong>Count established BGP sessions:</strong></p>
<pre><code class=""language-promql"">count(bgp_neighbor_state{state=&quot;established&quot;})
</code></pre>
<p><strong>Expected result:</strong> Should match total (if fabric is healthy)</p>
</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ BGP metrics visible</li>
<li>✅ All (or most) neighbors show state=&quot;established&quot;</li>
<li>✅ Count queries return expected numbers</li>
</ul>
<h3>Task 4: Understand Metric Labels</h3>
<p><strong>Objective:</strong> Learn how labels identify specific metrics</p>
<ol>
<li><p><strong>Query metrics with multiple labels:</strong></p>
<pre><code class=""language-promql"">interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;}
</code></pre>
<p><strong>Observe labels:</strong></p>
<ul>
<li><code>switch=&quot;leaf-01&quot;</code> - Which switch</li>
<li><code>interface=&quot;Ethernet1&quot;</code> - Which interface</li>
</ul>
</li>
<li><p><strong>Query with partial label matching:</strong></p>
<pre><code class=""language-promql"">interface_bytes_out{switch=~&quot;leaf-.*&quot;}
</code></pre>
<p><strong>Explanation:</strong> <code>=~</code> means &quot;matches regex&quot;, <code>leaf-.*</code> matches all leaf switches</p>
</li>
<li><p><strong>List all unique switches in metrics:</strong></p>
<pre><code class=""language-promql"">count by (switch) (cpu_usage_percent)
</code></pre>
<p><strong>Expected result:</strong> One entry per switch</p>
</li>
</ol>
<p><strong>Key Insight:</strong> Labels allow you to filter and aggregate metrics. Every metric has labels like <code>switch</code>, <code>interface</code>, <code>neighbor</code>, etc.</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Understand how labels filter metrics</li>
<li>✅ Can query specific switch or interface</li>
<li>✅ See how regex matching works</li>
</ul>
<h3>Lab Summary</h3>
<p><strong>What you accomplished:</strong></p>
<ul>
<li>✅ Accessed Prometheus UI and verified scrape targets</li>
<li>✅ Queried switch CPU and interface metrics</li>
<li>✅ Examined BGP neighbor status</li>
<li>✅ Calculated bandwidth utilization using PromQL</li>
<li>✅ Understood metric labels and filtering</li>
</ul>
<p><strong>What you learned:</strong></p>
<ul>
<li>Prometheus provides raw metric access for your fabric</li>
<li>PromQL enables powerful filtering and calculations</li>
<li>Labels identify specific metrics (switch, interface, neighbor)</li>
<li>Counters require <code>rate()</code> function for meaningful data</li>
<li>BGP metrics help verify fabric underlay health</li>
</ul>
<h2>Concepts &amp; Deep Dive</h2>
<h3>Telemetry Architecture</h3>
<p>Understanding how metrics flow from switches to your dashboard helps you troubleshoot when metrics are missing.</p>
<p><strong>The Telemetry Flow:</strong></p>
<pre><code>┌─────────────────────────────────────────┐
│          SONiC Switch (e.g., leaf-01)   │
│                                         │
│  ┌──────────────┐    ┌──────────────┐ │
│  │Fabric Agent  │◄───│Alloy Collector│ │
│  │(metrics      │    │(scrapes every │ │
│  │ source)      │    │ 120 seconds)  │ │
│  └──────────────┘    └───────┬───────┘ │
│                               │         │
│  ┌──────────────┐             │         │
│  │Node Exporter │◄────────────┘         │
│  │(system       │                       │
│  │ metrics)     │                       │
│  └──────────────┘                       │
│                               │         │
└───────────────────────────────┼─────────┘
                                │
                                │ Push metrics
                                ▼
                    ┌───────────────────┐
                    │  Control Node     │
                    │  ┌─────────────┐  │
                    │  │fabric-proxy │  │
                    │  │(aggregator) │  │
                    │  └──────┬──────┘  │
                    └─────────┼─────────┘
                              │
                              │ Remote Write
                              ▼
                  ┌───────────────────────┐
                  │   EMKC Cluster        │
                  │  ┌─────────────────┐  │
                  │  │  Prometheus     │  │
                  │  │  (stores metrics)│  │
                  │  └────────┬────────┘  │
                  │           │           │
                  │           ▼           │
                  │  ┌─────────────────┐  │
                  │  │   Grafana       │  │
                  │  │  (visualizes)   │  │
                  │  └─────────────────┘  │
                  └───────────────────────┘
</code></pre>
<p><strong>Components Explained:</strong></p>
<p><strong>1. Alloy Agents (on each switch)</strong></p>
<ul>
<li>Grafana&#39;s telemetry collector</li>
<li>Scrapes Fabric Agent metrics every 120 seconds</li>
<li>Scrapes Node Exporter metrics (CPU, memory, disk)</li>
<li>Collects syslog for log aggregation</li>
<li>Configured via <code>defaultAlloyConfig</code> in Fabricator</li>
</ul>
<p><strong>2. fabric-proxy (control node)</strong></p>
<ul>
<li>Receives metrics from all Alloy agents</li>
<li>Forwards to Prometheus via Remote Write protocol</li>
<li>Runs as a service on port 31028</li>
<li>Single aggregation point for all switch telemetry</li>
</ul>
<p><strong>3. Prometheus (EMKC)</strong></p>
<ul>
<li>Time-series database for metrics</li>
<li>Stores data with timestamps</li>
<li>Provides PromQL query language</li>
<li>Retention: 15 days by default (configurable)</li>
</ul>
<p><strong>4. Grafana (EMKC)</strong></p>
<ul>
<li>Queries Prometheus for data</li>
<li>Renders visualizations and dashboards</li>
<li>Provides 6 pre-built Hedgehog dashboards</li>
<li>Web UI: <a href=""http://localhost:3000"">http://localhost:3000</a></li>
</ul>
<p><strong>Key Insight:</strong> Metrics start on switches, flow through fabric-proxy, land in Prometheus, and are visualized in Grafana. Understanding this flow helps troubleshoot when metrics are missing.</p>
<h3>Metric Sources</h3>
<p>Hedgehog collects metrics from four primary sources:</p>
<p><strong>1. Fabric Agent (Switch Metrics)</strong></p>
<p>The Fabric Agent runs on each SONiC switch and provides:</p>
<ul>
<li><strong>BGP metrics</strong>: Neighbor status (up/down), prefixes received/sent</li>
<li><strong>Interface metrics</strong>: Packet counters, byte counters, error rates, operational state</li>
<li><strong>ASIC critical resources</strong>: Route table usage, ARP table size, ACL capacity</li>
<li><strong>Platform metrics</strong>: PSU voltage, fan speeds, temperature sensors</li>
</ul>
<p><strong>2. Node Exporter (System Metrics)</strong></p>
<p>Linux system metrics from each switch:</p>
<ul>
<li><strong>CPU</strong>: Utilization (user, system, idle, iowait), load average</li>
<li><strong>Memory</strong>: Total/available/free, swap usage</li>
<li><strong>Disk</strong>: Space used/available, I/O operations</li>
<li><strong>Network</strong>: Interface traffic (all ports), connection states</li>
</ul>
<p><strong>3. Fabric Controller (Control Plane Metrics)</strong></p>
<p>Kubernetes controller metrics:</p>
<ul>
<li>CRD reconciliation status</li>
<li>VPC allocation counts</li>
<li>Controller health and performance</li>
<li>API request latencies</li>
</ul>
<p><strong>4. DHCP Server (Network Services Metrics)</strong></p>
<p>DHCP service metrics:</p>
<ul>
<li>Lease counts per VPC subnet</li>
<li>Pool utilization percentage</li>
<li>DHCP request/response rates</li>
<li>Lease expiration tracking</li>
</ul>
<p><strong>Metric Access Pattern:</strong></p>
<ul>
<li><strong>Fabric Agent</strong>: Exposed on switch, scraped by Alloy</li>
<li><strong>Node Exporter</strong>: Built into SONiC OS, scraped by Alloy</li>
<li><strong>Controller</strong>: Exposes metrics on control plane, scraped directly by Prometheus</li>
<li><strong>DHCP</strong>: Metrics available via controller</li>
</ul>
<h3>Metric Types</h3>
<p>Prometheus uses different metric types for different data. Understanding the difference is crucial for writing correct queries.</p>
<p><strong>Counters (Monotonically Increasing)</strong></p>
<ul>
<li><strong>Definition</strong>: Values that only go up (never decrease)</li>
<li><strong>Reset</strong>: Only reset to 0 when system restarts</li>
<li><strong>Examples</strong>:<ul>
<li>Bytes transmitted on interface (keeps counting up)</li>
<li>Total BGP updates received (cumulative)</li>
<li>Packet errors (total count)</li>
</ul>
</li>
</ul>
<p><strong>Counter Example:</strong></p>
<pre><code>interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;} 1523456789
interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;} 1523789012  # 2 minutes later
interface_bytes_out{switch=&quot;leaf-01&quot;,interface=&quot;Ethernet1&quot;} 1524123456  # 2 minutes later
</code></pre>
<p><strong>Using Counters:</strong></p>
<ul>
<li>Raw counter values aren&#39;t directly useful (just big numbers)</li>
<li>Use <code>rate()</code> function to calculate per-second rates</li>
<li>Example: <code>rate(interface_bytes_out[5m])</code> = bytes per second over last 5 minutes</li>
</ul>
<p><strong>Gauges (Point-in-Time Values)</strong></p>
<ul>
<li><strong>Definition</strong>: Values that can go up or down</li>
<li><strong>Fluctuate</strong>: Represent current state at moment of scrape</li>
<li><strong>Examples</strong>:<ul>
<li>CPU percentage (0-100%)</li>
<li>Interface operational state (up=1, down=0)</li>
<li>BGP neighbor count (can increase or decrease)</li>
<li>Temperature in Celsius (varies with load)</li>
</ul>
</li>
</ul>
<p><strong>Gauge Example:</strong></p>
<pre><code>cpu_usage_percent{switch=&quot;leaf-01&quot;} 23.5
cpu_usage_percent{switch=&quot;leaf-01&quot;} 45.2  # 2 minutes later (CPU spike)
cpu_usage_percent{switch=&quot;leaf-01&quot;} 18.7  # 2 minutes later (CPU normalized)
</code></pre>
<p><strong>Using Gauges:</strong></p>
<ul>
<li>Values are directly meaningful</li>
<li>Can use directly in dashboards</li>
<li>Can set alerts on thresholds (e.g., CPU &gt; 80%)</li>
</ul>
<p><strong>Why This Matters:</strong></p>
<p>Choosing the right PromQL function depends on metric type:</p>
<ul>
<li><code>rate()</code> or <code>irate()</code> for counters (calculate change rate)</li>
<li>Direct value or <code>avg()</code> for gauges</li>
<li>Using <code>rate()</code> on a gauge or treating a counter like a gauge produces incorrect results</li>
</ul>
<h3>Introduction to PromQL</h3>
<p>PromQL is the query language for Prometheus, similar to SQL for databases. It enables powerful filtering, aggregation, and calculation capabilities.</p>
<p><strong>Basic Query Patterns:</strong></p>
<p><strong>1. Select a metric:</strong></p>
<pre><code class=""language-promql""># Get CPU usage for all switches
cpu_usage_percent
</code></pre>
<p><strong>2. Filter by labels:</strong></p>
<pre><code class=""language-promql""># Get CPU usage for specific switch
cpu_usage_percent{switch=&quot;leaf-01&quot;}

# Get interface bytes for specific interface
interface_bytes_out{switch=&quot;leaf-01&quot;, interface=&quot;Ethernet1&quot;}
</code></pre>
<p><strong>3. Calculate rates (for counters):</strong></p>
<pre><code class=""language-promql""># Bytes per second transmitted over last 5 minutes
rate(interface_bytes_out{switch=&quot;leaf-01&quot;, interface=&quot;Ethernet1&quot;}[5m])
</code></pre>
<p><strong>4. Aggregate across labels:</strong></p>
<pre><code class=""language-promql""># Total bytes per second across all interfaces on leaf-01
sum(rate(interface_bytes_out{switch=&quot;leaf-01&quot;}[5m]))

# Average CPU across all switches
avg(cpu_usage_percent)

# Maximum temperature across all sensors
max(temperature_celsius)
</code></pre>
<p><strong>5. Arithmetic operations:</strong></p>
<pre><code class=""language-promql""># Convert bytes/sec to bits/sec
rate(interface_bytes_out[5m]) * 8

# Calculate percentage
(memory_used / memory_total) * 100
</code></pre>
<p><strong>Common Functions:</strong></p>
<ul>
<li><code>rate()</code>: Per-second average rate over time range</li>
<li><code>irate()</code>: Instant rate (last 2 data points)</li>
<li><code>sum()</code>: Add values across dimensions</li>
<li><code>avg()</code>: Average values</li>
<li><code>max()</code> / <code>min()</code>: Maximum/minimum values</li>
<li><code>count()</code>: Count number of time series</li>
</ul>
<p><strong>Time Ranges:</strong></p>
<ul>
<li><code>[5m]</code>: Last 5 minutes</li>
<li><code>[1h]</code>: Last 1 hour</li>
<li><code>[24h]</code>: Last 24 hours</li>
</ul>
<p><strong>Example Queries:</strong></p>
<pre><code class=""language-promql""># BGP neighbors that are down
bgp_neighbor_state{state!=&quot;established&quot;}

# Interface error rate
rate(interface_errors_in[5m])

# Switches with high CPU
cpu_usage_percent &gt; 80

# Total VPCs in fabric
count(kube_vpc_info)
</code></pre>
<p><strong>Pro Tip:</strong> Start simple, add complexity incrementally. Test in Prometheus UI before using in Grafana dashboards.</p>
<h3>Data Retention and Aggregation</h3>
<p><strong>How Long Are Metrics Stored?</strong></p>
<p>Prometheus stores all metrics for <strong>15 days by default</strong> in your environment. After 15 days, data is automatically deleted to manage disk space.</p>
<p><strong>Why 15 Days?</strong></p>
<ul>
<li>Sufficient for troubleshooting recent issues</li>
<li>Balances storage cost with usefulness</li>
<li>Captures weekly patterns (7 days × 2)</li>
<li>Allows historical comparison</li>
</ul>
<p><strong>What Happens After 15 Days?</strong></p>
<ul>
<li>Metrics are deleted (no long-term storage by default)</li>
<li>For long-term retention, use Mimir (Prometheus long-term storage)</li>
<li>Grafana dashboards will show &quot;No Data&quot; for queries beyond retention</li>
</ul>
<p><strong>Scrape Interval: 120 seconds (2 minutes)</strong></p>
<ul>
<li>Alloy scrapes metrics every 2 minutes</li>
<li>This is the resolution of your data</li>
<li>You cannot see changes faster than 2 minutes</li>
<li>Reduces switch CPU load compared to 1-second scraping</li>
</ul>
<p><strong>Data Resolution Trade-offs:</strong></p>
<ul>
<li><strong>Faster scraping (e.g., 30 sec)</strong>: Higher resolution, more disk space, more switch CPU</li>
<li><strong>Slower scraping (e.g., 5 min)</strong>: Lower resolution, less disk space, less switch CPU</li>
<li><strong>Hedgehog default (2 min)</strong>: Balanced for typical network monitoring</li>
</ul>
<p><strong>Storage Calculations:</strong></p>
<p>Approximate storage per switch:</p>
<ul>
<li>~100 metrics per switch</li>
<li>120-second interval = 30 samples per hour</li>
<li>30 samples × 24 hours × 15 days = 10,800 samples per metric</li>
<li>Total: ~1 million data points per switch (minimal disk usage)</li>
</ul>
<p><strong>Querying Across Time:</strong></p>
<pre><code class=""language-promql""># Last 5 minutes (always available)
rate(interface_bytes_out[5m])

# Last 7 days (within retention)
rate(interface_bytes_out[7d])

# Last 30 days (ERROR - exceeds retention)
rate(interface_bytes_out[30d])  # Returns no data
</code></pre>
<p><strong>Best Practice:</strong> For long-term capacity planning, export Grafana dashboard snapshots monthly or configure Mimir for extended retention.</p>
<h2>Troubleshooting</h2>
<h3>No Metrics Appearing in Prometheus</h3>
<p><strong>Symptom:</strong> Prometheus shows no switch metrics or &quot;No Data&quot; on queries</p>
<p><strong>Cause:</strong> Alloy agents not configured or fabric-proxy not running</p>
<p><strong>Fix:</strong></p>
<ol>
<li><p><strong>Verify Alloy configuration:</strong></p>
<pre><code class=""language-bash"">kubectl get fabricator -n fab -o yaml | grep -A 20 defaultAlloyConfig
</code></pre>
<p>If <code>defaultAlloyConfig: {}</code>, telemetry is disabled. See Hedgehog documentation for enabling Alloy.</p>
</li>
<li><p><strong>Check fabric-proxy status:</strong></p>
<pre><code class=""language-bash"">kubectl get svc -n fab fabric-proxy
kubectl get pods -n fab | grep fabric-proxy
</code></pre>
</li>
<li><p><strong>Verify Prometheus is running:</strong></p>
<pre><code class=""language-bash"">curl http://localhost:9090/-/healthy
</code></pre>
</li>
</ol>
<h3>Metrics Missing for Specific Switch</h3>
<p><strong>Symptom:</strong> Metrics appear for some switches but not others</p>
<p><strong>Cause:</strong> Alloy agent on specific switch stopped or not running</p>
<p><strong>Fix:</strong></p>
<ol>
<li><p><strong>Check Prometheus targets:</strong></p>
<ul>
<li>Navigate to <a href=""http://localhost:9090/targets"">http://localhost:9090/targets</a></li>
<li>Look for down targets</li>
</ul>
</li>
<li><p><strong>SSH to affected switch and check Alloy:</strong></p>
<pre><code class=""language-bash"">hhfab vlab ssh leaf-01
systemctl status alloy
</code></pre>
</li>
<li><p><strong>Restart Alloy if needed:</strong></p>
<pre><code class=""language-bash"">systemctl restart alloy
</code></pre>
</li>
</ol>
<h3>PromQL Query Syntax Errors</h3>
<p><strong>Symptom:</strong> &quot;parse error&quot; or &quot;bad_data&quot; when running queries</p>
<p><strong>Cause:</strong> Incorrect PromQL syntax</p>
<p><strong>Common Mistakes:</strong></p>
<ol>
<li><p><strong>Using rate() on gauges:</strong></p>
<pre><code class=""language-promql""># WRONG
rate(cpu_usage_percent[5m])

# CORRECT
cpu_usage_percent
</code></pre>
</li>
<li><p><strong>Forgetting time range with rate():</strong></p>
<pre><code class=""language-promql""># WRONG
rate(interface_bytes_out)

# CORRECT
rate(interface_bytes_out[5m])
</code></pre>
</li>
<li><p><strong>Invalid label syntax:</strong></p>
<pre><code class=""language-promql""># WRONG
interface_bytes_out{switch=leaf-01}

# CORRECT
interface_bytes_out{switch=&quot;leaf-01&quot;}
</code></pre>
</li>
</ol>
<h3>Retention Window Exceeded</h3>
<p><strong>Symptom:</strong> Query returns no data for time periods older than 15 days</p>
<p><strong>Cause:</strong> Default retention is 15 days</p>
<p><strong>Solution:</strong></p>
<ul>
<li>For recent troubleshooting: Use data within 15-day window</li>
<li>For long-term analysis: Configure Mimir or Grafana Cloud for extended retention</li>
<li>For incident documentation: Export Grafana dashboard snapshots to PDF</li>
</ul>
<h2>Resources</h2>
<h3>Prometheus Documentation</h3>
<ul>
<li><a href=""https://prometheus.io/docs/prometheus/latest/querying/basics/"">PromQL Basics</a></li>
<li><a href=""https://prometheus.io/docs/prometheus/latest/querying/functions/"">PromQL Functions</a></li>
<li><a href=""https://prometheus.io/docs/prometheus/latest/querying/examples/"">Prometheus Query Examples</a></li>
</ul>
<h3>Hedgehog Documentation</h3>
<ul>
<li>Hedgehog Observability Guide (see OBSERVABILITY.md in research folder)</li>
<li>Hedgehog Fabric Controller Documentation</li>
<li>Grafana Dashboard Guide</li>
</ul>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""../course-2-provisioning/module-2.4-decommission-cleanup.md"">Module 2.4: Decommission &amp; Cleanup</a></li>
<li>Next: Module 3.2: Dashboard Interpretation (coming soon)</li>
<li>Pathway: Network Like a Hyperscaler</li>
</ul>
<h3>PromQL Cheat Sheet</h3>
<p><strong>Basic Selectors:</strong></p>
<pre><code class=""language-promql"">metric_name                          # All time series for this metric
metric_name{label=&quot;value&quot;}          # Filter by exact label match
metric_name{label=~&quot;regex&quot;}         # Filter by regex match
metric_name{label!=&quot;value&quot;}         # Exclude label value
</code></pre>
<p><strong>Time Ranges:</strong></p>
<pre><code class=""language-promql"">metric_name[5m]    # Last 5 minutes
metric_name[1h]    # Last 1 hour
metric_name[1d]    # Last 1 day
</code></pre>
<p><strong>Rate Functions:</strong></p>
<pre><code class=""language-promql"">rate(counter[5m])      # Average rate over 5 min
irate(counter[5m])     # Instant rate (last 2 points)
</code></pre>
<p><strong>Aggregation:</strong></p>
<pre><code class=""language-promql"">sum(metric)                        # Sum across all labels
avg(metric)                        # Average
max(metric) / min(metric)          # Maximum / Minimum
count(metric)                      # Count time series
sum by (switch) (metric)           # Sum per switch
avg by (switch,interface) (metric) # Average per switch+interface
</code></pre>
<p><strong>Arithmetic:</strong></p>
<pre><code class=""language-promql"">metric * 8             # Multiply (e.g., bytes to bits)
metric / 1000000       # Divide (e.g., to megabytes)
metric_a + metric_b    # Add metrics
</code></pre>
<p><strong>Comparison:</strong></p>
<pre><code class=""language-promql"">metric &gt; 100           # Greater than
metric &lt; 50            # Less than
metric == 1            # Equals (exact)
</code></pre>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve learned the fundamentals of fabric telemetry and Prometheus. Ready to interpret Grafana dashboards in Module 3.2.</p>
",301,15,,,"hedgehog,fabric,telemetry,prometheus,observability,metrics"
198578604067,VPC Attachments: Connecting Servers,fabric-operations-vpc-attachments,[object Object],"Learn to connect servers to VPCs using VPCAttachments. Master MCLAG, ESLAG, Bundled, and Unbundled connection types with hands-on provisioning.","<h2>Introduction</h2>
<p>In Module 2.1, you created the <code>web-app-prod</code> VPC with two subnets: <code>web-servers</code> (static IPs) and <code>worker-nodes</code> (DHCP). Your VPC is fully configured and ready—but there&#39;s one problem: <strong>no servers can reach it yet</strong>.</p>
<p>A VPC without servers is like a road without cars—perfectly built but completely unused. To make your VPC operational, you need to <strong>attach servers</strong> to it. This is where <strong>VPCAttachment</strong> comes in.</p>
<p>VPCAttachment bridges the virtual and physical layers of your infrastructure. It binds a VPC subnet (virtual network) to a server connection (physical wiring), automatically configuring the switches to enable connectivity. Once attached, your servers can communicate within the VPC, and your three-tier application network comes to life.</p>
<p>In this module, you&#39;ll attach two servers to the <code>web-app-prod</code> VPC you created in Module 2.1, learning about different connection types and validating the complete connectivity workflow.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Create VPCAttachments</strong> - Connect servers to VPC subnets using YAML manifests</li>
<li><strong>Distinguish connection types</strong> - Explain MCLAG, ESLAG, Bundled, and Unbundled connections</li>
<li><strong>Validate server connectivity</strong> - Confirm VPCAttachment status and server network access</li>
<li><strong>Troubleshoot attachment issues</strong> - Diagnose common VPCAttachment problems</li>
<li><strong>Understand the three-tier hierarchy</strong> - Recognize Wiring → VPC → VPCAttachment relationship</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 2.1 completion (VPC Provisioning Essentials)</li>
<li>Existing VPC (<code>web-app-prod</code> from Module 2.1)</li>
<li>Understanding of server-to-switch connections (from Module 1.1)</li>
<li>kubectl access to Hedgehog fabric</li>
<li>Basic familiarity with YAML syntax</li>
</ul>
<h2>Scenario: Completing the Connectivity Workflow</h2>
<p>The <code>web-app-prod</code> VPC is ready, but no servers can reach it yet. Your task: attach two servers to the VPC—<code>server-01</code> (MCLAG connection) to the <code>web-servers</code> subnet for static IPs, and <code>server-05</code> (ESLAG connection) to the <code>worker-nodes</code> subnet for DHCP. This will complete the connectivity workflow and make the VPC operational, allowing your web application to serve traffic and your worker nodes to scale dynamically.</p>
<h2>Lab Steps</h2>
<h3>Step 1: Review Existing Infrastructure</h3>
<p><strong>Objective:</strong> Verify VPC exists and identify available server connections</p>
<p>Before attaching servers, confirm your infrastructure is ready. You&#39;ll verify the VPC from Module 2.1 exists and discover available servers and their connection types.</p>
<p>List switches in the fabric:</p>
<pre><code class=""language-bash"">kubectl get switches -n fab
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME      AGE
leaf-01   2h
leaf-02   2h
leaf-03   2h
leaf-04   2h
leaf-05   2h
spine-01  2h
spine-02  2h
</code></pre>
<p>List servers in the fabric:</p>
<pre><code class=""language-bash"">kubectl get servers -n fab
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME        AGE
server-01   2h
server-02   2h
server-03   2h
server-04   2h
server-05   2h
server-06   2h
server-07   2h
server-08   2h
server-09   2h
server-10   2h
</code></pre>
<p>List server connections:</p>
<pre><code class=""language-bash"">kubectl get connections -n fab | grep server-
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME                                 AGE
server-01--mclag--leaf-01--leaf-02   2h
server-02--mclag--leaf-01--leaf-02   2h
server-03--unbundled--leaf-01        2h
server-04--bundled--leaf-02          2h
server-05--eslag--leaf-03--leaf-04   2h
server-06--eslag--leaf-03--leaf-04   2h
server-07--unbundled--leaf-03        2h
server-08--bundled--leaf-04          2h
server-09--unbundled--leaf-05        2h
server-10--bundled--leaf-05          2h
</code></pre>
<p>Verify the VPC from Module 2.1 exists:</p>
<pre><code class=""language-bash"">kubectl get vpc web-app-prod
</code></pre>
<p>Expected output:</p>
<pre><code>NAME           AGE
web-app-prod   15m
</code></pre>
<p>View VPC details to confirm subnets:</p>
<pre><code class=""language-bash"">kubectl get vpc web-app-prod -o yaml | grep -A 20 &quot;subnets:&quot;
</code></pre>
<p>You should see the two subnets you created: <code>web-servers</code> (10.10.10.0/24) and <code>worker-nodes</code> (10.10.20.0/24 with DHCP).</p>
<p><strong>Identify target servers for attachment:</strong></p>
<p>For this lab, you&#39;ll attach:</p>
<ul>
<li><strong>server-01</strong>: MCLAG connection to leaf-01 and leaf-02 → will attach to <code>web-servers</code> subnet (static IP)</li>
<li><strong>server-05</strong>: ESLAG connection to leaf-03 and leaf-04 → will attach to <code>worker-nodes</code> subnet (DHCP)</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can list switches, servers, and connections</li>
<li>✅ VPC <code>web-app-prod</code> exists with two subnets</li>
<li>✅ Identified server-01 (MCLAG) and server-05 (ESLAG) connection types</li>
</ul>
<h3>Step 2: Create VPCAttachment for server-01 (MCLAG, Static IP)</h3>
<p><strong>Objective:</strong> Bind server-01 to the web-servers subnet</p>
<p>Create the VPCAttachment manifest for server-01:</p>
<pre><code class=""language-bash"">cat &gt; server-01-attachment.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-01-web-servers
  namespace: default
spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/web-servers
EOF
</code></pre>
<p><strong>Understanding the YAML:</strong></p>
<ul>
<li><strong>apiVersion/kind</strong>: Defines this as a VPCAttachment resource</li>
<li><strong>metadata.name</strong>: Descriptive name for this attachment (<code>server-01-web-servers</code>)</li>
<li><strong>spec.connection</strong>: References the Connection CRD name (must match exactly)</li>
<li><strong>spec.subnet</strong>: VPC/subnet format (<code>vpc-name/subnet-name</code>)</li>
</ul>
<p><strong>Key points about this attachment:</strong></p>
<ul>
<li><strong>Connection type</strong>: MCLAG (dual-homed to leaf-01 and leaf-02 for redundancy)</li>
<li><strong>Subnet</strong>: <code>web-servers</code> (IPv4 subnet without DHCP, for static IP assignment)</li>
<li><strong>Result</strong>: VLAN 1010 will be configured on server-01&#39;s ports on both switches</li>
</ul>
<p>Apply the VPCAttachment:</p>
<pre><code class=""language-bash"">kubectl apply -f server-01-attachment.yaml
</code></pre>
<p>Expected output:</p>
<pre><code>vpcattachment.vpc.githedgehog.com/server-01-web-servers created
</code></pre>
<p>Validate the attachment was created:</p>
<pre><code class=""language-bash""># List all VPCAttachments
kubectl get vpcattachments

# Get detailed information
kubectl describe vpcattachment server-01-web-servers
</code></pre>
<p>Check for events showing reconciliation progress:</p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=server-01-web-servers --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p>Look for events like:</p>
<ul>
<li><code>Normal  Created</code> - VPCAttachment created</li>
<li><code>Normal  Reconciling</code> - Fabric controller processing</li>
<li><code>Normal  Applied</code> - Configuration sent to switches</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPCAttachment created successfully</li>
<li>✅ Status shows reconciliation events</li>
<li>✅ No error events in describe output</li>
</ul>
<h3>Step 3: Create VPCAttachment for server-05 (ESLAG, DHCP)</h3>
<p><strong>Objective:</strong> Bind server-05 to the worker-nodes subnet with DHCP</p>
<p>Create the VPCAttachment manifest for server-05:</p>
<pre><code class=""language-bash"">cat &gt; server-05-attachment.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-05-worker-nodes
  namespace: default
spec:
  connection: server-05--eslag--leaf-03--leaf-04
  subnet: web-app-prod/worker-nodes
EOF
</code></pre>
<p><strong>Key points about this attachment:</strong></p>
<ul>
<li><strong>Connection type</strong>: ESLAG (EVPN-based multi-homing to leaf-03 and leaf-04)</li>
<li><strong>Subnet</strong>: <code>worker-nodes</code> (DHCPv4 subnet for dynamic IP assignment)</li>
<li><strong>DHCP range</strong>: 10.10.20.10-250 (configured in the VPC subnet)</li>
<li><strong>Result</strong>: VLAN 1020 will be configured on server-05&#39;s ports, DHCP server will offer IPs</li>
</ul>
<p>Apply the VPCAttachment:</p>
<pre><code class=""language-bash"">kubectl apply -f server-05-attachment.yaml
</code></pre>
<p>Expected output:</p>
<pre><code>vpcattachment.vpc.githedgehog.com/server-05-worker-nodes created
</code></pre>
<p>Validate the attachment:</p>
<pre><code class=""language-bash""># List VPCAttachments (should see both now)
kubectl get vpcattachments

# Get detailed information
kubectl describe vpcattachment server-05-worker-nodes
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPCAttachment created successfully</li>
<li>✅ Server-05 will receive DHCP IP from range when booted (10.10.20.10-250)</li>
<li>✅ No error events</li>
</ul>
<h3>Step 4: Validate VPCAttachments</h3>
<p><strong>Objective:</strong> Confirm both attachments are operational</p>
<p>List all VPCAttachments to see the complete picture:</p>
<pre><code class=""language-bash"">kubectl get vpcattachments
</code></pre>
<p>Expected output (similar to):</p>
<pre><code>NAME                       AGE
server-01-web-servers      2m
server-05-worker-nodes     1m
</code></pre>
<p>Describe each attachment to see detailed status:</p>
<pre><code class=""language-bash""># Check server-01 attachment
kubectl describe vpcattachment server-01-web-servers

# Check server-05 attachment
kubectl describe vpcattachment server-05-worker-nodes
</code></pre>
<p>View events for reconciliation progress:</p>
<pre><code class=""language-bash""># All VPCAttachment events
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | grep vpcattachment
</code></pre>
<p>Verify connection types match expectations:</p>
<pre><code class=""language-bash""># Verify server-01 uses MCLAG
kubectl get connection server-01--mclag--leaf-01--leaf-02 -n fab -o yaml | grep -A 5 &quot;mclag:&quot;

# Verify server-05 uses ESLAG
kubectl get connection server-05--eslag--leaf-03--leaf-04 -n fab -o yaml | grep -A 5 &quot;eslag:&quot;
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Both attachments appear in list</li>
<li>✅ Status shows successful reconciliation in events</li>
<li>✅ No warning or error events</li>
<li>✅ Connection types confirmed (MCLAG vs ESLAG)</li>
</ul>
<h3>Step 5: Understand Connection Type Impact</h3>
<p><strong>Objective:</strong> Learn what happens on the switches for different connection types</p>
<p><strong>Server-01 (MCLAG) Configuration:</strong></p>
<p>MCLAG (Multi-Chassis Link Aggregation) provides active-active redundancy:</p>
<ul>
<li><strong>Switches involved</strong>: leaf-01 and leaf-02</li>
<li><strong>Redundancy</strong>: If leaf-01 fails, server-01 stays connected via leaf-02</li>
<li><strong>Technology</strong>: Proprietary MCLAG protocol coordinates both switches</li>
<li><strong>Port configuration</strong>: VLAN 1010 applied to both switch ports</li>
<li><strong>Active-Active</strong>: Both links carry traffic simultaneously</li>
</ul>
<p>View the MCLAG connection details:</p>
<pre><code class=""language-bash"">kubectl get connection server-01--mclag--leaf-01--leaf-02 -n fab -o yaml
</code></pre>
<p><strong>Server-05 (ESLAG) Configuration:</strong></p>
<p>ESLAG (EVPN ESI LAG) provides standards-based multi-homing:</p>
<ul>
<li><strong>Switches involved</strong>: leaf-03 and leaf-04</li>
<li><strong>Redundancy</strong>: If leaf-03 fails, server-05 stays connected via leaf-04</li>
<li><strong>Technology</strong>: EVPN RFC 7432 (standards-based, not proprietary)</li>
<li><strong>Port configuration</strong>: VLAN 1020 applied to both switch ports</li>
<li><strong>Active-Active</strong>: Both links carry traffic</li>
<li><strong>ESI</strong>: Ethernet Segment Identifier coordinates EVPN multi-homing</li>
</ul>
<p>View the ESLAG connection details:</p>
<pre><code class=""language-bash"">kubectl get connection server-05--eslag--leaf-03--leaf-04 -n fab -o yaml
</code></pre>
<p><strong>What happens during VPCAttachment reconciliation:</strong></p>
<ol>
<li><strong>Fabric Controller detects VPCAttachment</strong> - Watches for new/changed VPCAttachment CRDs</li>
<li><strong>Validates references</strong> - Ensures Connection exists and VPC/subnet exists</li>
<li><strong>Identifies switches</strong> - Determines which switches serve this connection</li>
<li><strong>Computes configuration</strong> - Calculates VLAN on server-facing ports, VXLAN tunnel config, BGP EVPN routes</li>
<li><strong>Updates Agent CRDs</strong> - Writes switch configurations to Agent CRD specs</li>
<li><strong>Switch agents apply config</strong> - Each switch agent configures its local switch</li>
<li><strong>Status reported</strong> - Success or errors shown in events</li>
</ol>
<p><strong>From an operator perspective:</strong></p>
<ul>
<li>VPCAttachment workflow is the same regardless of connection type</li>
<li>MCLAG vs ESLAG vs Bundled vs Unbundled differences are handled by the fabric</li>
<li>You simply reference the connection name—the fabric does the rest</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Understand MCLAG provides dual-switch redundancy</li>
<li>✅ Understand ESLAG provides standards-based multi-homing</li>
<li>✅ Can explain what VPCAttachment does on switches</li>
<li>✅ Ready to attach more servers independently</li>
</ul>
<h2>Concepts &amp; Deep Dive</h2>
<h3>The Three-Tier Hierarchy</h3>
<p>Hedgehog infrastructure has three layers that work together:</p>
<p><strong>Layer 1: Wiring (Physical Infrastructure)</strong></p>
<p>This layer defines physical devices and their connections:</p>
<ul>
<li><strong>Switch</strong> - Physical switch in the fabric (e.g., leaf-01, spine-01)</li>
<li><strong>Server</strong> - Physical server (e.g., server-01)</li>
<li><strong>Connection</strong> - Server-to-switch wiring (e.g., server-01--mclag--leaf-01--leaf-02)</li>
</ul>
<p><strong>Layer 2: VPC (Virtual Networks)</strong></p>
<p>This layer defines virtual networks:</p>
<ul>
<li><strong>VPC</strong> - Virtual Private Cloud with one or more subnets</li>
<li><strong>Subnet</strong> - IP address range within a VPC (e.g., web-servers: 10.10.10.0/24)</li>
</ul>
<p><strong>Layer 3: VPCAttachment (Binding Virtual to Physical)</strong></p>
<p>This layer bridges the two:</p>
<ul>
<li><strong>VPCAttachment</strong> - Binds a Connection (physical) to a VPC subnet (virtual)</li>
</ul>
<p><strong>The hierarchy visualized:</strong></p>
<pre><code>Wiring Layer (Physical)
├─ Switch CRD ──────────► leaf-01
├─ Server CRD ──────────► server-01
└─ Connection CRD ──────► server-01--mclag--leaf-01--leaf-02
                              ▼
                    VPCAttachment CRD
                              ▼
VPC Layer (Virtual)
└─ VPC CRD ─────────────► web-app-prod
   └─ Subnet ───────────► web-servers (10.10.10.0/24)
</code></pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Wiring layer is managed by platform team (typically set once)</li>
<li>VPC layer is managed by network operators (create/modify VPCs)</li>
<li>VPCAttachment layer is managed by operators to connect servers to VPCs</li>
<li>This separation enables self-service networking without touching physical wiring</li>
</ul>
<h3>VPCAttachment CRD Deep Dive</h3>
<p><strong>Required Fields:</strong></p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: &lt;attachment-name&gt;              # Descriptive name
  namespace: default                   # Usually default
spec:
  connection: &lt;connection-crd-name&gt;    # Full connection name
  subnet: &lt;vpc-name&gt;/&lt;subnet-name&gt;     # VPC/subnet format
</code></pre>
<p><strong>Field Explanations:</strong></p>
<ul>
<li><strong>metadata.name</strong>: Choose a descriptive name like <code>webapp-frontend-server-01</code></li>
<li><strong>metadata.namespace</strong>: Usually <code>default</code> (matches VPC namespace)</li>
<li><strong>spec.connection</strong>: MUST be the exact Connection CRD name (e.g., <code>server-01--mclag--leaf-01--leaf-02</code>)</li>
<li><strong>spec.subnet</strong>: Format is <code>&lt;vpc-name&gt;/&lt;subnet-name&gt;</code> (e.g., <code>web-app-prod/web-servers</code>)</li>
</ul>
<p><strong>Status Fields:</strong></p>
<p>Like VPCs, VPCAttachment status is minimal. Use events to track reconciliation:</p>
<pre><code class=""language-bash"">kubectl describe vpcattachment &lt;name&gt;
</code></pre>
<p>Look in the Events section for:</p>
<ul>
<li><code>Normal  Created</code> - Resource created</li>
<li><code>Normal  Reconciling</code> - Controller processing</li>
<li><code>Normal  Applied</code> - Configuration applied to switches</li>
<li><code>Warning  Error</code> - Problems during reconciliation</li>
</ul>
<h3>Connection Types and VPCAttachments</h3>
<p>When you create a VPCAttachment, the connection type determines how the fabric configures redundancy and bandwidth. As an operator, you reference the connection name—the fabric handles the complexity.</p>
<h4>MCLAG (Multi-Chassis Link Aggregation)</h4>
<p><strong>What it is:</strong></p>
<p>MCLAG dual-homes a server to two switches using traditional MCLAG protocol.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Redundancy</strong>: Active-active to two switches (e.g., leaf-01 + leaf-02)</li>
<li><strong>Bandwidth</strong>: Aggregated bandwidth across two links</li>
<li><strong>Failure handling</strong>: If one switch fails, server stays connected</li>
<li><strong>Technology</strong>: Proprietary MCLAG protocol</li>
</ul>
<p><strong>Connection CRD Example:</strong></p>
<pre><code class=""language-yaml"">apiVersion: wiring.githedgehog.com/v1beta1
kind: Connection
metadata:
  name: server-01--mclag--leaf-01--leaf-02
  namespace: fab
spec:
  mclag:
    links:
      - server:
          port: server-01/enp2s1
        switch:
          port: leaf-01/E1/5
      - server:
          port: server-01/enp2s2
        switch:
          port: leaf-02/E1/5
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>High-availability servers requiring dual-homed redundancy</li>
<li>Web servers, API servers, databases</li>
<li>Any server where uptime is critical</li>
</ul>
<p><strong>VPCAttachment for MCLAG:</strong></p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-01-web-servers
spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/web-servers
</code></pre>
<p>From your perspective as an operator, the VPCAttachment is simple. The fabric automatically configures both switches.</p>
<h4>ESLAG (Ethernet Segment LAG / EVPN Multi-homing)</h4>
<p><strong>What it is:</strong></p>
<p>ESLAG dual-homes a server to two switches using standards-based EVPN.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Redundancy</strong>: Active-active to two switches (e.g., leaf-03 + leaf-04)</li>
<li><strong>Standards-based</strong>: EVPN RFC 7432 (not proprietary)</li>
<li><strong>Bandwidth</strong>: Aggregated bandwidth across two links</li>
<li><strong>Scalability</strong>: Can extend to more than 2 switches (future capability)</li>
<li><strong>ESI</strong>: Uses Ethernet Segment Identifier for EVPN coordination</li>
</ul>
<p><strong>Connection CRD Example:</strong></p>
<pre><code class=""language-yaml"">apiVersion: wiring.githedgehog.com/v1beta1
kind: Connection
metadata:
  name: server-05--eslag--leaf-03--leaf-04
  namespace: fab
spec:
  eslag:
    links:
      - server:
          port: server-05/enp2s1
        switch:
          port: leaf-03/E1/1
      - server:
          port: server-05/enp2s2
        switch:
          port: leaf-04/E1/1
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Modern multi-homing with standards-based EVPN</li>
<li>Servers requiring redundancy with vendor interoperability</li>
<li>Scalable multi-homing scenarios</li>
</ul>
<p><strong>VPCAttachment for ESLAG:</strong></p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-05-worker-nodes
spec:
  connection: server-05--eslag--leaf-03--leaf-04
  subnet: web-app-prod/worker-nodes
</code></pre>
<p>Like MCLAG, the VPCAttachment syntax is identical—the fabric handles the EVPN complexity.</p>
<h4>Bundled (Port Channel / LAG)</h4>
<p><strong>What it is:</strong></p>
<p>Bundled connections aggregate multiple links to a <strong>single switch</strong> using LACP.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>No switch redundancy</strong>: All links go to one switch</li>
<li><strong>Bandwidth aggregation</strong>: 2x or more link bandwidth</li>
<li><strong>Active-active</strong>: Load balancing across links</li>
<li><strong>Simpler</strong>: No multi-switch coordination</li>
</ul>
<p><strong>Connection CRD Example:</strong></p>
<pre><code class=""language-yaml"">apiVersion: wiring.githedgehog.com/v1beta1
kind: Connection
metadata:
  name: server-10--bundled--leaf-05
  namespace: fab
spec:
  bundled:
    links:
      - server:
          port: server-10/enp2s1
        switch:
          port: leaf-05/E1/2
      - server:
          port: server-10/enp2s2
        switch:
          port: leaf-05/E1/3
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Bandwidth aggregation without switch redundancy requirement</li>
<li>Cost-sensitive deployments</li>
<li>Servers where switch-level HA is not required</li>
</ul>
<p><strong>VPCAttachment for Bundled:</strong></p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-10-workers
spec:
  connection: server-10--bundled--leaf-05
  subnet: web-app-prod/worker-nodes
</code></pre>
<h4>Unbundled (Single Link)</h4>
<p><strong>What it is:</strong></p>
<p>Unbundled connections use a single server port to a single switch port.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Simplest</strong>: One link, one switch</li>
<li><strong>No redundancy</strong>: Switch or link failure disconnects server</li>
<li><strong>No aggregation</strong>: Single link bandwidth</li>
<li><strong>Lowest complexity</strong>: Easiest to configure</li>
</ul>
<p><strong>Connection CRD Example:</strong></p>
<pre><code class=""language-yaml"">apiVersion: wiring.githedgehog.com/v1beta1
kind: Connection
metadata:
  name: server-09--unbundled--leaf-05
  namespace: fab
spec:
  unbundled:
    link:
      server:
        port: server-09/enp2s1
      switch:
        port: leaf-05/E1/1
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Development environments</li>
<li>Non-critical workloads</li>
<li>Cost-sensitive deployments</li>
<li>Testing and prototyping</li>
</ul>
<p><strong>VPCAttachment for Unbundled:</strong></p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-09-database
spec:
  connection: server-09--unbundled--leaf-05
  subnet: web-app-prod/database
</code></pre>
<h4>Connection Type Decision Tree</h4>
<pre><code>Do you need switch redundancy?
├─ YES → Do you have MCLAG or ESLAG pair available?
│        ├─ MCLAG pair (leaf-01/02) → Use MCLAG
│        └─ ESLAG pair (leaf-03/04) → Use ESLAG
│
└─ NO  → Do you need bandwidth aggregation?
         ├─ YES → Use Bundled (port channel)
         └─ NO  → Use Unbundled (single link)
</code></pre>
<h3>What Happens During VPCAttachment Reconciliation</h3>
<p><strong>Step-by-step process:</strong></p>
<ol>
<li><p><strong>Fabric Controller detects VPCAttachment</strong></p>
<ul>
<li>Controller watches for new/changed VPCAttachment CRDs</li>
<li>Detects your <code>kubectl apply</code> or GitOps sync</li>
</ul>
</li>
<li><p><strong>Validates references</strong></p>
<ul>
<li>Does the Connection exist? (<code>server-01--mclag--leaf-01--leaf-02</code>)</li>
<li>Does the VPC and subnet exist? (<code>web-app-prod/web-servers</code>)</li>
</ul>
</li>
<li><p><strong>Identifies switches</strong></p>
<ul>
<li>Which switches serve this connection?</li>
<li>For MCLAG: leaf-01 and leaf-02</li>
<li>For ESLAG: leaf-03 and leaf-04</li>
</ul>
</li>
<li><p><strong>Computes configuration</strong></p>
<ul>
<li>VLAN on server-facing ports (e.g., VLAN 1010)</li>
<li>VXLAN tunnel configuration</li>
<li>BGP EVPN routes for VPC connectivity</li>
<li>DHCP relay configuration (if DHCPv4 subnet)</li>
</ul>
</li>
<li><p><strong>Updates Agent CRDs</strong></p>
<ul>
<li>Controller writes switch configurations to Agent CRD specs</li>
<li>Each switch has its own Agent CRD</li>
</ul>
</li>
<li><p><strong>Switch agents apply config</strong></p>
<ul>
<li>Switch agents watch their Agent CRD</li>
<li>Detect spec changes</li>
<li>Apply configuration to physical switch via gNMI</li>
<li>Configure ports, VLANs, VXLAN, BGP</li>
</ul>
</li>
<li><p><strong>Status reported</strong></p>
<ul>
<li>Success or errors shown in VPCAttachment events</li>
<li>Agent CRD status shows applied configuration</li>
</ul>
</li>
</ol>
<p><strong>Timeline:</strong></p>
<ul>
<li>VPCAttachment creation: &lt; 1 second (Kubernetes write)</li>
<li>Reconciliation: 5-15 seconds (depends on fabric size)</li>
<li>Configuration application: 10-30 seconds (switch agents apply config)</li>
<li>Full convergence: 30-60 seconds (BGP EVPN convergence)</li>
</ul>
<h3>Static IP Assignment on VPCAttachments</h3>
<p>For IPv4 subnets (without DHCP), you can optionally specify a static IP in the VPCAttachment:</p>
<pre><code class=""language-yaml"">apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-01-web-servers
spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/web-servers
  staticAddress: 10.10.10.10  # Optional static IP assignment
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Servers requiring fixed IPs (databases, load balancer backends)</li>
<li>IP address management tracked in Hedgehog</li>
<li>DNS A records pointing to specific IPs</li>
</ul>
<p><strong>If omitted:</strong></p>
<p>Server uses its own network configuration (manual IP assignment in server OS).</p>
<p><strong>Note:</strong></p>
<p>For DHCPv4 subnets, omit <code>staticAddress</code>—the DHCP server will assign IPs dynamically.</p>
<h3>Multiple Attachments Per Server</h3>
<p>A server can attach to multiple VPCs or multiple subnets within the same VPC:</p>
<p><strong>Example: Server in two subnets</strong></p>
<pre><code class=""language-yaml""># Attachment 1: server-01 to web-servers subnet
---
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-01-web-servers
spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/web-servers
---
# Attachment 2: server-01 to worker-nodes subnet
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPCAttachment
metadata:
  name: server-01-workers
spec:
  connection: server-01--mclag--leaf-01--leaf-02
  subnet: web-app-prod/worker-nodes
</code></pre>
<p><strong>Result:</strong></p>
<p>Server-01 has access to both VLANs (1010 and 1020).</p>
<p><strong>Use cases:</strong></p>
<ul>
<li>Management network + production network</li>
<li>Multiple application tiers on same server</li>
<li>Separate storage network</li>
<li>Development + testing networks</li>
</ul>
<p><strong>Server NIC configuration:</strong></p>
<p>When using multiple attachments, configure VLAN subinterfaces on the server OS:</p>
<pre><code class=""language-bash""># Example on Linux server
# eth0.1010 for web-servers VLAN
# eth0.1020 for worker-nodes VLAN
</code></pre>
<h2>Troubleshooting</h2>
<h3>Issue: VPCAttachment fails with &quot;Connection not found&quot;</h3>
<p><strong>Symptom:</strong> Error during apply: <code>Connection &quot;server-01&quot; not found</code></p>
<p><strong>Cause:</strong> Connection name doesn&#39;t match any Connection CRD</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># List all connections to find the correct name
kubectl get connections -n fab | grep server-01

# Expected output:
# server-01--mclag--leaf-01--leaf-02   2h

# Update your VPCAttachment YAML with exact connection name
# Wrong:
#   connection: server-01
# Correct:
#   connection: server-01--mclag--leaf-01--leaf-02

# Reapply:
kubectl apply -f server-01-attachment.yaml
</code></pre>
<h3>Issue: VPCAttachment fails with &quot;Subnet not found&quot;</h3>
<p><strong>Symptom:</strong> Error during apply: <code>Subnet &quot;web-servers&quot; not found in VPC &quot;web-app-prod&quot;</code></p>
<p><strong>Cause:</strong> VPC/subnet format incorrect or subnet doesn&#39;t exist in VPC</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Verify VPC exists
kubectl get vpc web-app-prod

# Check subnet names in VPC
kubectl get vpc web-app-prod -o yaml | grep -A 2 &quot;subnets:&quot;

# Expected output shows subnet names:
#   subnets:
#     web-servers:
#       subnet: 10.10.10.0/24
#     worker-nodes:
#       subnet: 10.10.20.0/24

# Ensure VPCAttachment uses correct format:
# Format: &lt;vpc-name&gt;/&lt;subnet-name&gt;
# Correct: web-app-prod/web-servers
# Wrong: web-servers (missing VPC name)

# Update YAML and reapply
kubectl apply -f server-01-attachment.yaml
</code></pre>
<h3>Issue: VPCAttachment created but server has no connectivity</h3>
<p><strong>Symptom:</strong> VPCAttachment shows no errors, but server can&#39;t reach network</p>
<p><strong>Cause:</strong> Server OS network configuration not set, or wrong VLAN on server NIC</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Verify VPCAttachment reconciled successfully
kubectl describe vpcattachment server-01-web-servers

# Look for:
# Events:
#   Normal  Applied  Configuration applied to leaf-01, leaf-02

# Step 2: Check server OS network configuration
# SSH to server and verify interface config

# For static IP subnet (no DHCP):
# Manually configure IP on server
# Example on Linux:
sudo ip addr add 10.10.10.10/24 dev eth0
sudo ip route add default via 10.10.10.1

# For DHCP subnet:
# Verify DHCP client is running
sudo systemctl status dhclient
# Or request DHCP:
sudo dhclient eth0

# Step 3: Verify VLAN configuration (if server expects tagged VLAN)
# Check if server NIC is configured for VLAN tagging
# If VPCAttachment has nativeVLAN: false, server needs VLAN subinterface
</code></pre>
<h3>Issue: DHCP not working for server on DHCPv4 subnet</h3>
<p><strong>Symptom:</strong> Server doesn&#39;t get DHCP IP from worker-nodes subnet</p>
<p><strong>Cause:</strong> DHCP client not running on server, or server NIC not requesting DHCP</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: Verify VPCAttachment to DHCPv4 subnet
kubectl get vpcattachment server-05-worker-nodes -o yaml

# Verify subnet has DHCP enabled:
kubectl get vpc web-app-prod -o jsonpath=&#39;{.spec.subnets.worker-nodes.dhcp}&#39; | jq

# Expected output:
# {
#   &quot;enable&quot;: true,
#   &quot;range&quot;: {
#     &quot;start&quot;: &quot;10.10.20.10&quot;,
#     &quot;end&quot;: &quot;10.10.20.250&quot;
#   }
# }

# Step 2: Check DHCP client on server
# SSH to server
sudo systemctl status dhclient  # or NetworkManager, dhcpcd

# Start DHCP client if not running:
sudo dhclient eth0

# Step 3: Check for DHCP range exhaustion
# If range is full, expand DHCP range in VPC:
kubectl edit vpc web-app-prod
# Modify dhcp.range.end to larger value
</code></pre>
<h3>Issue: Multiple VPCAttachments to same server causing conflicts</h3>
<p><strong>Symptom:</strong> Server connectivity intermittent, some VLANs work, others don&#39;t</p>
<p><strong>Cause:</strong> Server OS not configured for multiple VLANs/subnets</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Step 1: List all VPCAttachments for the server
kubectl get vpcattachments | grep server-01

# Example output:
# server-01-web-servers     5m
# server-01-workers         3m

# Step 2: Identify VLANs for each attachment
kubectl get vpc web-app-prod -o yaml | grep vlan

# Example output:
# web-servers: vlan: 1010
# worker-nodes: vlan: 1020

# Step 3: Configure VLAN subinterfaces on server
# SSH to server and create VLAN interfaces
# Example on Linux:
sudo ip link add link eth0 name eth0.1010 type vlan id 1010
sudo ip link add link eth0 name eth0.1020 type vlan id 1020
sudo ip link set eth0.1010 up
sudo ip link set eth0.1020 up

# Step 4: Assign IPs to VLAN interfaces
sudo ip addr add 10.10.10.10/24 dev eth0.1010  # web-servers
sudo dhclient eth0.1020                         # worker-nodes (DHCP)
</code></pre>
<h3>Issue: &quot;Namespace mismatch&quot; error</h3>
<p><strong>Symptom:</strong> Error during apply: <code>VPCAttachment namespace doesn&#39;t match VPC namespace</code></p>
<p><strong>Cause:</strong> VPCAttachment and VPC are in different Kubernetes namespaces</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check VPC namespace
kubectl get vpc web-app-prod -A

# Example output:
# NAMESPACE   NAME           AGE
# default     web-app-prod   1h

# Ensure VPCAttachment uses same namespace
# Edit VPCAttachment YAML:
# metadata:
#   namespace: default  # Must match VPC namespace

# Reapply:
kubectl apply -f server-01-attachment.yaml
</code></pre>
<h2>Resources</h2>
<h3>Hedgehog CRDs</h3>
<p><strong>VPCAttachment</strong> - Binds server connection to VPC subnet</p>
<ul>
<li>View all: <code>kubectl get vpcattachments</code></li>
<li>View specific: <code>kubectl get vpcattachment &lt;name&gt;</code></li>
<li>Inspect: <code>kubectl describe vpcattachment &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpcattachment &lt;name&gt; -o yaml</code></li>
<li>Delete: <code>kubectl delete vpcattachment &lt;name&gt;</code></li>
</ul>
<p><strong>Connection</strong> - Server-to-switch wiring (from Module 1.1)</p>
<ul>
<li>View all: <code>kubectl get connections -n fab</code></li>
<li>View server connections: <code>kubectl get connections -n fab | grep server-</code></li>
<li>View specific: <code>kubectl get connection &lt;name&gt; -n fab -o yaml</code></li>
</ul>
<p><strong>VPC</strong> - Virtual network (from Module 2.1)</p>
<ul>
<li>View all: <code>kubectl get vpcs</code></li>
<li>View specific: <code>kubectl get vpc &lt;name&gt;</code></li>
<li>Inspect: <code>kubectl describe vpc &lt;name&gt;</code></li>
</ul>
<p><strong>Server</strong> - Physical server definition</p>
<ul>
<li>View all: <code>kubectl get servers -n fab</code></li>
<li>View specific: <code>kubectl get server &lt;name&gt; -n fab -o yaml</code></li>
</ul>
<h3>kubectl Commands</h3>
<p><strong>VPCAttachment lifecycle:</strong></p>
<pre><code class=""language-bash""># Create VPCAttachment
kubectl apply -f attachment.yaml

# List all VPCAttachments
kubectl get vpcattachments

# List in all namespaces
kubectl get vpcattachments -A

# Get VPCAttachment details
kubectl describe vpcattachment &lt;name&gt;

# View as YAML
kubectl get vpcattachment &lt;name&gt; -o yaml

# View events for specific attachment
kubectl get events --field-selector involvedObject.name=&lt;name&gt;

# Delete VPCAttachment
kubectl delete vpcattachment &lt;name&gt;

# Delete from file
kubectl delete -f attachment.yaml
</code></pre>
<p><strong>Discovery and validation:</strong></p>
<pre><code class=""language-bash""># List servers
kubectl get servers -n fab

# List connections
kubectl get connections -n fab

# Filter for server connections
kubectl get connections -n fab | grep server-

# Get connection details
kubectl get connection &lt;connection-name&gt; -n fab -o yaml

# Verify VPC and subnets
kubectl get vpc &lt;vpc-name&gt;
kubectl get vpc &lt;vpc-name&gt; -o yaml | grep -A 20 &quot;subnets:&quot;

# Check DHCP configuration
kubectl get vpc &lt;vpc-name&gt; -o jsonpath=&#39;{.spec.subnets.&lt;subnet-name&gt;.dhcp}&#39; | jq
</code></pre>
<p><strong>Event monitoring:</strong></p>
<pre><code class=""language-bash""># View all VPCAttachment events
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | grep vpcattachment

# View events for specific attachment
kubectl get events --field-selector involvedObject.name=&lt;attachment-name&gt;

# Watch events in real-time
kubectl get events --watch --field-selector involvedObject.name=&lt;attachment-name&gt;
</code></pre>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""./module-2.1-vpc-provisioning.md"">Module 2.1: VPC Provisioning Essentials</a></li>
<li>Next: Module 2.3: Day 1 Operations (coming soon)</li>
<li>Module 1.1: <a href=""../course-1-foundations/fabric-operations-welcome/README.md"">Welcome to Fabric Operations</a> (for Server/Connection review)</li>
</ul>
<h3>External Documentation</h3>
<ul>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog VPCAttachment Documentation</a></li>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog Fabric Architecture</a></li>
<li><a href=""https://datatracker.ietf.org/doc/html/rfc7432"">EVPN VXLAN Overview</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Multi-chassis_link_aggregation_group"">Understanding MCLAG</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve successfully learned VPCAttachment provisioning and connected servers to VPCs. In Module 2.3, you&#39;ll learn Day 1 operations for managing VPCs and attachments.</p>
",202,25,,,"hedgehog,fabric,vpc,attachments,provisioning,operations"
198578604071,VPC Provisioning Essentials,fabric-operations-vpc-provisioning,[object Object],Master VPC provisioning with hands-on creation of IPv4 and DHCPv4 subnets. Learn to validate configurations and troubleshoot common issues.,"<h2>Introduction</h2>
<p>In Course 1, you explored Hedgehog fabric as a read-only observer. Now you&#39;re ready to <strong>provision real infrastructure</strong>. In this module, you&#39;ll create your first production-ready VPC with multiple subnets, learning the complete workflow from YAML creation to validation. This is where Fabric Operators spend most of their time—provisioning network resources safely and confidently using declarative infrastructure.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Provision a VPC</strong> - Create a VPC using kubectl apply with YAML manifest</li>
<li><strong>Distinguish subnet types</strong> - Explain the difference between IPv4 (static) and DHCPv4 subnets</li>
<li><strong>Validate VPC creation</strong> - Use kubectl get/describe to confirm VPC status</li>
<li><strong>Interpret VPC events</strong> - Read Kubernetes events to track reconciliation</li>
<li><strong>Troubleshoot common issues</strong> - Diagnose and fix VPC provisioning problems</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Course 1 completion (Foundations &amp; Interfaces)</li>
<li>kubectl access to Hedgehog fabric</li>
<li>Basic YAML editing skills</li>
<li>Understanding of IP subnets and CIDR notation</li>
</ul>
<h2>Scenario: Onboarding a New Application</h2>
<p>Your company is deploying a new application called &quot;web-app-prod&quot; that requires network isolation. The application has two tiers: web servers that need static IP assignments (so you can configure load balancers), and worker nodes that can use DHCP (since they scale dynamically). You&#39;ll create a VPC with two subnets—one IPv4 subnet for static assignments and one DHCPv4 subnet for dynamic assignments—providing complete network segmentation for this workload.</p>
<h2>Lab Steps</h2>
<h3>Step 1: Create VPC YAML Manifest</h3>
<p><strong>Objective:</strong> Define a VPC with two subnets (one IPv4, one DHCPv4)</p>
<p>Create the VPC manifest file:</p>
<pre><code class=""language-bash"">cat &gt; web-app-prod-vpc.yaml &lt;&lt;&#39;EOF&#39;
apiVersion: vpc.githedgehog.com/v1beta1
kind: VPC
metadata:
  name: web-app-prod
  namespace: default
spec:
  ipv4Namespace: default
  vlanNamespace: default
  subnets:
    web-servers:
      subnet: 10.10.10.0/24
      gateway: 10.10.10.1
      vlan: 1010
    worker-nodes:
      subnet: 10.10.20.0/24
      gateway: 10.10.20.1
      vlan: 1020
      dhcp:
        enable: true
        range:
          start: 10.10.20.10
          end: 10.10.20.250
EOF
</code></pre>
<p><strong>Understanding the YAML:</strong></p>
<ul>
<li><strong>apiVersion/kind:</strong> Defines this as a VPC resource</li>
<li><strong>metadata.name:</strong> Unique identifier for this VPC (&quot;web-app-prod&quot;)</li>
<li><strong>ipv4Namespace/vlanNamespace:</strong> Groups for managing IP and VLAN allocations (use &quot;default&quot; for most cases)</li>
<li><strong>subnets:</strong> Dictionary of subnet definitions<ul>
<li><strong>web-servers:</strong> IPv4 subnet (no DHCP = static IPs only)<ul>
<li>10.10.10.0/24 provides 254 usable IPs (.1-.254)</li>
<li>Gateway at .1 (first usable IP)</li>
<li>VLAN 1010 for layer 2 isolation</li>
</ul>
</li>
<li><strong>worker-nodes:</strong> DHCPv4 subnet (DHCP enabled)<ul>
<li>10.10.20.0/24 subnet</li>
<li>DHCP range: .10-.250 (240 IPs available)</li>
<li>Gateway at .1 (outside DHCP range)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Validation:</strong></p>
<pre><code class=""language-bash""># Verify YAML syntax (should show no errors)
cat web-app-prod-vpc.yaml | grep -E &#39;(apiVersion|kind|name|subnet|vlan)&#39;

# Count subnets (should be 2)
grep -c &quot;subnet:&quot; web-app-prod-vpc.yaml
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ YAML file created with correct syntax</li>
<li>✅ Two subnets defined (web-servers and worker-nodes)</li>
<li>✅ One subnet has DHCP enabled, one does not</li>
</ul>
<h3>Step 2: Apply VPC Configuration</h3>
<p><strong>Objective:</strong> Submit the VPC to the fabric and verify creation</p>
<p>Apply the VPC manifest:</p>
<pre><code class=""language-bash"">kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>vpc.vpc.githedgehog.com/web-app-prod created
</code></pre>
<p><strong>Immediately verify creation:</strong></p>
<pre><code class=""language-bash""># Check VPC appears in list
kubectl get vpcs

# Expected output (similar to):
# NAME            AGE
# web-app-prod    5s
</code></pre>
<p><strong>Check for errors:</strong></p>
<pre><code class=""language-bash""># View recent events for this VPC
kubectl get events --field-selector involvedObject.name=web-app-prod --sort-by=&#39;.lastTimestamp&#39;

# Look for:
# - Normal events: Created, Reconciling, Ready
# - Warning events: Configuration errors (if any)
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC appears in <code>kubectl get vpcs</code> output</li>
<li>✅ No error messages during apply</li>
<li>✅ Events show &quot;Created&quot; or &quot;Reconciling&quot; (not warnings)</li>
</ul>
<h3>Step 3: Inspect VPC Status</h3>
<p><strong>Objective:</strong> View detailed VPC information and confirm readiness</p>
<p>Use kubectl describe to see comprehensive VPC details:</p>
<pre><code class=""language-bash"">kubectl describe vpc web-app-prod
</code></pre>
<p><strong>Key sections to review:</strong></p>
<p><strong>1. Metadata section:</strong></p>
<pre><code>Name:         web-app-prod
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  vpc.githedgehog.com/v1beta1
Kind:         VPC
</code></pre>
<p><strong>2. Spec section (what you requested):</strong></p>
<pre><code>Spec:
  Ipv4 Namespace:  default
  Vlan Namespace:  default
  Subnets:
    web-servers:
      Subnet:   10.10.10.0/24
      Gateway:  10.10.10.1
      Vlan:     1010
    worker-nodes:
      Subnet:   10.10.20.0/24
      Gateway:  10.10.20.1
      Vlan:     1020
      Dhcp:
        Enable: true
        Range:
          Start: 10.10.20.10
          End:   10.10.20.250
</code></pre>
<p><strong>3. Events section (at the bottom):</strong></p>
<pre><code>Events:
  Type    Reason      Age   From              Message
  ----    ------      ----  ----              -------
  Normal  Created     30s   fabric-controller VPC web-app-prod created
  Normal  Reconciling 25s   fabric-controller Processing VPC configuration
</code></pre>
<p><strong>Verification checklist:</strong></p>
<pre><code class=""language-bash""># Get VPC in YAML format to see status
kubectl get vpc web-app-prod -o yaml | grep -A 10 &quot;status:&quot;

# Note: VPC status may be minimal or empty - that&#39;s normal!
# Events tell the real story of what happened
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ VPC shows both subnets with correct CIDR blocks</li>
<li>✅ VLANs assigned (1010 and 1020)</li>
<li>✅ DHCP range visible for worker-nodes subnet</li>
<li>✅ No error events in Events section</li>
</ul>
<h3>Step 4: View VPC Events for Reconciliation</h3>
<p><strong>Objective:</strong> Understand the reconciliation timeline</p>
<p>View events in chronological order:</p>
<pre><code class=""language-bash"">kubectl get events --field-selector involvedObject.name=web-app-prod --sort-by=&#39;.lastTimestamp&#39;
</code></pre>
<p><strong>Example event timeline:</strong></p>
<pre><code>LAST SEEN   TYPE     REASON          OBJECT                MESSAGE
2m          Normal   Created         vpc/web-app-prod      VPC created
2m          Normal   Reconciling     vpc/web-app-prod      Processing VPC configuration
1m          Normal   AgentUpdate     vpc/web-app-prod      Updated agent specs for switches
1m          Normal   Ready           vpc/web-app-prod      VPC reconciliation complete
</code></pre>
<p><strong>Understanding reconciliation flow:</strong></p>
<ol>
<li><strong>Created:</strong> VPC object stored in Kubernetes</li>
<li><strong>Reconciling:</strong> Fabric Controller picked up the change</li>
<li><strong>AgentUpdate:</strong> Switch Agent CRDs updated with configuration</li>
<li><strong>Ready:</strong> Switch agents applied configuration successfully</li>
</ol>
<p><strong>Viewing events continuously (optional):</strong></p>
<pre><code class=""language-bash""># Watch events in real-time (useful when applying changes)
kubectl get events --watch --field-selector involvedObject.name=web-app-prod

# Press Ctrl+C to exit watch mode
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Can view VPC-specific events</li>
<li>✅ Events show reconciliation progression</li>
<li>✅ Understand event timeline from creation to ready</li>
</ul>
<h3>Step 5: Validate Configuration Matches Desired State</h3>
<p><strong>Objective:</strong> Confirm actual state matches what you requested</p>
<p>Compare your YAML to the applied configuration:</p>
<pre><code class=""language-bash""># View original YAML spec
cat web-app-prod-vpc.yaml | grep -A 20 &quot;subnets:&quot;

# View applied configuration
kubectl get vpc web-app-prod -o yaml | grep -A 20 &quot;subnets:&quot;

# They should match exactly
</code></pre>
<p><strong>Verify VPC is ready for server attachments:</strong></p>
<pre><code class=""language-bash""># Check that VPC exists and has no errors
kubectl get vpc web-app-prod

# VPC should appear with no error indicators
# (Status field may be empty - that&#39;s normal)
</code></pre>
<p><strong>Review DHCP configuration:</strong></p>
<pre><code class=""language-bash""># Inspect DHCP settings for worker-nodes subnet
kubectl get vpc web-app-prod -o jsonpath=&#39;{.spec.subnets.worker-nodes.dhcp}&#39; | jq

# Expected output (similar to):
# {
#   &quot;enable&quot;: true,
#   &quot;range&quot;: {
#     &quot;start&quot;: &quot;10.10.20.10&quot;,
#     &quot;end&quot;: &quot;10.10.20.250&quot;
#   }
# }
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Applied config matches desired state in YAML</li>
<li>✅ VPC appears stable (no errors when queried)</li>
<li>✅ DHCP range configured correctly</li>
<li>✅ VPC ready for VPCAttachment operations (next module)</li>
</ul>
<h3>Lab Summary</h3>
<p><strong>What you accomplished:</strong></p>
<ul>
<li>✅ Created production-ready VPC manifest with two subnets</li>
<li>✅ Applied VPC configuration using kubectl</li>
<li>✅ Validated VPC status and configuration</li>
<li>✅ Observed reconciliation events</li>
<li>✅ Confirmed desired state matches actual state</li>
</ul>
<p><strong>What you learned:</strong></p>
<ul>
<li>IPv4 subnets (no DHCP) are for static IP assignments</li>
<li>DHCPv4 subnets (DHCP enabled) provide dynamic IP allocation</li>
<li>VPC reconciliation happens automatically via the control loop</li>
<li>Events provide visibility into the provisioning process</li>
<li>kubectl is your primary tool for VPC lifecycle management</li>
</ul>
<p><strong>Next steps:</strong></p>
<p>In Module 2.2, you&#39;ll learn to attach servers to this VPC using VPCAttachments, completing the connectivity workflow.</p>
<h2>Concepts &amp; Deep Dive</h2>
<h3>VPC Architecture in Hedgehog</h3>
<p>A VPC (Virtual Private Cloud) provides network isolation using VXLAN overlays and VLAN segmentation:</p>
<p><strong>What is a VPC?</strong></p>
<ul>
<li>Logical network boundary isolating workloads</li>
<li>Contains one or more subnets (layer 3 networks)</li>
<li>Maps to VXLANs for fabric-wide isolation</li>
<li>Uses VLANs for server-facing ports</li>
</ul>
<p><strong>VPC vs. Traditional VLAN Management:</strong></p>
<table>
<thead>
<tr>
<th>Traditional VLANs</th>
<th>Hedgehog VPCs</th>
</tr>
</thead>
<tbody><tr>
<td>Manual VLAN provisioning per switch</td>
<td>Declare VPC once, fabric handles VLAN allocation</td>
</tr>
<tr>
<td>Switch-by-switch configuration</td>
<td>Kubernetes-native YAML definition</td>
</tr>
<tr>
<td>Spreadsheet tracking</td>
<td>kubectl get vpcs</td>
</tr>
<tr>
<td>Configuration drift risk</td>
<td>Declarative, self-healing</td>
</tr>
<tr>
<td>Limited scalability (4096 VLANs)</td>
<td>VXLAN provides 16M+ VNIs</td>
</tr>
</tbody></table>
<p><strong>VPC Isolation Mechanisms:</strong></p>
<ol>
<li><strong>VXLAN (fabric core):</strong> VNI (VXLAN Network Identifier) isolates traffic between spine and leaf switches</li>
<li><strong>VLAN (server access):</strong> Traditional VLANs tag traffic on server-facing ports</li>
<li><strong>VRF (routing):</strong> Each VPC has isolated routing table on switches</li>
</ol>
<h3>IPv4 vs DHCPv4 Subnets</h3>
<p><strong>IPv4 Subnet (Static):</strong></p>
<pre><code class=""language-yaml"">web-servers:
  subnet: 10.10.10.0/24
  gateway: 10.10.10.1
  vlan: 1010
  # No DHCP section = static IP assignment only
</code></pre>
<p><strong>Use cases:</strong></p>
<ul>
<li>Database servers (fixed IPs for connection strings)</li>
<li>Load balancer backends (consistent targets)</li>
<li>Servers you manage with tools like Ansible</li>
<li>Services requiring DNS A records</li>
</ul>
<p><strong>IP management:</strong> You assign and track IPs manually (IP Address Management tools help at scale)</p>
<hr>
<p><strong>DHCPv4 Subnet (Dynamic):</strong></p>
<pre><code class=""language-yaml"">worker-nodes:
  subnet: 10.10.20.0/24
  gateway: 10.10.20.1
  vlan: 1020
  dhcp:
    enable: true
    range:
      start: 10.10.20.10
      end: 10.10.20.250
</code></pre>
<p><strong>Use cases:</strong></p>
<ul>
<li>Kubernetes worker nodes (ephemeral, autoscaling)</li>
<li>Compute instances in cloud-like environments</li>
<li>Testing/development servers (temporary)</li>
<li>Any workload where IP doesn&#39;t need to be stable</li>
</ul>
<p><strong>IP management:</strong> Hedgehog DHCP server automatically assigns IPs from the range</p>
<p><strong>DHCP Range Considerations:</strong></p>
<ul>
<li>Reserve space outside the range for static assignments (e.g., .1-.9)</li>
<li>Size range based on maximum expected servers</li>
<li>Example: /24 subnet = 254 usable IPs<ul>
<li>Gateway: .1</li>
<li>Static reservations: .2-.9 (8 IPs)</li>
<li>DHCP range: .10-.250 (241 IPs)</li>
<li>Broadcast: .255</li>
</ul>
</li>
</ul>
<p><strong>Can you mix static and DHCP in one subnet?</strong></p>
<p>Yes! Even with DHCP enabled, you can assign static IPs <strong>outside</strong> the DHCP range:</p>
<pre><code class=""language-yaml"">subnet: 10.10.20.0/24
gateway: 10.10.20.1
dhcp:
  enable: true
  range:
    start: 10.10.20.100  # DHCP starts at .100
    end: 10.10.20.250    # DHCP ends at .250
# You can statically assign 10.10.20.2 - 10.10.20.99
</code></pre>
<h3>VPC CRD Deep Dive</h3>
<p><strong>Key Spec Fields:</strong></p>
<pre><code class=""language-yaml"">spec:
  ipv4Namespace: default        # Groups VPCs for IP allocation management
  vlanNamespace: default        # Groups VPCs for VLAN allocation management
  subnets:
    &lt;subnet-name&gt;:              # User-defined name (descriptive)
      subnet: 10.0.0.0/24       # CIDR block
      gateway: 10.0.0.1         # Default gateway IP
      vlan: 1000                # VLAN ID (manual or auto-assigned)
      dhcp:                     # Optional DHCP configuration
        enable: true
        range:
          start: 10.0.0.10
          end: 10.0.0.250
        pxeURL: http://...      # Optional PXE boot server
</code></pre>
<p><strong>Status Fields:</strong></p>
<p>VPC status is intentionally minimal. Most operational state lives in Agent CRDs (as you learned in Module 1.2).</p>
<pre><code class=""language-yaml"">status:
  # Status fields are minimal or empty
  # Use events and Agent CRDs for detailed state
</code></pre>
<p><strong>Why minimal status?</strong></p>
<ul>
<li>VPC is declarative intent (what you want)</li>
<li>Agent CRDs contain switch operational state (what exists)</li>
<li>Events show reconciliation progress (what happened)</li>
<li>This separation keeps VPC definition clean and focused</li>
</ul>
<p><strong>Auto-assigned VLANs:</strong></p>
<p>If you omit <code>vlan: &lt;id&gt;</code>, Hedgehog auto-assigns from the VLAN namespace pool:</p>
<pre><code class=""language-yaml"">web-servers:
  subnet: 10.10.10.0/24
  gateway: 10.10.10.1
  # vlan: omitted - will be auto-assigned
</code></pre>
<p><strong>When to use auto-assignment:</strong></p>
<ul>
<li>Rapid prototyping (don&#39;t care about specific VLAN IDs)</li>
<li>Avoiding VLAN conflicts in large fabrics</li>
<li>Letting the system manage allocation</li>
</ul>
<p><strong>When to manually specify:</strong></p>
<ul>
<li>Integrating with existing networks (must match specific VLANs)</li>
<li>Regulatory compliance (VLAN-based auditing)</li>
<li>Troubleshooting with external tools that expect specific VLANs</li>
</ul>
<h3>Reconciliation for VPCs</h3>
<p><strong>What happens when you <code>kubectl apply -f vpc.yaml</code>?</strong></p>
<p><strong>Step-by-step reconciliation:</strong></p>
<ol>
<li><strong>Kubernetes API Server:</strong> Stores VPC CRD in etcd</li>
<li><strong>Fabric Controller Watch:</strong> Detects new/changed VPC</li>
<li><strong>Validation:</strong> Controller validates spec (CIDR, VLAN ranges, namespace membership)</li>
<li><strong>Compute Configuration:</strong> Controller determines:<ul>
<li>Which switches need this VPC configuration</li>
<li>VXLAN VNI assignment</li>
<li>VLAN-to-VXLAN mappings</li>
<li>BGP EVPN route targets</li>
</ul>
</li>
<li><strong>Agent CRD Update:</strong> Controller writes switch configurations to Agent CRD specs</li>
<li><strong>Switch Agent Detection:</strong> Each switch agent watches its Agent CRD</li>
<li><strong>Configuration Application:</strong> Switch agents apply config via gNMI to SONiC</li>
<li><strong>Status Reporting:</strong> Switch agents report back to Agent CRD status</li>
<li><strong>Event Generation:</strong> Controller emits events showing progress</li>
</ol>
<p><strong>Timeline:</strong></p>
<ul>
<li>VPC creation: &lt; 1 second (Kubernetes write)</li>
<li>Reconciliation: 5-15 seconds (depends on fabric size)</li>
<li>Full propagation: 30-60 seconds (BGP convergence)</li>
</ul>
<p><strong>When is a VPC &quot;Ready&quot;?</strong></p>
<p>A VPC is ready when:</p>
<ul>
<li>✅ No error events in <code>kubectl get events</code></li>
<li>✅ VPC can be queried without errors</li>
<li>✅ Events show &quot;Ready&quot; or reconciliation completion</li>
<li>✅ Relevant switches have updated Agent CRD status</li>
</ul>
<p><strong>Note:</strong> VPC status field may be empty even when ready. Trust the events!</p>
<h3>VPC Naming Best Practices</h3>
<p><strong>Good VPC names:</strong></p>
<ul>
<li><code>web-app-prod</code> - Application name + environment</li>
<li><code>ml-training-dev</code> - Workload type + environment</li>
<li><code>data-plane-1</code> - Function + identifier</li>
<li><code>tenant-acme-prod</code> - Multi-tenant pattern</li>
</ul>
<p><strong>Avoid:</strong></p>
<ul>
<li><code>vpc-1</code> - Not descriptive</li>
<li><code>test</code> - Too generic</li>
<li><code>johns-vpc</code> - Owner names (people change roles)</li>
<li><code>10.10.10.0</code> - Using IP as name (IPs may change)</li>
</ul>
<p><strong>Subnet naming:</strong></p>
<ul>
<li>Descriptive: <code>web-servers</code>, <code>worker-nodes</code>, <code>database-tier</code></li>
<li>Functional: <code>frontend</code>, <code>backend</code>, <code>storage</code></li>
<li>By role: <code>compute</code>, <code>control-plane</code>, <code>ingress</code></li>
</ul>
<p><strong>Namespace strategy:</strong></p>
<ul>
<li><strong>Single namespace:</strong> Most deployments use <code>ipv4Namespace: default</code> and <code>vlanNamespace: default</code></li>
<li><strong>Multi-namespace:</strong> For large organizations needing isolation:<ul>
<li>By team: <code>ipv4Namespace: team-network</code>, <code>vlanNamespace: team-network</code></li>
<li>By environment: <code>ipv4Namespace: production</code>, <code>vlanNamespace: production</code></li>
<li>By region: <code>ipv4Namespace: us-west</code>, <code>vlanNamespace: us-west</code></li>
</ul>
</li>
</ul>
<h2>Troubleshooting</h2>
<h3>Issue: VPC stuck in &quot;Pending&quot; state</h3>
<p><strong>Symptom:</strong> <code>kubectl get vpc</code> shows VPC but events show warnings</p>
<p><strong>Cause:</strong> Configuration validation error or VLAN conflict</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check events for specific error
kubectl describe vpc web-app-prod

# Look for error messages like:
# - &quot;VLAN 1010 already in use&quot;
# - &quot;Subnet overlaps with existing VPC&quot;
# - &quot;Invalid CIDR notation&quot;

# If VLAN conflict, change VLAN ID in YAML
# If subnet overlap, choose different subnet range
# Then reapply:
kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<h3>Issue: Subnet CIDR overlap error</h3>
<p><strong>Symptom:</strong> Error during apply: &quot;subnet overlaps with existing VPC&quot;</p>
<p><strong>Cause:</strong> Another VPC already uses overlapping IP space</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># List all VPCs and their subnets
kubectl get vpcs -o yaml | grep -E &quot;(name:|subnet:)&quot;

# Example output:
#   name: existing-vpc
#     subnet: 10.10.0.0/16
#   name: web-app-prod
#     subnet: 10.10.10.0/24  # Overlaps with 10.10.0.0/16!

# Solution: Choose non-overlapping subnet
# Edit web-app-prod-vpc.yaml:
# Change: subnet: 10.10.10.0/24
# To:     subnet: 10.20.10.0/24

# Reapply:
kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<h3>Issue: DHCPv4 range too small</h3>
<p><strong>Symptom:</strong> Servers fail to get DHCP addresses, DHCP exhaustion warnings in logs</p>
<p><strong>Cause:</strong> DHCP range doesn&#39;t accommodate number of servers</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Calculate needed size:
# - Current range: .10 to .250 = 241 IPs
# - Needed: 300 servers

# Solution 1: Expand to /23 (512 IPs)
# Edit YAML:
#   subnet: 10.10.20.0/23      # was /24
#   gateway: 10.10.20.1
#   dhcp:
#     range:
#       start: 10.10.20.10
#       end: 10.10.21.250        # Now spans into .21.x range

# Solution 2: Use multiple subnets
# Create worker-nodes-2 subnet with additional DHCP range

# Reapply configuration:
kubectl apply -f web-app-prod-vpc.yaml
</code></pre>
<h3>Issue: &quot;VPC not found&quot; after creation</h3>
<p><strong>Symptom:</strong> <code>kubectl get vpc web-app-prod</code> returns &quot;not found&quot; immediately after apply</p>
<p><strong>Cause:</strong> Wrong namespace or kubectl context</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># Check current namespace
kubectl config view --minify | grep namespace

# If empty, you&#39;re in default namespace - verify VPC namespace matches
cat web-app-prod-vpc.yaml | grep namespace

# List VPCs in all namespaces
kubectl get vpcs -A

# If VPC is in different namespace, specify it:
kubectl get vpc web-app-prod -n &lt;namespace&gt;

# Or set your default namespace:
kubectl config set-context --current --namespace=&lt;namespace&gt;
</code></pre>
<h3>Issue: VLAN ID conflict with existing infrastructure</h3>
<p><strong>Symptom:</strong> VPC applies but connectivity fails; external VLAN already in use</p>
<p><strong>Cause:</strong> Specified VLAN conflicts with existing network infrastructure outside Hedgehog</p>
<p><strong>Fix:</strong></p>
<pre><code class=""language-bash""># If integrating with existing network, document VLAN usage
# Create a VLAN allocation spreadsheet or use IPAM tool

# Change VLAN to unused ID
# Edit web-app-prod-vpc.yaml:
#   vlan: 1010  # Change to unused VLAN, e.g., 2010

# Reapply:
kubectl apply -f web-app-prod-vpc.yaml

# If you don&#39;t care about specific VLAN, omit it entirely:
# Remove &quot;vlan: 1010&quot; line - Hedgehog will auto-assign
</code></pre>
<h2>Resources</h2>
<h3>Hedgehog CRDs</h3>
<p><strong>VPC</strong> - Virtual Private Cloud definition</p>
<ul>
<li>View all: <code>kubectl get vpcs</code></li>
<li>View specific: <code>kubectl get vpc &lt;name&gt;</code></li>
<li>Inspect details: <code>kubectl describe vpc &lt;name&gt;</code></li>
<li>View YAML: <code>kubectl get vpc &lt;name&gt; -o yaml</code></li>
<li>Delete: <code>kubectl delete vpc &lt;name&gt;</code></li>
</ul>
<h3>kubectl Commands Reference</h3>
<p><strong>VPC lifecycle:</strong></p>
<pre><code class=""language-bash""># Create or update VPC
kubectl apply -f vpc.yaml

# List VPCs
kubectl get vpcs

# List VPCs in all namespaces
kubectl get vpcs -A

# Get VPC details
kubectl describe vpc &lt;name&gt;

# View VPC in YAML format
kubectl get vpc &lt;name&gt; -o yaml

# View specific field (e.g., subnets)
kubectl get vpc &lt;name&gt; -o jsonpath=&#39;{.spec.subnets}&#39; | jq

# Delete VPC
kubectl delete vpc &lt;name&gt;

# Delete VPC from file
kubectl delete -f vpc.yaml
</code></pre>
<p><strong>Event monitoring:</strong></p>
<pre><code class=""language-bash""># View VPC events
kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt;

# Sort events by time
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | grep &lt;vpc-name&gt;

# Watch events in real-time
kubectl get events --watch --field-selector involvedObject.name=&lt;vpc-name&gt;

# View all recent events
kubectl get events --sort-by=&#39;.lastTimestamp&#39; | tail -20
</code></pre>
<p><strong>Validation and troubleshooting:</strong></p>
<pre><code class=""language-bash""># Check for errors
kubectl describe vpc &lt;name&gt; | grep -A 5 Events

# View all VPCs with their subnets
kubectl get vpcs -o yaml | grep -E &quot;(name:|subnet:)&quot;

# Verify DHCP configuration
kubectl get vpc &lt;name&gt; -o jsonpath=&#39;{.spec.subnets.*.dhcp}&#39; | jq
</code></pre>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""../fabric-operations-foundations-recap/README.md"">Module 1.4: Course 1 Recap</a></li>
<li>Next: Module 2.2: VPC Attachments (coming soon)</li>
<li>Pathway: <a href=""../../pathways/network-like-hyperscaler.json"">Network Like a Hyperscaler</a></li>
</ul>
<h3>External Documentation</h3>
<ul>
<li><a href=""https://docs.hedgehog.io/docs/concepts/vpc"">Hedgehog VPC Documentation</a></li>
<li><a href=""https://docs.hedgehog.io/"">Hedgehog Fabric Guide</a></li>
<li><a href=""https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"">Kubernetes Custom Resources</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/"">Kubernetes Events</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing"">Understanding CIDR Notation</a></li>
</ul>
<hr>
<p><strong>Module Complete!</strong> You&#39;ve successfully learned VPC provisioning. Ready to attach servers to this VPC in Module 2.2.</p>
",201,20,,,"hedgehog,fabric,vpc,provisioning,operations"
198563771461,Hands-On Fabric Diagnosis Lab,fabric-operations-diagnosis-lab,[object Object],"Apply systematic troubleshooting methodology in a hands-on lab, diagnosing a real VPCAttachment connectivity failure using hypothesis-driven investigation.","<h2>Introduction</h2>
<p>In <strong>Module 4.1a: Systematic Troubleshooting Framework</strong>, you learned the methodology for diagnosing fabric issues:</p>
<ul>
<li>Hypothesis-driven investigation</li>
<li>Common failure modes</li>
<li>Layered diagnostic workflow (Events → Agent CRD → Grafana → Logs)</li>
<li>Decision trees for structured diagnosis</li>
</ul>
<p>Now it&#39;s time to <strong>put that methodology into practice</strong>.</p>
<h3>The Lab Scenario</h3>
<p>You&#39;ll diagnose a real connectivity failure: <strong>A VPCAttachment was created successfully with no errors in kubectl events, but the server cannot communicate within the VPC.</strong></p>
<p>This is a classic troubleshooting challenge: Configuration looks correct, system reports success, but it doesn&#39;t work.</p>
<p>Using the systematic methodology from Module 4.1a, you&#39;ll:</p>
<ol>
<li>Gather symptoms and form hypotheses</li>
<li>Test hypotheses systematically using kubectl and Grafana</li>
<li>Identify the root cause through elimination</li>
<li>Document your findings and solution</li>
</ol>
<p>By the end of this lab, you&#39;ll have practiced the complete diagnostic workflow on a realistic scenario.</p>
<hr>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Apply hypothesis-driven investigation</strong> - Form and test hypotheses systematically</li>
<li><strong>Use layered diagnostic workflow</strong> - Progress from events to Agent CRD to Grafana</li>
<li><strong>Follow decision trees</strong> - Apply structured diagnostic paths to real scenarios</li>
<li><strong>Identify VLAN configuration issues</strong> - Diagnose VLAN conflicts and mismatches</li>
<li><strong>Document troubleshooting findings</strong> - Create clear problem statements and solutions</li>
</ol>
<hr>
<h2>Prerequisites</h2>
<p>Before starting this module, you should have:</p>
<p><strong>Completed Modules:</strong></p>
<ul>
<li>Module 4.1a: Systematic Troubleshooting Framework (methodology and decision trees)</li>
<li>All previous courses (Courses 1-3)</li>
</ul>
<p><strong>Understanding:</strong></p>
<ul>
<li>Hypothesis-driven investigation framework</li>
<li>Common failure modes (VPC attachment, BGP, interface, configuration drift)</li>
<li>Layered diagnostic workflow</li>
<li>Decision trees for common scenarios</li>
</ul>
<p><strong>Environment:</strong></p>
<ul>
<li>kubectl configured and authenticated</li>
<li>Grafana access (<a href=""http://localhost:3000"">http://localhost:3000</a>)</li>
<li>Hedgehog fabric with at least one VPC deployed</li>
</ul>
<hr>
<h2>Scenario</h2>
<p><strong>Incident Report:</strong></p>
<p>A developer reports that <strong>server-07</strong> in VPC <strong>customer-app-vpc</strong> cannot reach the gateway or other servers in the VPC. The VPCAttachment was created this morning using the standard GitOps workflow.</p>
<p><strong>Initial Investigation:</strong></p>
<p>You run <code>kubectl describe vpcattachment customer-app-vpc-server-07</code> and see no error events. The resource exists, the controller processed it successfully, and there are no warnings.</p>
<p>But the server still has no connectivity.</p>
<p><strong>Your Task:</strong></p>
<p>Use systematic troubleshooting methodology to identify the root cause and document a solution.</p>
<p><strong>Known Information:</strong></p>
<ul>
<li>VPC: <code>customer-app-vpc</code></li>
<li>Subnet: <code>frontend</code> (10.20.10.0/24, gateway 10.20.10.1, VLAN 1025)</li>
<li>Server: <code>server-07</code></li>
<li>Expected Connection: <code>server-07--unbundled--leaf-04</code></li>
<li>VPCAttachment: <code>customer-app-vpc-server-07</code></li>
<li>Symptom: Server cannot ping gateway 10.20.10.1</li>
</ul>
<hr>
<h2>Hands-On Lab</h2>
<h3>Lab Overview</h3>
<p><strong>Objective:</strong> Diagnose a VPCAttachment connectivity failure using systematic troubleshooting methodology.</p>
<p><strong>Scenario:</strong> Server-07 in VPC <code>customer-app-vpc</code> cannot reach the gateway or other servers. The VPCAttachment was created this morning and <code>kubectl describe</code> shows no error events.</p>
<p><strong>Environment:</strong></p>
<ul>
<li>kubectl: Already configured</li>
<li>Grafana: <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li>Server access: Available if needed</li>
</ul>
<p><strong>Known Information:</strong></p>
<ul>
<li>VPC: <code>customer-app-vpc</code></li>
<li>Subnet: <code>frontend</code> (10.20.10.0/24, gateway 10.20.10.1, VLAN 1025)</li>
<li>Server: <code>server-07</code></li>
<li>Connection: <code>server-07--unbundled--leaf-04</code> (correct connection name)</li>
<li>VPCAttachment: <code>customer-app-vpc-server-07</code></li>
</ul>
<hr>
<h3>Task 1: Gather Symptoms and Form Hypotheses</h3>
<p><strong>Estimated Time:</strong> 2 minutes</p>
<p><strong>Objective:</strong> Document symptoms and generate possible causes using systematic methodology.</p>
<h4>Step 1.1: Document Symptoms</h4>
<p>Write down what you know (use a text file or notebook):</p>
<pre><code>Symptoms:
- Expected behavior: server-07 should ping gateway 10.20.10.1
- Actual behavior: ping fails with &quot;Destination Host Unreachable&quot;
- Recent change: VPCAttachment created today via GitOps
- kubectl describe: No error events

Timeline:
- VPCAttachment created: This morning (10:00 AM)
- Issue reported: This morning (10:15 AM)
- First check (kubectl describe): No errors visible
</code></pre>
<p>This documentation helps you:</p>
<ul>
<li>Clarify what&#39;s actually broken</li>
<li>Establish a timeline</li>
<li>Identify recent changes</li>
</ul>
<h4>Step 1.2: Form Hypotheses</h4>
<p>Based on symptoms, list possible causes. Use the common failure modes from Module 4.1a:</p>
<p><strong>Hypothesis List:</strong></p>
<ol>
<li><strong>Wrong connection name</strong> - VPCAttachment references incorrect switch or connection</li>
<li><strong>Wrong subnet</strong> - VPCAttachment references non-existent subnet</li>
<li><strong>VLAN not configured</strong> - Reconciliation failed, VLAN 1025 not on switch interface</li>
<li><strong>VLAN mismatch</strong> - VLAN conflict caused different VLAN to be allocated</li>
<li><strong>Interface down</strong> - leaf-04 Ethernet interface is operationally down</li>
<li><strong>Server interface misconfigured</strong> - Server enp2s1 not configured correctly</li>
<li><strong>nativeVLAN mismatch</strong> - VPCAttachment and server expect different VLAN tagging</li>
</ol>
<p><strong>Why these hypotheses?</strong></p>
<ul>
<li>Covers configuration issues (1, 2, 4, 7)</li>
<li>Covers connectivity issues (3, 5)</li>
<li>Covers server-side issues (6)</li>
</ul>
<h4>Success Criteria</h4>
<ul>
<li>✅ Symptoms documented clearly</li>
<li>✅ At least 5 hypotheses listed</li>
<li>✅ Hypotheses cover configuration, connectivity, and server issues</li>
</ul>
<p><strong>Time check:</strong> You should complete this in 2 minutes or less. Don&#39;t overthink—list possibilities quickly, you&#39;ll test them systematically.</p>
<hr>
<h3>Task 2: Test Hypotheses with kubectl</h3>
<p><strong>Estimated Time:</strong> 3 minutes</p>
<p><strong>Objective:</strong> Systematically test each hypothesis using kubectl commands.</p>
<h4>Step 2.1: Check VPCAttachment Configuration</h4>
<p>Test hypotheses 1 and 2: Wrong connection or wrong subnet.</p>
<pre><code class=""language-bash""># Get full VPCAttachment spec
kubectl get vpcattachment customer-app-vpc-server-07 -o yaml

# Check connection reference
kubectl get vpcattachment customer-app-vpc-server-07 -o jsonpath=&#39;{.spec.connection}&#39;
# Expected: server-07--unbundled--leaf-04

# Check subnet reference
kubectl get vpcattachment customer-app-vpc-server-07 -o jsonpath=&#39;{.spec.subnet}&#39;
# Expected: customer-app-vpc/frontend
</code></pre>
<p><strong>Verify connection exists:</strong></p>
<pre><code class=""language-bash"">kubectl get connection server-07--unbundled--leaf-04 -n fab
# Should show: Connection exists
</code></pre>
<p><strong>Test Result:</strong></p>
<ul>
<li>Connection reference: ✅ Correct (server-07--unbundled--leaf-04)</li>
<li>Subnet reference: ✅ Correct (customer-app-vpc/frontend)</li>
</ul>
<p><strong>Hypotheses 1 and 2: ELIMINATED</strong></p>
<hr>
<h4>Step 2.2: Verify Subnet Exists in VPC</h4>
<p>Test hypothesis 2 more thoroughly:</p>
<pre><code class=""language-bash""># Check if frontend subnet exists in customer-app-vpc
kubectl get vpc customer-app-vpc -o yaml | grep -A 5 &quot;frontend:&quot;

# Expected output shows:
#   frontend:
#     subnet: 10.20.10.0/24
#     gateway: 10.20.10.1
#     vlan: 1025
</code></pre>
<p><strong>Test Result:</strong></p>
<ul>
<li>Subnet exists: ✅</li>
<li>VLAN specified: 1025</li>
</ul>
<p><strong>Hypothesis 2: ELIMINATED (subnet exists)</strong></p>
<hr>
<h4>Step 2.3: Check Agent CRD for leaf-04</h4>
<p>Test hypotheses 3, 4, and 5: VLAN configuration and interface state.</p>
<p><strong>Identify which interface server-07 connects to:</strong></p>
<pre><code class=""language-bash"">kubectl get connection server-07--unbundled--leaf-04 -n fab -o yaml | grep &quot;port:&quot;
# Expected output: leaf-04/E1/8 (which maps to Ethernet8)
</code></pre>
<p><strong>Check interface state in Agent CRD:</strong></p>
<pre><code class=""language-bash""># Check if interface is up
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8.oper}&#39;
# Expected: up

# Check which VLANs are configured
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8.vlans}&#39;
# Look for: VLAN list
</code></pre>
<p><strong>CRITICAL FINDING:</strong></p>
<p>The Agent CRD shows:</p>
<pre><code class=""language-json"">{
  &quot;oper&quot;: &quot;up&quot;,
  &quot;admin&quot;: &quot;up&quot;,
  &quot;vlans&quot;: [1020],
  ...
}
</code></pre>
<p><strong>Interface is up (✅) but VLAN is 1020, not 1025!</strong></p>
<p><strong>Test Result:</strong></p>
<ul>
<li>Interface oper: ✅ Up (Hypothesis 5 eliminated)</li>
<li>VLAN configured: ❌ VLAN 1020, expected 1025</li>
</ul>
<p><strong>Hypothesis 4: CONFIRMED (VLAN mismatch)</strong></p>
<hr>
<h4>Step 2.4: Investigate Why VLAN is Wrong</h4>
<p>Now that you&#39;ve identified the mismatch, investigate the root cause:</p>
<pre><code class=""language-bash""># Check VPC configuration again
kubectl get vpc customer-app-vpc -o yaml | grep -A 10 &quot;frontend:&quot;

# Check all VLANs currently in use
kubectl get vpc -A -o yaml | grep &quot;vlan:&quot; | sort

# Look for VLAN 1025 usage
kubectl get vpc -A -o yaml | grep &quot;1025&quot;
</code></pre>
<p><strong>Discovery:</strong></p>
<p>Another VPC (<code>existing-vpc-prod</code>) is using VLAN 1025. When <code>customer-app-vpc</code> was created, the VLANNamespace automatically allocated VLAN 1020 instead due to the conflict!</p>
<p><strong>Root Cause Identified:</strong></p>
<p>VLAN conflict. The VPC subnet definition manually specifies VLAN 1025, but the VLANNamespace allocated VLAN 1020 because 1025 was already in use by another VPC.</p>
<p><strong>Why kubectl describe showed no errors:</strong></p>
<p>The controller successfully reconciled the VPCAttachment with VLAN 1020 (the allocated VLAN). No error occurred from the controller&#39;s perspective—the configuration just doesn&#39;t match the operator&#39;s expectation.</p>
<hr>
<h4>Step 2.5: Document Root Cause</h4>
<p>Write your findings:</p>
<pre><code>Root Cause:
- VLAN conflict between customer-app-vpc and existing-vpc-prod
- VPC subnet manually specified VLAN 1025
- VLANNamespace allocated VLAN 1020 instead (1025 already in use)
- Switch configured with VLAN 1020 (correct according to allocation)
- Server expects VLAN 1025 (incorrect expectation)

Evidence:
- Agent CRD shows VLAN 1020 on leaf-04/Ethernet8
- VPC spec shows VLAN 1025 in subnet definition
- existing-vpc-prod is using VLAN 1025

Solution:
- Update customer-app-vpc frontend subnet VLAN to 1020 (match allocation)
- OR choose a different unused VLAN for customer-app-vpc
</code></pre>
<h4>Success Criteria</h4>
<ul>
<li>✅ Hypotheses tested systematically</li>
<li>✅ Root cause identified (VLAN mismatch due to conflict)</li>
<li>✅ Evidence documented</li>
<li>✅ Solution path clear</li>
</ul>
<p><strong>Time check:</strong> You should complete this in 3 minutes or less with practice.</p>
<hr>
<h3>Task 3: Validate with Grafana</h3>
<p><strong>Estimated Time:</strong> 1-2 minutes</p>
<p><strong>Objective:</strong> Confirm findings using Grafana dashboards for visual validation.</p>
<h4>Step 3.1: Check Interfaces Dashboard</h4>
<ol>
<li>Open Grafana: <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li>Navigate to &quot;Hedgehog Interfaces&quot; dashboard</li>
<li>Set filters:<ul>
<li>Switch: <code>leaf-04</code></li>
<li>Interface: <code>Ethernet8</code></li>
</ul>
</li>
<li>Observe:<ul>
<li><strong>Operational State:</strong> Should show &quot;up&quot;</li>
<li><strong>VLANs Configured:</strong> Should show VLAN 1020</li>
<li><strong>Traffic Patterns:</strong> Should show minimal or no traffic (server can&#39;t communicate)</li>
</ul>
</li>
</ol>
<p><strong>Expected Finding:</strong></p>
<p>Grafana confirms:</p>
<ul>
<li>Interface is up ✅</li>
<li>VLAN 1020 configured ✅</li>
<li>Low or zero traffic (consistent with connectivity failure)</li>
</ul>
<hr>
<h4>Step 3.2: Check Fabric Dashboard</h4>
<ol>
<li>Navigate to &quot;Hedgehog Fabric&quot; dashboard</li>
<li>Check BGP sessions for leaf-04</li>
<li>Verify all BGP sessions are established</li>
</ol>
<p><strong>Expected Finding:</strong></p>
<p>All BGP sessions show &quot;established&quot; state, confirming this is not a BGP routing issue.</p>
<hr>
<h4>Step 3.3: Correlation Check</h4>
<p>Look at Grafana timeline:</p>
<ul>
<li>When was VLAN 1020 added to Ethernet8? (Should correlate with VPCAttachment creation time)</li>
<li>Any interface state changes around that time? (Should show VLAN added, no flapping)</li>
</ul>
<p><strong>What Grafana Tells You:</strong></p>
<ul>
<li>Visual confirmation of Agent CRD findings</li>
<li>No intermittent issues (interface stable)</li>
<li>VLAN was added successfully (just the wrong VLAN ID)</li>
</ul>
<h4>Success Criteria</h4>
<ul>
<li>✅ Grafana confirms VLAN 1020 configured (not 1025)</li>
<li>✅ Interface is up and stable</li>
<li>✅ BGP sessions healthy (not a routing issue)</li>
</ul>
<p><strong>Time check:</strong> 1-2 minutes for visual confirmation.</p>
<hr>
<h3>Task 4: Document Root Cause and Solution</h3>
<p><strong>Estimated Time:</strong> 1 minute</p>
<p><strong>Objective:</strong> Write clear problem statement and solution for handoff or documentation.</p>
<h4>Step 4.1: Problem Statement</h4>
<p>Write a concise problem statement:</p>
<pre><code>Problem Statement:

VPCAttachment customer-app-vpc-server-07 created successfully without errors,
but server-07 cannot communicate within VPC.

Root Cause:
VLAN conflict. VPC subnet specifies VLAN 1025, but switch interface configured
with VLAN 1020 due to conflict with existing-vpc-prod (already using VLAN 1025).
VLANNamespace automatically allocated VLAN 1020 instead.

Controller reconciled successfully with VLAN 1020 (no errors), but configuration
does not match operator expectation (VLAN 1025).

Impact:
- Server-07 has no connectivity within customer-app-vpc
- Application dependent on server-07 is down
</code></pre>
<hr>
<h4>Step 4.2: Solution Options</h4>
<p>Document solution paths:</p>
<p><strong>Option 1 (Recommended): Update VPC VLAN to match allocation</strong></p>
<pre><code class=""language-bash""># Edit customer-app-vpc in Gitea
# Change frontend subnet VLAN from 1025 to 1020

# In Gitea: network-like-hyperscaler/vpcs/customer-app-vpc.yaml
spec:
  subnets:
    frontend:
      subnet: 10.20.10.0/24
      gateway: 10.20.10.1
      vlan: 1020  # Changed from 1025

# Commit change
git add vpcs/customer-app-vpc.yaml
git commit -m &quot;Fix VLAN conflict: use allocated VLAN 1020 for customer-app-vpc frontend&quot;
git push

# Wait for ArgoCD sync
kubectl get vpc customer-app-vpc -w

# Verify with kubectl events
kubectl get events --field-selector involvedObject.name=customer-app-vpc
</code></pre>
<p><strong>Why recommended:</strong> Aligns configuration with reality (VLAN 1020 already allocated and configured).</p>
<hr>
<p><strong>Option 2: Choose unused VLAN</strong></p>
<pre><code class=""language-bash""># Check available VLANs
kubectl get vpc -A -o yaml | grep &quot;vlan:&quot; | sort

# Identify unused VLAN (e.g., 1026)

# Edit customer-app-vpc in Gitea
# Change frontend subnet VLAN to 1026

# Commit to Gitea, wait for sync
</code></pre>
<p><strong>Why consider:</strong> If VLAN 1025 has significance (e.g., organizational standard).</p>
<hr>
<p><strong>Option 3: Remove VLAN specification (let VLANNamespace allocate)</strong></p>
<pre><code class=""language-bash""># Edit customer-app-vpc in Gitea
# Remove manual VLAN specification

spec:
  subnets:
    frontend:
      subnet: 10.20.10.0/24
      gateway: 10.20.10.1
      # vlan: 1025  # Remove this line

# VLANNamespace will auto-allocate next available VLAN
</code></pre>
<p><strong>Why consider:</strong> Prevents future conflicts, follows GitOps best practices.</p>
<hr>
<h4>Step 4.3: Prevention</h4>
<p>Document how to prevent this issue in the future:</p>
<pre><code>Prevention:

1. Do not manually specify VLANs in VPC subnets unless required
   - Let VLANNamespace auto-allocate to avoid conflicts

2. If manual VLAN required, check for conflicts first:
   kubectl get vpc -A -o yaml | grep &quot;vlan:&quot; | sort

3. Use VLANNamespace ranges to segregate VLAN usage:
   - VLANNamespace &quot;production&quot;: 1000-1999
   - VLANNamespace &quot;development&quot;: 2000-2999

4. Monitor kubectl events after VPC creation:
   kubectl get events --field-selector involvedObject.name=&lt;vpc-name&gt;
</code></pre>
<h4>Success Criteria</h4>
<ul>
<li>✅ Root cause documented clearly</li>
<li>✅ Solution options identified with pros/cons</li>
<li>✅ Next steps defined</li>
<li>✅ Prevention measures documented</li>
</ul>
<hr>
<h3>Lab Summary</h3>
<p><strong>What You Accomplished:</strong></p>
<p>You successfully diagnosed a VPCAttachment connectivity failure using systematic troubleshooting methodology:</p>
<ol>
<li>✅ Gathered symptoms and formed hypotheses</li>
<li>✅ Tested hypotheses systematically using kubectl</li>
<li>✅ Identified root cause (VLAN mismatch due to conflict)</li>
<li>✅ Validated findings with Grafana</li>
<li>✅ Documented solution path and prevention measures</li>
</ol>
<p><strong>Key Techniques Used:</strong></p>
<ul>
<li>Hypothesis-driven investigation (not random checking)</li>
<li>Layered diagnostic approach (events → Agent CRD → Grafana)</li>
<li>Evidence-based elimination (tested each hypothesis)</li>
<li>Root cause identification (VLAN conflict, not symptoms)</li>
</ul>
<p><strong>Time to Resolution:</strong></p>
<ul>
<li>Task 1: 2 minutes (symptoms and hypotheses)</li>
<li>Task 2: 3 minutes (hypothesis testing)</li>
<li>Task 3: 1-2 minutes (Grafana validation)</li>
<li>Task 4: 1 minute (documentation)</li>
<li><strong>Total: 7-8 minutes from symptom to solution</strong></li>
</ul>
<p><strong>Contrast with Random Checking:</strong></p>
<p>Without systematic methodology, you might have:</p>
<ul>
<li>Checked controller logs (no useful info)</li>
<li>Restarted the controller (no effect)</li>
<li>Deleted and recreated VPCAttachment (same result)</li>
<li>Checked BGP (not relevant)</li>
<li>Escalated to support (with no evidence)</li>
<li><strong>Spent 30+ minutes without identifying root cause</strong></li>
</ul>
<hr>
<h2>Troubleshooting</h2>
<h3>Common Lab Challenges</h3>
<h4>Challenge: &quot;All my hypotheses were eliminated, but the issue persists&quot;</h4>
<p><strong>What this means:</strong> Your initial hypothesis list didn&#39;t include the actual root cause.</p>
<p><strong>What to do:</strong></p>
<ol>
<li>Review your evidence collection (Agent CRD, Grafana, events)</li>
<li>Form new hypotheses based on what you <em>did</em> find</li>
<li>Example: If VLAN is configured and interface is up, maybe VLAN ID is wrong</li>
</ol>
<p><strong>Key insight:</strong> Hypothesis-driven investigation is iterative. Eliminating hypotheses is progress—it narrows the problem space.</p>
<hr>
<h4>Challenge: &quot;I found the root cause but don&#39;t know how to fix it&quot;</h4>
<p><strong>What this means:</strong> Diagnosis succeeded, but solution implementation is unclear.</p>
<p><strong>What to do:</strong></p>
<ol>
<li>Consult Module 2.2 (VPC Design Patterns) for configuration guidance</li>
<li>Check Module 1.3 (GitOps Workflow) for making changes</li>
<li>Reference Hedgehog documentation for CRD field definitions</li>
</ol>
<p><strong>Key insight:</strong> Diagnosis and resolution are separate skills. This module focuses on diagnosis—Module 4.2 covers rollback and recovery.</p>
<hr>
<h4>Challenge: &quot;kubectl commands are slow or timing out&quot;</h4>
<p><strong>What this means:</strong> Kubernetes API server may be under load or network issues.</p>
<p><strong>What to do:</strong></p>
<ol>
<li>Check kubectl cluster-info and basic connectivity</li>
<li>Use <code>--request-timeout</code> flag to extend timeout</li>
<li>If persistent, check control node resources (CPU, memory)</li>
</ol>
<hr>
<h4>Challenge: &quot;Grafana dashboards show &#39;No Data&#39;&quot;</h4>
<p><strong>What this means:</strong> Telemetry may not be configured or Prometheus/Loki not accessible.</p>
<p><strong>What to do:</strong></p>
<ol>
<li>Check if telemetry is configured in Fabricator</li>
<li>Rely on kubectl and Agent CRD for this lab</li>
<li>Grafana validation is optional if telemetry is not configured</li>
</ol>
<p><strong>Reference:</strong> Module 3.1 (Telemetry and Prometheus) for telemetry setup.</p>
<hr>
<h4>Challenge: &quot;I&#39;m not sure which hypothesis to test first&quot;</h4>
<p><strong>What this means:</strong> You need a prioritization strategy.</p>
<p><strong>What to do:</strong>
Test hypotheses in this order:</p>
<ol>
<li><strong>Fastest to check:</strong> kubectl events (10 seconds)</li>
<li><strong>Most likely:</strong> Common failure modes from Module 4.1a</li>
<li><strong>Highest impact:</strong> Issues that would affect multiple resources</li>
</ol>
<p><strong>Key insight:</strong> Start with quick checks, then move to detailed investigation.</p>
<hr>
<h3>Debugging the Diagnostic Process</h3>
<p>If you&#39;re stuck, ask yourself:</p>
<ol>
<li><p><strong>Did I collect evidence from all four layers?</strong></p>
<ul>
<li>Events, Agent CRD, Grafana, logs</li>
</ul>
</li>
<li><p><strong>Am I testing hypotheses or guessing?</strong></p>
<ul>
<li>Each hypothesis should have a specific test</li>
</ul>
</li>
<li><p><strong>Am I documenting what I find?</strong></p>
<ul>
<li>Write down results to avoid re-checking</li>
</ul>
</li>
<li><p><strong>Have I used decision trees?</strong></p>
<ul>
<li>Follow Decision Tree 3 for &quot;VPCAttachment shows success but doesn&#39;t work&quot;</li>
</ul>
</li>
<li><p><strong>Am I comparing expected vs. actual?</strong></p>
<ul>
<li>VPC expects VLAN 1025, Agent CRD shows VLAN 1020 → mismatch</li>
</ul>
</li>
</ol>
<hr>
<h2>Resources</h2>
<h3>Reference Documentation</h3>
<p><strong>Hedgehog CRD Reference:</strong></p>
<ul>
<li>VPC and VPCAttachment spec fields</li>
<li>Agent CRD status fields (interfaces, bgpNeighbors, platform)</li>
<li>Connection CRD structure</li>
</ul>
<p><strong>Observability and Diagnostics:</strong></p>
<ul>
<li>Module 3.1: Telemetry and Prometheus</li>
<li>Module 3.2: Grafana Dashboards</li>
<li>Module 3.3: Agent CRD Deep Dive</li>
<li>Module 3.4: Pre-Escalation Diagnostic Checklist</li>
</ul>
<p><strong>GitOps Workflow:</strong></p>
<ul>
<li>Module 1.3: GitOps with Hedgehog Fabric</li>
<li>Module 4.2: Rollback and Recovery (upcoming)</li>
</ul>
<hr>
<h3>Quick Reference: Diagnostic Commands</h3>
<p><strong>Layer 1: Events</strong></p>
<pre><code class=""language-bash""># Check for Warning events
kubectl get events --field-selector type=Warning --sort-by=&#39;.lastTimestamp&#39;

# Events for specific resource
kubectl describe vpcattachment &lt;name&gt;
</code></pre>
<p><strong>Layer 2: Agent CRD</strong></p>
<pre><code class=""language-bash""># Check agent readiness
kubectl get agents -n fab

# View interface state
kubectl get agent &lt;switch&gt; -n fab -o jsonpath=&#39;{.status.state.interfaces.&lt;interface&gt;}&#39; | jq

# View BGP neighbors
kubectl get agent &lt;switch&gt; -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq
</code></pre>
<p><strong>Layer 3: Grafana</strong></p>
<ul>
<li>Fabric Dashboard: <a href=""http://localhost:3000/d/fabric/hedgehog-fabric"">http://localhost:3000/d/fabric/hedgehog-fabric</a></li>
<li>Interfaces Dashboard: <a href=""http://localhost:3000/d/interfaces/hedgehog-interfaces"">http://localhost:3000/d/interfaces/hedgehog-interfaces</a></li>
<li>Logs Dashboard: <a href=""http://localhost:3000/d/logs/hedgehog-logs"">http://localhost:3000/d/logs/hedgehog-logs</a></li>
</ul>
<p><strong>Layer 4: Logs</strong></p>
<pre><code class=""language-bash""># Controller logs
kubectl logs -n fab deployment/fabric-controller-manager --tail=200

# Agent logs
kubectl logs -n fab &lt;agent-pod-name&gt;
</code></pre>
<hr>
<h3>Decision Tree Quick Reference</h3>
<p><strong>Use Decision Tree 1 when:</strong></p>
<ul>
<li>Server cannot communicate within VPC</li>
<li>VPCAttachment exists, no errors</li>
</ul>
<p><strong>Use Decision Tree 2 when:</strong></p>
<ul>
<li>Cross-VPC connectivity fails</li>
<li>VPCPeering exists</li>
</ul>
<p><strong>Use Decision Tree 3 when:</strong></p>
<ul>
<li>kubectl describe shows success</li>
<li>Server has no connectivity</li>
<li>No obvious errors</li>
</ul>
<hr>
<h3>Common VLAN Issues</h3>
<table>
<thead>
<tr>
<th>Symptom</th>
<th>Root Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody><tr>
<td>VLAN mismatch (allocated ≠ specified)</td>
<td>VLAN conflict</td>
<td>Update VPC VLAN to match allocation</td>
</tr>
<tr>
<td>VLAN not configured on interface</td>
<td>Wrong connection reference</td>
<td>Fix VPCAttachment connection field</td>
</tr>
<tr>
<td>VLAN configured but wrong ID</td>
<td>Manual VLAN specification conflict</td>
<td>Remove manual VLAN, let VLANNamespace allocate</td>
</tr>
<tr>
<td>nativeVLAN mismatch</td>
<td>VPCAttachment nativeVLAN ≠ server config</td>
<td>Align nativeVLAN setting with server interface</td>
</tr>
</tbody></table>
<hr>
<h3>Common BGP Issues</h3>
<table>
<thead>
<tr>
<th>Symptom</th>
<th>Root Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody><tr>
<td>BGP state: idle</td>
<td>Neighbor IP unreachable</td>
<td>Check ExternalAttachment switch IP and neighbor IP</td>
</tr>
<tr>
<td>BGP state: active</td>
<td>ASN mismatch or config error</td>
<td>Verify ASN in ExternalAttachment matches external router</td>
</tr>
<tr>
<td>BGP established but no routes</td>
<td>Permit list missing subnets</td>
<td>Update VPCPeering or ExternalPeering permit</td>
</tr>
<tr>
<td>Routes filtered</td>
<td>Community mismatch</td>
<td>Check External inboundCommunity and outboundCommunity</td>
</tr>
</tbody></table>
<hr>
<h3>Escalation Criteria</h3>
<p><strong>When to escalate to support:</strong></p>
<ol>
<li><p><strong>All decision tree paths exhausted</strong></p>
<ul>
<li>Followed relevant decision tree to end</li>
<li>Issue doesn&#39;t match known patterns</li>
</ul>
</li>
<li><p><strong>Evidence collected but root cause unclear</strong></p>
<ul>
<li>Completed all 4 layers of diagnostic workflow</li>
<li>Findings don&#39;t point to specific root cause</li>
</ul>
</li>
<li><p><strong>Suspected platform issue</strong></p>
<ul>
<li>Agent CRD shows switch failures (PSU, temperature)</li>
<li>Controller logs show internal errors</li>
</ul>
</li>
<li><p><strong>Time-sensitive production outage</strong></p>
<ul>
<li>Issue blocking critical services</li>
<li>Need expert assistance to resolve quickly</li>
</ul>
</li>
</ol>
<p><strong>Before escalating, ensure you have:</strong></p>
<ul>
<li>✅ Symptoms documented</li>
<li>✅ Hypotheses tested</li>
<li>✅ Evidence collected (events, Agent CRD, Grafana, logs)</li>
<li>✅ Decision tree followed</li>
<li>✅ Relevant kubectl outputs saved</li>
</ul>
<p>Reference Module 4.3 (Coordinating with Support) for escalation procedures.</p>
<hr>
<h3>Next Steps</h3>
<p><strong>Module 4.2: Rollback and Recovery</strong></p>
<p>Learn how to safely undo changes when things go wrong:</p>
<ul>
<li>GitOps rollback procedures</li>
<li>Safe deletion order for Hedgehog resources</li>
<li>Handling stuck resources</li>
<li>Emergency recovery patterns</li>
</ul>
<p><strong>Module 4.3: Coordinating with Support</strong></p>
<p>Learn how to work effectively with Hedgehog support:</p>
<ul>
<li>Crafting effective support tickets</li>
<li>Providing diagnostic evidence</li>
<li>Troubleshooting with support engineers</li>
<li>Post-resolution follow-up</li>
</ul>
<p><strong>Module 4.4: Post-Incident Review</strong></p>
<p>Learn how to conduct effective post-incident reviews:</p>
<ul>
<li>Documenting incidents</li>
<li>Root cause analysis</li>
<li>Prevention measures</li>
<li>Knowledge sharing</li>
</ul>
<hr>
<h2>Assessment</h2>
<p>Test your understanding of systematic troubleshooting methodology.</p>
<h3>Question 1: Troubleshooting Methodology</h3>
<p><strong>Scenario:</strong> Server-03 in VPC <code>prod-vpc</code> cannot reach server-04 in the same VPC. You&#39;ve checked kubectl events (no errors) and verified both VPCAttachments exist.</p>
<p>What is your NEXT diagnostic step using systematic methodology?</p>
<ul>
<li>A) Restart the fabric controller</li>
<li>B) Check Agent CRD to verify VLANs configured on both switch interfaces</li>
<li>C) Escalate to support immediately</li>
<li>D) Delete and recreate both VPCAttachments</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Check Agent CRD to verify VLANs configured on both switch interfaces</p>
<p><strong>Explanation:</strong></p>
<p>Following the diagnostic workflow (Layer 1: Events → Layer 2: Agent CRD):</p>
<ol>
<li><strong>You&#39;ve completed Layer 1</strong> (kubectl events) - no errors found</li>
<li><strong>Next step is Layer 2</strong> (Agent CRD) - verify switch configuration</li>
</ol>
<p><strong>Why B is correct:</strong></p>
<ul>
<li>Agent CRD shows actual switch interface configuration</li>
<li>Reveals if VLANs are properly configured</li>
<li>Tests hypothesis: &quot;VLAN configuration issue&quot;</li>
<li>Follows systematic layered approach</li>
</ul>
<p><strong>Example commands:</strong></p>
<pre><code class=""language-bash""># Identify which switches server-03 and server-04 connect to
kubectl get vpcattachment prod-vpc-server-03 -o jsonpath=&#39;{.spec.connection}&#39;
kubectl get vpcattachment prod-vpc-server-04 -o jsonpath=&#39;{.spec.connection}&#39;

# Check Agent CRD for those switches
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq
kubectl get agent leaf-02 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet6}&#39; | jq

# Look for: VLAN configured, interface oper=up
</code></pre>
<p><strong>Why others are wrong:</strong></p>
<p><strong>A) Restart controller:</strong></p>
<ul>
<li>No evidence of controller failure</li>
<li>Premature action without diagnosis</li>
<li>Could disrupt fabric unnecessarily</li>
<li>VPCAttachments exist (controller is working)</li>
</ul>
<p><strong>C) Escalate to support:</strong></p>
<ul>
<li>Haven&#39;t completed basic troubleshooting</li>
<li>No evidence collected yet from Agent CRD or Grafana</li>
<li>Should check switch state first</li>
<li>Escalation should be last resort after diagnostic workflow</li>
</ul>
<p><strong>D) Delete and recreate VPCAttachments:</strong></p>
<ul>
<li>No diagnosis performed yet</li>
<li>May not fix underlying issue (e.g., VLAN conflict)</li>
<li>Could make troubleshooting harder (lose state)</li>
<li>Action without understanding root cause</li>
</ul>
<p><strong>Systematic approach:</strong> Complete evidence collection (all 4 layers) before taking corrective action.</p>
<p><strong>Module Reference:</strong> Module 4.1a - Concept 3: Diagnostic Workflow (Layer 2: Agent CRD Status)</p>
</details>

<hr>
<h3>Question 2: Common Failure Modes</h3>
<p><strong>Scenario:</strong> You observe these symptoms:</p>
<ul>
<li>VPCPeering between <code>vpc-a</code> and <code>vpc-b</code> exists</li>
<li>kubectl describe shows no errors</li>
<li>Server in <code>vpc-a</code> can ping its own gateway</li>
<li>Server in <code>vpc-a</code> CANNOT ping server in <code>vpc-b</code></li>
</ul>
<p>Which failure mode is most likely?</p>
<ul>
<li>A) BGP peering problem (sessions down)</li>
<li>B) VPC isolation settings or permit list misconfiguration</li>
<li>C) Interface errors (physical layer issue)</li>
<li>D) Configuration drift (GitOps reconciliation failure)</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) VPC isolation settings or permit list misconfiguration</p>
<p><strong>Explanation:</strong></p>
<p><strong>Symptoms indicate:</strong></p>
<ul>
<li>✅ Intra-VPC connectivity works (can ping gateway)</li>
<li>❌ Cross-VPC connectivity fails</li>
<li>✅ VPCPeering resource exists</li>
<li>✅ No error events</li>
</ul>
<p>This pattern strongly suggests a <strong>permit list issue</strong>.</p>
<p><strong>Why B is correct:</strong></p>
<p>VPCPeering failure modes include:</p>
<ol>
<li><p><strong>Permit list missing subnets</strong> - VPCPeering exists but doesn&#39;t include required subnets</p>
<pre><code class=""language-yaml"">spec:
  permit:
    - vpc-a: {subnets: [frontend]}  # Missing backend subnet!
      vpc-b: {subnets: [dmz]}
</code></pre>
</li>
<li><p><strong>VPC isolation=true without permit</strong> - Subnet marked isolated but no permit list entry</p>
<pre><code class=""language-yaml""># In vpc-a:
subnets:
  backend:
    isolated: true  # Isolated from other subnets
# But VPCPeering permit doesn&#39;t include backend → blocked
</code></pre>
</li>
<li><p><strong>Different IPv4Namespaces</strong> - VPCPeering requires same namespace</p>
<pre><code class=""language-bash"">kubectl get vpc vpc-a -o jsonpath=&#39;{.spec.ipv4Namespace}&#39;
# Output: production
kubectl get vpc vpc-b -o jsonpath=&#39;{.spec.ipv4Namespace}&#39;
# Output: development  # DIFFERENT! VPCPeering won&#39;t work
</code></pre>
</li>
</ol>
<p><strong>Diagnostic steps:</strong></p>
<pre><code class=""language-bash""># Check permit list
kubectl get vpcpeering vpc-a--vpc-b -o yaml

# Look for:
spec:
  permit:
    - vpc-a: {subnets: [...]}
      vpc-b: {subnets: [...]}

# Verify both subnets are included

# Check VPC isolation flags
kubectl get vpc vpc-a -o jsonpath=&#39;{.spec.subnets.*.isolated}&#39;
</code></pre>
<p><strong>Why others are wrong:</strong></p>
<p><strong>A) BGP peering problem:</strong></p>
<ul>
<li>Intra-VPC connectivity works, so fabric underlay BGP likely up</li>
<li>Would affect more than just cross-VPC traffic</li>
<li>Symptoms would include gateway unreachable</li>
<li>Check with: <code>kubectl get agent &lt;switch&gt; -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq</code></li>
</ul>
<p><strong>C) Interface errors:</strong></p>
<ul>
<li>Would affect intra-VPC connectivity too</li>
<li>Server can ping gateway (interface working)</li>
<li>Would see errors in Grafana Interface Dashboard</li>
<li>Symptoms: intermittent failures, packet loss</li>
</ul>
<p><strong>D) Configuration drift:</strong></p>
<ul>
<li>VPCPeering exists (not a sync issue)</li>
<li>No evidence of ArgoCD OutOfSync</li>
<li>kubectl describe shows no errors</li>
<li>Would see Warning events if reconciliation failed</li>
</ul>
<p><strong>Decision Tree:</strong> Use Decision Tree 2 (Cross-VPC Connectivity Fails) from Module 4.1a.</p>
<p><strong>Module Reference:</strong> Module 4.1a - Concept 2: Common Failure Modes</p>
</details>

<hr>
<h3>Question 3: Decision Trees</h3>
<p><strong>Scenario:</strong> Using Decision Tree 3 (&quot;VPCAttachment Shows Success But Doesn&#39;t Work&quot;), you&#39;ve verified:</p>
<ul>
<li>VPCAttachment references correct connection ✅</li>
<li>Subnet exists in VPC ✅</li>
<li>Agent CRD shows VLAN configured on interface ✅</li>
</ul>
<p>According to the decision tree, what should you check NEXT?</p>
<ul>
<li>A) Controller logs for reconciliation errors</li>
<li>B) Grafana Interface Dashboard for errors</li>
<li>C) nativeVLAN setting matches server expectation</li>
<li>D) Escalate immediately (all checks passed)</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> C) nativeVLAN setting matches server expectation</p>
<p><strong>Explanation:</strong></p>
<p><strong>Decision Tree 3 path:</strong></p>
<pre><code>VPCAttachment shows success but doesn&#39;t work
  ↓
Verify connection reference ✅ (already checked)
  ↓
Verify subnet exists ✅ (already checked)
  ↓
Check Agent CRD for VLAN ✅ (already checked)
  ↓
→ Check nativeVLAN setting ← YOU ARE HERE
  ↓
Escalate if still unresolved
</code></pre>
<p><strong>Why C is correct:</strong></p>
<p><strong>nativeVLAN mismatch is a common issue:</strong></p>
<p><strong>Scenario 1: VPCAttachment expects tagged, server expects untagged</strong></p>
<pre><code class=""language-yaml""># VPCAttachment
spec:
  nativeVLAN: false  # Switch sends tagged VLAN 1010 traffic

# Server interface (expects untagged)
# Interface: enp2s1 (no VLAN subinterface)
# Result: Server sees VLAN-tagged frames, doesn&#39;t process them → no connectivity
</code></pre>
<p><strong>Scenario 2: VPCAttachment expects untagged, server expects tagged</strong></p>
<pre><code class=""language-yaml""># VPCAttachment
spec:
  nativeVLAN: true  # Switch sends untagged traffic

# Server interface (expects tagged)
# Interface: enp2s1.1010 (VLAN subinterface)
# Result: Server expects VLAN tag, receives untagged → no connectivity
</code></pre>
<p><strong>How to check:</strong></p>
<pre><code class=""language-bash""># VPCAttachment nativeVLAN setting
kubectl get vpcattachment customer-app-vpc-server-07 -o jsonpath=&#39;{.spec.nativeVLAN}&#39;
# Output: false (tagged) or true (untagged)

# Server interface configuration (SSH to server)
ip link show
# Look for:
# enp2s1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;  ← Untagged interface
# enp2s1.1010: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;  ← Tagged interface

# If VPCAttachment nativeVLAN=false, server should have enp2s1.1010
# If VPCAttachment nativeVLAN=true, server should have enp2s1 (no subinterface)
</code></pre>
<p><strong>Resolution:</strong></p>
<pre><code class=""language-bash""># Option 1: Update VPCAttachment to match server
# In Gitea: change nativeVLAN setting

# Option 2: Update server interface configuration
# Add VLAN subinterface or remove it to match VPCAttachment
</code></pre>
<p><strong>Why others are wrong:</strong></p>
<p><strong>A) Controller logs:</strong></p>
<ul>
<li>Agent CRD shows VLAN configured (reconciliation succeeded)</li>
<li>Logs won&#39;t reveal server-side configuration issue</li>
<li>Controller successfully applied configuration</li>
<li>No error events (controller perspective is success)</li>
</ul>
<p><strong>B) Grafana Interface Dashboard:</strong></p>
<ul>
<li>Already verified VLAN configured via Agent CRD</li>
<li>Physical layer likely working (VLAN present)</li>
<li>Doesn&#39;t check nativeVLAN setting (tagged vs. untagged)</li>
<li>Grafana shows interface up, VLAN configured—appears healthy</li>
</ul>
<p><strong>D) Escalate immediately:</strong></p>
<ul>
<li>Decision tree not complete yet</li>
<li>One more hypothesis to test (nativeVLAN)</li>
<li>Premature escalation</li>
<li>Should exhaust decision tree before escalating</li>
</ul>
<p><strong>Best Practice:</strong></p>
<p>Always check nativeVLAN when:</p>
<ul>
<li>VPCAttachment exists, no errors</li>
<li>VLAN configured correctly</li>
<li>Interface up</li>
<li>Server still has no connectivity</li>
</ul>
<p>This is a <strong>configuration mismatch</strong> between VPCAttachment and server—easy to overlook but common in practice.</p>
<p><strong>Module Reference:</strong> Module 4.1a - Concept 4: Decision Trees (Decision Tree 3)</p>
</details>

<hr>
<h3>Question 4: Diagnostic Workflow</h3>
<p><strong>Scenario:</strong> You&#39;re investigating a connectivity issue. You&#39;ve checked kubectl events (no errors) and Agent CRD (all interfaces up, VLANs configured correctly). Server still cannot communicate.</p>
<p>Why should you check Grafana BEFORE checking controller logs?</p>
<ul>
<li>A) Grafana is faster to load than kubectl logs</li>
<li>B) Grafana provides visual trends and historical context (e.g., intermittent errors over time)</li>
<li>C) Controller logs are unreliable</li>
<li>D) Grafana is always the first troubleshooting step</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Grafana provides visual trends and historical context (e.g., intermittent errors over time)</p>
<p><strong>Explanation:</strong></p>
<p><strong>Diagnostic Workflow Order:</strong></p>
<ol>
<li><strong>kubectl events</strong> - Current errors and warnings (fast check)</li>
<li><strong>Agent CRD</strong> - Current switch state (detailed config)</li>
<li><strong>Grafana</strong> - Historical trends and visual patterns ← YOU ARE HERE</li>
<li><strong>Controller logs</strong> - Reconciliation details (specific events)</li>
</ol>
<p><strong>Why Grafana before controller logs:</strong></p>
<p><strong>Grafana reveals patterns that kubectl cannot:</strong></p>
<p><strong>Example 1: Intermittent Interface Errors</strong></p>
<pre><code>Agent CRD (right now):
- Interface Ethernet8: oper=up, ine=0, oute=0
- Looks healthy!

Grafana Interface Dashboard (last 6 hours):
- 10:00 AM: 0 errors
- 11:30 AM: Spike to 10,000 input errors
- 12:00 PM: Back to 0 errors
- Pattern: Intermittent issue, not current state problem
</code></pre>
<p><strong>Without Grafana:</strong> You see current state (healthy) and miss the intermittent errors.</p>
<p><strong>With Grafana:</strong> You see the spike and know to investigate physical layer or MTU issues.</p>
<p><strong>Example 2: BGP Flapping</strong></p>
<pre><code>Agent CRD (right now):
- BGP neighbor 172.30.128.10: state=established
- Looks healthy!

Grafana Fabric Dashboard (last 24 hours):
- BGP session up/down 15 times
- Pattern: Flapping session, unstable routing
</code></pre>
<p><strong>Without Grafana:</strong> You see current state (established) and miss the instability.</p>
<p><strong>With Grafana:</strong> You see the flapping and investigate route filtering or keepalive issues.</p>
<p><strong>Example 3: Traffic Patterns</strong></p>
<pre><code>Agent CRD (right now):
- Interface Ethernet8: oper=up
- Counters: inb=123456, outb=654321

Grafana Interface Dashboard (last 1 hour):
- Traffic: Zero bytes in/out for entire hour
- Pattern: Interface up but no traffic (server not sending)
</code></pre>
<p><strong>Without Grafana:</strong> You see non-zero counters (historical total) and assume traffic flowing.</p>
<p><strong>With Grafana:</strong> You see zero current traffic and know server-side issue.</p>
<p><strong>Controller logs:</strong></p>
<ul>
<li>Show reconciliation events (discrete actions)</li>
<li>Don&#39;t show operational metrics over time</li>
<li>Useful for understanding &quot;why did controller make this decision?&quot;</li>
<li>Not useful for &quot;is this interface flapping over time?&quot;</li>
</ul>
<p><strong>Example controller log:</strong></p>
<pre><code>2025-10-17T10:15:00Z INFO Reconciling VPCAttachment customer-app-vpc-server-07
2025-10-17T10:15:01Z INFO VLAN 1020 configured on leaf-04/Ethernet8
2025-10-17T10:15:02Z INFO Reconciliation successful
</code></pre>
<p>Logs show discrete reconciliation success, not ongoing operational state.</p>
<p><strong>Why others are wrong:</strong></p>
<p><strong>A) Grafana is faster:</strong></p>
<ul>
<li>Not the reason for ordering</li>
<li>kubectl can be just as fast (or faster)</li>
<li>Speed is not the primary consideration</li>
<li>Order is about information type, not speed</li>
</ul>
<p><strong>C) Controller logs unreliable:</strong></p>
<ul>
<li><strong>False</strong> - controller logs are critical for reconciliation debugging</li>
<li>Just not useful for trending/historical analysis</li>
<li>Logs are reliable but serve different purpose</li>
<li>Use logs for &quot;why did controller fail?&quot; not &quot;is interface flapping?&quot;</li>
</ul>
<p><strong>D) Grafana always first:</strong></p>
<ul>
<li><strong>False</strong> - kubectl events should be first (fastest check for errors)</li>
<li>Correct order: Events → Agent CRD → Grafana → Logs</li>
<li>Grafana is Layer 3, not Layer 1</li>
<li>Events catch most configuration errors immediately</li>
</ul>
<p><strong>When to use each tool:</strong></p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>When to Use</th>
</tr>
</thead>
<tbody><tr>
<td>kubectl events</td>
<td>First check: Are there any error events?</td>
</tr>
<tr>
<td>Agent CRD</td>
<td>Second: What is current switch state?</td>
</tr>
<tr>
<td>Grafana</td>
<td>Third: Are there patterns over time? Intermittent issues?</td>
</tr>
<tr>
<td>Controller logs</td>
<td>Fourth: Why did reconciliation fail or succeed?</td>
</tr>
</tbody></table>
<p><strong>Practical Example:</strong></p>
<p><strong>Scenario:</strong> Server reports &quot;occasional packet loss&quot;</p>
<ol>
<li><strong>kubectl events:</strong> No errors (rules out configuration issue)</li>
<li><strong>Agent CRD:</strong> Interface up, 0 errors right now (looks healthy)</li>
<li><strong>Grafana:</strong> Shows error spike every 15 minutes for last 6 hours → <strong>Root cause visible!</strong></li>
<li><strong>Controller logs:</strong> Not needed (issue is operational, not reconciliation)</li>
</ol>
<p><strong>Grafana revealed the intermittent pattern that Agent CRD (current state) missed.</strong></p>
<p><strong>Module Reference:</strong> Module 4.1a - Concept 3: Diagnostic Workflow (Layer 3: Grafana Dashboards)</p>
</details>

<hr>
<h2>Conclusion</h2>
<p>You&#39;ve completed Module 4.1b: Hands-On Fabric Diagnosis Lab!</p>
<h3>What You Learned</h3>
<p><strong>Practical Application:</strong></p>
<ul>
<li>Applied hypothesis-driven investigation to real scenario</li>
<li>Used layered diagnostic workflow (Events → Agent CRD → Grafana)</li>
<li>Followed decision trees for structured diagnosis</li>
<li>Identified VLAN mismatch through systematic testing</li>
</ul>
<p><strong>Troubleshooting Skills:</strong></p>
<ul>
<li>Form and test hypotheses systematically</li>
<li>Eliminate possibilities through evidence</li>
<li>Document findings clearly</li>
<li>Propose multiple solution options</li>
</ul>
<p><strong>Time Efficiency:</strong></p>
<ul>
<li>Diagnosed issue in 7-8 minutes using systematic approach</li>
<li>Contrast: Random checking could take 30+ minutes without success</li>
<li>Systematic methodology saves time and ensures thorough diagnosis</li>
</ul>
<h3>Key Takeaways</h3>
<ol>
<li><p><strong>Systematic approach is faster</strong> - 7-8 minutes with methodology vs. 30+ minutes randomly checking</p>
</li>
<li><p><strong>Hypothesis elimination is progress</strong> - Each test narrows the problem space</p>
</li>
<li><p><strong>VLAN conflicts are subtle</strong> - Controller shows success but configuration doesn&#39;t match expectation</p>
</li>
<li><p><strong>Documentation enables handoff</strong> - Clear problem statement and solution options support team collaboration</p>
</li>
<li><p><strong>Prevention matters</strong> - Document lessons to prevent recurrence</p>
</li>
</ol>
<h3>Troubleshooting Mindset</h3>
<p>As you continue operating Hedgehog fabrics:</p>
<ul>
<li><strong>Stay systematic:</strong> Don&#39;t jump to conclusions based on hunches</li>
<li><strong>Test hypotheses:</strong> Verify assumptions with evidence</li>
<li><strong>Document findings:</strong> Track what you&#39;ve checked to avoid re-work</li>
<li><strong>Think about &quot;why&quot;:</strong> Understand the cause, not just the symptom</li>
<li><strong>Iterate when needed:</strong> If all hypotheses eliminated, form new ones based on evidence</li>
</ul>
<h3>Course 4 Progress</h3>
<p><strong>Completed:</strong></p>
<ul>
<li>✅ Module 4.1a: Systematic Troubleshooting Framework</li>
<li>✅ Module 4.1b: Hands-On Fabric Diagnosis Lab</li>
</ul>
<p><strong>Up Next:</strong></p>
<ul>
<li>Module 4.2: Rollback and Recovery (safe undo procedures, handling stuck resources)</li>
<li>Module 4.3: Coordinating with Support (effective tickets, working with engineers)</li>
<li>Module 4.4: Post-Incident Review (documentation, prevention, knowledge sharing)</li>
</ul>
<hr>
<p><strong>You&#39;re now equipped to diagnose fabric issues systematically and confidently. See you in Module 4.2!</strong></p>
",402,8,,,"hedgehog,fabric,troubleshooting,diagnostics,hands-on,lab"
198563771474,Post-Incident Review,fabric-operations-post-incident-review,[object Object],"Conduct blameless post-incident reviews, document lessons learned, and build operational knowledge for continuous improvement.","<h2>Introduction</h2>
<p>You&#39;ve completed an incident:</p>
<ul>
<li>Diagnosed the issue (Module 4.1)</li>
<li>Rolled back the problematic change (Module 4.2)</li>
<li>Worked with support (Module 4.3)</li>
<li>Resolved the outage</li>
</ul>
<p><strong>Most teams stop here.</strong> Incident resolved, move on to the next task.</p>
<p><strong>High-performing teams add one more step:</strong> Post-incident review.</p>
<h3>Learning From What Went Wrong</h3>
<p><strong>Why conduct post-incident reviews?</strong></p>
<ul>
<li><strong>Incidents are expensive learning opportunities</strong> - Already paid the cost (downtime, lost productivity), maximize the learning</li>
<li><strong>Same issues recur if root causes aren&#39;t addressed</strong> - Fixing symptoms doesn&#39;t prevent recurrence</li>
<li><strong>Team knowledge improves when insights are shared</strong> - One person&#39;s lesson becomes everyone&#39;s knowledge</li>
<li><strong>Process gaps become visible through reflection</strong> - See systemic issues that daily operations hide</li>
</ul>
<p><strong>Traditional response to incidents:</strong></p>
<pre><code>Incident occurs → Firefight → Resolve → Blame someone → Move on → Same incident recurs
</code></pre>
<p><strong>High-performing team response:</strong></p>
<pre><code>Incident occurs → Firefight → Resolve → Post-Incident Review → Document learnings →
Update processes → Implement improvements → Incident prevented/easier next time
</code></pre>
<h3>SRE Culture - Blameless Reviews</h3>
<p>Site Reliability Engineering (SRE) teaches:</p>
<blockquote>
<p><strong>&quot;Failure is inevitable in complex systems. Learning from failure is optional.&quot;</strong></p>
</blockquote>
<p>Post-incident reviews (PIRs) embody SRE principles:</p>
<p><strong>Blameless culture:</strong></p>
<ul>
<li>Focus on systems and processes, not individuals</li>
<li>&quot;What allowed this mistake?&quot; not &quot;Who made this mistake?&quot;</li>
<li>Psychological safety to report issues honestly</li>
</ul>
<p><strong>Continuous improvement:</strong></p>
<ul>
<li>Every incident improves operations</li>
<li>Track patterns to identify systemic issues</li>
<li>Measure improvement over time</li>
</ul>
<p><strong>Shared learning:</strong></p>
<ul>
<li>Team knowledge grows through documentation</li>
<li>New operators learn from past incidents</li>
<li>Organizational memory prevents forgotten lessons</li>
</ul>
<p><strong>Forward-looking:</strong></p>
<ul>
<li>&quot;How do we prevent this?&quot; not &quot;Who caused this?&quot;</li>
<li>Create actionable improvements</li>
<li>Update runbooks and processes</li>
</ul>
<h3>What You&#39;ll Learn</h3>
<p><strong>Blameless Post-Incident Review:</strong></p>
<ul>
<li>Creating factual timelines (what happened, when)</li>
<li>Identifying root cause with 5 Whys technique</li>
<li>Distinguishing root cause from proximate cause</li>
<li>Facilitating reviews without blame</li>
</ul>
<p><strong>Action Item Creation:</strong></p>
<ul>
<li>SMART action items (Specific, Measurable, Actionable, Relevant, Time-bound)</li>
<li>Categorizing improvements (immediate, short-term, long-term)</li>
<li>Assigning ownership and due dates</li>
<li>Tracking completion</li>
</ul>
<p><strong>Operational Knowledge Management:</strong></p>
<ul>
<li>Updating runbooks based on incidents</li>
<li>Building troubleshooting guides</li>
<li>Creating searchable PIR repository</li>
<li>Sharing learnings with team</li>
</ul>
<h3>Module Scenario</h3>
<p>You&#39;ll conduct a post-incident review for the VLAN conflict issue from Modules 4.1-4.3:</p>
<ul>
<li>Document complete timeline from detection to resolution</li>
<li>Identify root cause using 5 Whys</li>
<li>Extract lessons learned</li>
<li>Create 2-3 actionable improvements with SMART criteria</li>
</ul>
<p>By the end of this module, you&#39;ll complete the incident lifecycle and contribute to continuous improvement culture.</p>
<hr>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Conduct blameless post-incident reviews</strong> - Facilitate reviews focused on learning, not blaming</li>
<li><strong>Document lessons learned</strong> - Create actionable improvement items from incidents</li>
<li><strong>Update operational runbooks</strong> - Improve documentation based on incident experiences</li>
<li><strong>Identify systemic improvements</strong> - Recognize patterns that require process or tool changes</li>
<li><strong>Build operational knowledge</strong> - Contribute to team learning and continuous improvement</li>
</ol>
<hr>
<h2>Prerequisites</h2>
<p>Before starting this module, you should have:</p>
<p><strong>Completed Modules:</strong></p>
<ul>
<li>Module 4.1: Diagnosing Fabric Issues (provides incident to review)</li>
<li>Module 4.2: Rollback &amp; Recovery (resolution actions documented)</li>
<li>Module 4.3: Coordinating with Support (escalation timeline)</li>
<li>All previous courses (Courses 1-3)</li>
</ul>
<p><strong>Understanding:</strong></p>
<ul>
<li>Incident lifecycle (detection → diagnosis → resolution)</li>
<li>Troubleshooting methodology</li>
<li>GitOps rollback procedures</li>
<li>Support escalation process</li>
</ul>
<p><strong>Context:</strong></p>
<ul>
<li>This module reviews the VLAN conflict incident from Modules 4.1-4.3</li>
<li>You&#39;ll use real incident data to practice PIR skills</li>
</ul>
<hr>
<h2>Scenario</h2>
<p><strong>Incident Summary (from Modules 4.1-4.3):</strong></p>
<p><strong>What happened:</strong></p>
<ul>
<li>VPCAttachment <code>customer-app-vpc-server-07</code> created successfully (no error events)</li>
<li>Server-07 had no connectivity within VPC</li>
<li>Root cause: VLAN mismatch (VPC expected 1025, switch configured with 1020)</li>
<li>Attempted fix: Update VPC to VLAN 1020 → Failed with &quot;VLAN reserved for system use&quot; error</li>
<li>Resolution: Escalated to support, updated VPC to VLAN 1030, connectivity restored</li>
</ul>
<p><strong>Timeline:</strong></p>
<ul>
<li>10:00 UTC: VPCAttachment created</li>
<li>10:07 UTC: Issue reported (no connectivity)</li>
<li>10:15-10:45 UTC: Investigation and diagnosis</li>
<li>11:00 UTC: Attempted self-resolution (failed)</li>
<li>11:30 UTC: Escalated to support</li>
<li>14:00 UTC: Support responded with solution</li>
<li>14:20 UTC: Incident resolved</li>
</ul>
<p><strong>Duration:</strong> 4 hours 20 minutes (detection to resolution)</p>
<p><strong>Your Task:</strong></p>
<p>Conduct a complete post-incident review to extract learnings and create improvements that prevent recurrence.</p>
<hr>
<h2>Core Concepts &amp; Deep Dive</h2>
<h3>Concept 1: Blameless Culture</h3>
<h4>What is a Blameless Culture?</h4>
<p><strong>Traditional approach (blame-focused):</strong></p>
<ul>
<li>&quot;Who made the mistake?&quot;</li>
<li>&quot;Why didn&#39;t they check before applying the change?&quot;</li>
<li>&quot;This person needs retraining.&quot;</li>
<li>Focus on individual actions</li>
</ul>
<p><strong>Blameless approach (systems-focused):</strong></p>
<ul>
<li>&quot;Why did the system allow this mistake?&quot;</li>
<li>&quot;What process would have caught this earlier?&quot;</li>
<li>&quot;How do we make the right thing easy to do?&quot;</li>
<li>Focus on systemic improvements</li>
</ul>
<h4>Key Principles</h4>
<p><strong>Principle 1: Systems Thinking</strong></p>
<p>Incidents result from <strong>system failures</strong>, not individual failures.</p>
<p><strong>System failures include:</strong></p>
<ul>
<li><strong>Process gaps</strong> - No VLAN conflict checking before VPC creation</li>
<li><strong>Tool limitations</strong> - VLANNamespace doesn&#39;t show which VLANs are in use</li>
<li><strong>Documentation gaps</strong> - VLAN reservation ranges not documented</li>
<li><strong>Design issues</strong> - No validation that prevents conflicts at creation time</li>
</ul>
<p><strong>Human error is a symptom, not a root cause.</strong></p>
<p>When someone makes a &quot;mistake,&quot; ask:</p>
<ul>
<li>What allowed that mistake to reach production?</li>
<li>What tools or checks would have caught it earlier?</li>
<li>How do we make the correct action the easy action?</li>
</ul>
<p><strong>Example:</strong></p>
<p><strong>Blame-focused question:</strong></p>
<blockquote>
<p>&quot;Why did Alice commit the wrong VLAN to Git?&quot;</p>
</blockquote>
<p><strong>Systems-focused question:</strong></p>
<blockquote>
<p>&quot;Why doesn&#39;t our Git workflow validate VLAN conflicts before accepting commits?&quot;</p>
</blockquote>
<p>The second question leads to actionable improvements (pre-commit hooks, CI/CD validation). The first question leads nowhere useful.</p>
<hr>
<p><strong>Principle 2: Forward-Looking Questions</strong></p>
<p><strong>Avoid backward-looking blame:</strong></p>
<ul>
<li>&quot;Who committed the broken YAML?&quot;</li>
<li>&quot;Why didn&#39;t you test before pushing to prod?&quot;</li>
<li>&quot;How could you not know VLAN 1025 was in use?&quot;</li>
</ul>
<p><strong>Ask forward-looking improvement questions:</strong></p>
<ul>
<li>&quot;What would have caught this error earlier in the process?&quot;</li>
<li>&quot;How can we make VLAN selection less error-prone?&quot;</li>
<li>&quot;What tools or checks would prevent this in the future?&quot;</li>
</ul>
<p><strong>Difference:</strong></p>
<table>
<thead>
<tr>
<th>Blame Question</th>
<th>Forward-Looking Question</th>
<th>Leads To</th>
</tr>
</thead>
<tbody><tr>
<td>&quot;Who broke it?&quot;</td>
<td>&quot;What in our process allowed this?&quot;</td>
<td>Process improvement</td>
</tr>
<tr>
<td>&quot;Why didn&#39;t you check?&quot;</td>
<td>&quot;How do we make checking automatic?&quot;</td>
<td>Tool development</td>
</tr>
<tr>
<td>&quot;You should have known.&quot;</td>
<td>&quot;How do we make knowledge explicit?&quot;</td>
<td>Documentation update</td>
</tr>
</tbody></table>
<hr>
<p><strong>Principle 3: Psychological Safety</strong></p>
<p>Teams with blameless culture exhibit:</p>
<ul>
<li><strong>Report incidents honestly</strong> - Don&#39;t hide issues out of fear</li>
<li><strong>Share near-misses</strong> - &quot;I almost made this mistake&quot; is valuable learning</li>
<li><strong>Ask for help early</strong> - Not seen as weakness</li>
<li><strong>Experiment with improvements</strong> - Failure during experimentation is acceptable</li>
<li><strong>Admit mistakes quickly</strong> - Reduces mean time to resolution</li>
</ul>
<p><strong>Without psychological safety:</strong></p>
<ul>
<li><strong>Incidents get hidden</strong> - Fear of blame prevents reporting</li>
<li><strong>Knowledge isn&#39;t shared</strong> - Protective, siloed behavior</li>
<li><strong>Improvements don&#39;t happen</strong> - Risk-averse culture avoids change</li>
<li><strong>Mean time to resolution increases</strong> - People hesitate to escalate</li>
<li><strong>Recurring incidents</strong> - Lessons learned aren&#39;t captured or shared</li>
</ul>
<p><strong>Building psychological safety in PIRs:</strong></p>
<ul>
<li>Facilitator sets blameless tone at start</li>
<li>Redirect blame to system focus</li>
<li>Celebrate learning and honesty</li>
<li>Thank participants for sharing insights</li>
<li>Document improvements, not individual mistakes</li>
</ul>
<hr>
<h3>Concept 2: Post-Incident Review Template</h3>
<h4>PIR Structure (4 Sections)</h4>
<p><strong>Section 1: What Happened? (Timeline)</strong></p>
<p>Document the incident chronologically with factual observations.</p>
<p><strong>Example Timeline:</strong></p>
<pre><code>2025-10-17 10:00 UTC - VPCAttachment customer-app-vpc-server-07 created via Gitea commit
2025-10-17 10:05 UTC - ArgoCD synced VPCAttachment to cluster successfully
2025-10-17 10:07 UTC - Developer reported: server-07 no connectivity to VPC gateway
2025-10-17 10:15 UTC - Investigation started (checked kubectl events - no errors found)
2025-10-17 10:30 UTC - Agent CRD checked: VLAN 1020 on leaf-04/Ethernet8 (VPC expects 1025)
2025-10-17 10:45 UTC - Root cause identified: VLAN conflict (1025 in use, system allocated 1020)
2025-10-17 11:00 UTC - Attempted fix: Updated VPC YAML to VLAN 1020
2025-10-17 11:05 UTC - ArgoCD sync failed: &quot;VLAN 1020 reserved for system use&quot;
2025-10-17 11:30 UTC - Escalated to support (P2 ticket) with complete diagnostics
2025-10-17 14:00 UTC - Support responded: VLANs 1020-1029 reserved, use 1030+
2025-10-17 14:15 UTC - Updated VPC to VLAN 1030, ArgoCD synced successfully
2025-10-17 14:20 UTC - Connectivity verified, incident resolved
</code></pre>
<p><strong>Goal:</strong> Factual timeline with timestamps. No interpretation, blame, or conclusions yet.</p>
<p><strong>Metrics:</strong></p>
<ul>
<li><strong>Mean Time to Detect (MTTD):</strong> 7 minutes (10:00 creation → 10:07 reported)</li>
<li><strong>Mean Time to Resolve (MTTR):</strong> 4 hours 13 minutes (10:07 detected → 14:20 resolved)</li>
</ul>
<hr>
<p><strong>Section 2: Why Did It Happen? (Root Cause)</strong></p>
<p>Identify underlying cause using structured technique. Don&#39;t stop at first obvious answer.</p>
<p><strong>5 Whys Technique:</strong></p>
<p>Start with symptom, ask &quot;why&quot; five times to reach root cause:</p>
<p><strong>1. Why did server-07 have no connectivity?</strong></p>
<ul>
<li>Because VPC VLAN (1025) didn&#39;t match switch VLAN (1020)</li>
</ul>
<p><strong>2. Why didn&#39;t VPC VLAN match switch VLAN?</strong></p>
<ul>
<li>Because VLAN 1025 was already in use by another VPC, so system auto-allocated 1020</li>
</ul>
<p><strong>3. Why wasn&#39;t VLAN conflict detected before VPC creation?</strong></p>
<ul>
<li>Because VLANNamespace doesn&#39;t validate VLAN availability at creation time</li>
</ul>
<p><strong>4. Why doesn&#39;t VLANNamespace validate VLAN conflicts?</strong></p>
<ul>
<li>Because it defines ranges, not tracks usage (current design limitation)</li>
</ul>
<p><strong>5. Why wasn&#39;t VLAN reservation (1020-1029) documented for operators?</strong></p>
<ul>
<li>Because system reservations aren&#39;t exposed via API or operator documentation</li>
</ul>
<p><strong>Root Cause:</strong></p>
<p>No pre-creation validation for VLAN conflicts. VLANNamespace allows VLAN selection from range without checking current usage or communicating reserved ranges.</p>
<p><strong>Contributing Factors:</strong></p>
<ul>
<li>Documentation gap: Reserved VLANs (1020-1029) not listed in runbook</li>
<li>Operator knowledge gap: Didn&#39;t know to manually check existing VPC VLANs first</li>
<li>Error message unclear: &quot;Reserved for system use&quot; but no list of reserved VLANs provided</li>
<li>No tooling to show available VLANs programmatically</li>
</ul>
<p><strong>Root Cause vs. Proximate Cause:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example</th>
<th>Can Recur?</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Proximate Cause</strong></td>
<td>Immediate trigger</td>
<td>Operator chose VLAN 1025</td>
<td>Yes - next operator can make same choice</td>
</tr>
<tr>
<td><strong>Root Cause</strong></td>
<td>Systemic issue</td>
<td>No VLAN conflict validation</td>
<td>No - if fixed, prevents recurrence</td>
</tr>
</tbody></table>
<p><strong>Always address root cause, not just proximate cause.</strong></p>
<hr>
<p><strong>Section 3: How Was It Resolved? (Actions Taken)</strong></p>
<p>Document resolution path with what worked and what could improve.</p>
<p><strong>Immediate Actions:</strong></p>
<ol>
<li>Diagnosed VLAN mismatch using Agent CRD inspection (10:30 UTC)</li>
<li>Attempted self-resolution by updating VPC VLAN to 1020 (11:00 UTC)</li>
<li>Encountered &quot;reserved VLAN&quot; error blocking self-resolution (11:05 UTC)</li>
<li>Escalated to support with complete diagnostics (11:30 UTC)</li>
<li>Received clarification from support: VLANs 1020-1029 reserved (14:00 UTC)</li>
<li>Updated VPC to VLAN 1030 and synced via ArgoCD (14:15 UTC)</li>
<li>Verified connectivity restored (14:20 UTC)</li>
</ol>
<p><strong>What Worked Well:</strong></p>
<ul>
<li>Systematic troubleshooting using Module 4.1 methodology identified root cause quickly</li>
<li>Complete diagnostic bundle attachment enabled fast support response</li>
<li>ArgoCD GitOps workflow made rollback/update safe and auditable</li>
</ul>
<p><strong>What Could Be Improved:</strong></p>
<ul>
<li>Earlier escalation after first failed fix attempt (11:05) could have saved ~3 hours</li>
<li>Documentation of reserved VLANs would have prevented initial issue</li>
<li>VLAN validation tooling would catch conflicts before creation</li>
</ul>
<p><strong>Time Breakdown:</strong></p>
<ul>
<li><strong>Detection:</strong> 7 minutes (fast - developer reported immediately)</li>
<li><strong>Diagnosis:</strong> 38 minutes (10:15-10:53, systematic troubleshooting)</li>
<li><strong>Failed self-resolution:</strong> 35 minutes (11:00-11:35)</li>
<li><strong>Waiting for support:</strong> 2 hours 30 minutes (11:30-14:00)</li>
<li><strong>Resolution application:</strong> 20 minutes (14:00-14:20)</li>
</ul>
<p><strong>Opportunities:</strong> Biggest time saver would be preventing issue entirely through VLAN validation or earlier escalation recognition.</p>
<hr>
<p><strong>Section 4: How Do We Prevent Recurrence? (Improvements)</strong></p>
<p>Create actionable improvements categorized by timeline.</p>
<p><strong>Immediate (Do This Week):</strong></p>
<ol>
<li>Update operator runbook: Document reserved VLAN ranges (1020-1029)</li>
<li>Create VLAN selection checklist: Check existing VPCs, avoid reserved ranges, document choices</li>
<li>Add VLAN conflict troubleshooting example to diagnostic guide</li>
</ol>
<p><strong>Short-Term (Do This Month):</strong>
4. File Hedgehog GitHub issue requesting documentation of reserved VLANs in official docs
5. Create kubectl helper script (show-available-vlans.sh) to list reserved, used, and available VLANs</p>
<p><strong>Long-Term (Product Team Proposals):</strong>
6. Feature request: VLANNamespace API to expose available VLANs with validation
7. Feature request: VPC creation pre-validation with clear error messages and suggestions</p>
<p><strong>Prioritization:</strong> Focus on immediate/short-term (operator-actionable). Long-term documents desired product improvements.</p>
<hr>
<h3>Concept 3: Creating Actionable Improvements</h3>
<h4>What Makes a Good Action Item?</h4>
<p><strong>Bad Action Items (Too Vague):</strong></p>
<ul>
<li>&quot;Be more careful with VLAN selection&quot; ❌ Not specific, not measurable</li>
<li>&quot;Check things before creating VPCs&quot; ❌ Not actionable (&quot;things&quot;?)</li>
<li>&quot;Improve documentation&quot; ❌ Not time-bound or owned</li>
</ul>
<p><strong>Good Action Items (Specific, Measurable, Owned):</strong></p>
<ul>
<li>&quot;Document reserved VLANs 1020-1029 in operator runbook section &#39;VLAN Selection Guidelines&#39; (Owner: Alice, Due: Oct 20)&quot; ✅</li>
<li>&quot;Create show-available-vlans.sh script and commit to kubectl-fabric-helpers repo (Owner: Bob, Due: Oct 25)&quot; ✅</li>
<li>&quot;File Hedgehog GitHub issue requesting VLAN validation feature with use case and examples (Owner: Charlie, Due: Oct 18)&quot; ✅</li>
</ul>
<h4>SMART Criteria</h4>
<p><strong>S - Specific:</strong></p>
<ul>
<li>What exactly will be done?</li>
<li>Where will it be documented/implemented?</li>
<li>What content will be included?</li>
</ul>
<p><strong>M - Measurable:</strong></p>
<ul>
<li>How will we know it&#39;s complete?</li>
<li>What are the success criteria?</li>
<li>Can we verify completion objectively?</li>
</ul>
<p><strong>A - Actionable:</strong></p>
<ul>
<li>Can someone actually do this?</li>
<li>Do they have the tools/access needed?</li>
<li>Is the action clear?</li>
</ul>
<p><strong>R - Relevant:</strong></p>
<ul>
<li>Does this prevent recurrence?</li>
<li>Does it address root cause or contributing factor?</li>
<li>Is it worth the effort?</li>
</ul>
<p><strong>T - Time-bound:</strong></p>
<ul>
<li>When will it be done?</li>
<li>Is the deadline realistic?</li>
<li>Does priority match urgency?</li>
</ul>
<h4>SMART Action Item Example</h4>
<pre><code class=""language-markdown"">Title: Document Reserved VLAN Ranges
Owner: Alice Thompson
Due Date: 2025-10-20
Priority: High

Description: Update operator runbook section &quot;VLAN Selection Guidelines&quot; documenting reserved VLAN ranges (1020-1029), kubectl command to check existing VLANs, recommended selection procedure, and troubleshooting tips.

Success Criteria:
- Runbook section created with reserved VLANs documented
- kubectl command example included
- Committed to docs repository

Status: In Progress
</code></pre>
<p><strong>Key elements:</strong> Specific owner and deadline, measurable success criteria, actionable steps.</p>
<hr>
<h3>Concept 4: Operational Knowledge Management</h3>
<h4>Building Team Knowledge</h4>
<p>Post-incident reviews create <strong>organizational memory</strong>—knowledge that persists beyond individual team members.</p>
<p><strong>Knowledge Artifacts:</strong></p>
<p><strong>1. PIR Documents</strong></p>
<ul>
<li>Incident history and timeline</li>
<li>Root cause analysis</li>
<li>Lessons learned</li>
<li>Action items tracked to completion</li>
</ul>
<p><strong>2. Updated Runbooks</strong></p>
<ul>
<li>Procedures improved based on real experience</li>
<li>New troubleshooting steps added</li>
<li>Known issues documented with workarounds</li>
</ul>
<p><strong>3. Troubleshooting Guides</strong></p>
<ul>
<li>Common failure modes and solutions</li>
<li>Decision trees for diagnosis</li>
<li>Quick reference commands</li>
</ul>
<p><strong>4. Known Issues List</strong></p>
<ul>
<li>Product limitations with workarounds</li>
<li>Configuration gotchas</li>
<li>Environment-specific quirks</li>
</ul>
<h4>Knowledge Sharing Practices</h4>
<p><strong>1. Team PIR Review Meetings (15-30 min)</strong></p>
<ul>
<li>Present PIR findings to team</li>
<li>Discuss action items</li>
<li>Share insights and questions</li>
<li>Build shared understanding</li>
</ul>
<p><strong>2. Documentation Repository</strong></p>
<ul>
<li>Git repository for PIRs and runbooks</li>
<li>Searchable by date, tag, or keyword</li>
<li>Indexed for easy navigation</li>
</ul>
<p><strong>3. New Operator Onboarding</strong></p>
<ul>
<li>Review past PIRs as learning material</li>
<li>&quot;Here&#39;s what we&#39;ve learned&quot; orientation</li>
<li>Understand common issues before encountering them</li>
</ul>
<p><strong>4. Quarterly Pattern Analysis</strong></p>
<ul>
<li>Review all PIRs from quarter</li>
<li>Identify recurring root causes</li>
<li>Prioritize systemic improvements</li>
<li>Track incident reduction metrics</li>
</ul>
<h4>Continuous Improvement Cycle</h4>
<pre><code>Incident Occurs
    ↓
Troubleshoot &amp; Resolve (Modules 4.1-4.3)
    ↓
Post-Incident Review (Module 4.4) ← YOU ARE HERE
    ↓
Document Lessons Learned
    ↓
Update Runbooks &amp; Processes
    ↓
Share Knowledge with Team
    ↓
Implement Improvements (SMART action items)
    ↓
Monitor for Recurrence
    ↓
(Fewer incidents over time, faster resolution when they occur)
</code></pre>
<p><strong>Goal:</strong> Each incident makes the next one:</p>
<ul>
<li>Less likely to occur (prevention through improvements)</li>
<li>Faster to detect (better monitoring)</li>
<li>Faster to resolve (documented procedures)</li>
</ul>
<p><strong>Metrics to Track:</strong></p>
<ul>
<li>Incident frequency (decreasing over time)</li>
<li>Mean time to detect (MTTD - decreasing)</li>
<li>Mean time to resolve (MTTR - decreasing)</li>
<li>Recurring incident rate (decreasing)</li>
<li>Runbook utilization (increasing)</li>
</ul>
<hr>
<h2>Hands-On Lab</h2>
<h3>Lab Overview</h3>
<p><strong>Title:</strong> Conduct Post-Incident Review</p>
<p><strong>Scenario:</strong></p>
<p>Conduct a PIR for the VLAN conflict incident from Modules 4.1-4.3.</p>
<p><strong>Duration:</strong> 4-5 minutes</p>
<p><strong>Tasks:</strong></p>
<ol>
<li>Document timeline (1-2 min)</li>
<li>Identify root cause using 5 Whys (1-2 min)</li>
<li>Create 2-3 SMART action items (1-2 min)</li>
<li>(Optional) Update personal runbook (1 min)</li>
</ol>
<hr>
<h3>Task 1: Document Timeline</h3>
<p><strong>Estimated Time:</strong> 1-2 minutes</p>
<p><strong>Objective:</strong> Create factual chronological timeline of the incident.</p>
<h4>Step 1.1: List Key Events</h4>
<p>Using information from Modules 4.1-4.3, list events in chronological order.</p>
<p><strong>Your Timeline:</strong></p>
<pre><code>YYYY-MM-DD HH:MM UTC - [Event description]
</code></pre>
<p><strong>Events to Include:</strong></p>
<ul>
<li>VPCAttachment created (via Gitea commit)</li>
<li>ArgoCD synced resource</li>
<li>Issue reported by user/developer</li>
<li>Investigation started</li>
<li>Root cause identified (VLAN mismatch)</li>
<li>Attempted self-resolution (updating VPC YAML)</li>
<li>Self-resolution failed (reserved VLAN error)</li>
<li>Escalated to support (with diagnostics)</li>
<li>Support response received</li>
<li>Resolution applied (VPC updated to VLAN 1030)</li>
<li>Connectivity verified</li>
<li>Incident resolved</li>
</ul>
<p><strong>Fill in Timeline:</strong> Create 10-12 timestamped entries from VPCAttachment creation through incident resolution, including investigation milestones, failed attempts, escalation, and final resolution.</p>
<h4>Step 1.2: Calculate Metrics</h4>
<p><strong>Mean Time to Detect (MTTD):</strong> Time from incident start (10:00) to detection (10:07) = 7 minutes</p>
<p><strong>Mean Time to Resolve (MTTR):</strong> Time from detection (10:07) to resolution (14:20) = 4 hours 13 minutes</p>
<h4>Success Criteria</h4>
<ul>
<li>✅ At least 10 timeline entries</li>
<li>✅ Events in chronological order with timestamps</li>
<li>✅ Factual descriptions (no blame language like &quot;Alice made mistake&quot;)</li>
<li>✅ Metrics calculated (MTTD and MTTR)</li>
</ul>
<hr>
<h3>Task 2: Root Cause Analysis</h3>
<p><strong>Estimated Time:</strong> 1-2 minutes</p>
<p><strong>Objective:</strong> Use 5 Whys to identify systemic root cause.</p>
<h4>Step 2.1: Complete 5 Whys</h4>
<p>Start with the symptom &quot;server-07 has no connectivity&quot; and ask &quot;why&quot; five times:</p>
<ol>
<li>Why did server-07 have no connectivity?</li>
<li>Why [answer to #1]?</li>
<li>Why [answer to #2]?</li>
<li>Why [answer to #3]?</li>
<li>Why [answer to #4]?</li>
</ol>
<p>Each &quot;why&quot; should dig deeper from the proximate cause toward the systemic root cause.</p>
<h4>Step 2.2: State Root Cause</h4>
<p>Based on your 5 Whys analysis, identify:</p>
<ul>
<li><strong>Root Cause:</strong> The systemic issue (process gap, tool limitation, or documentation gap)</li>
<li><strong>Contributing Factors:</strong> 2-3 additional factors that enabled the issue</li>
</ul>
<h4>Step 2.3: Verify Root Cause</h4>
<p><strong>Root Cause Test:</strong> &quot;If we fix this root cause, would the same incident be impossible or much less likely?&quot;</p>
<p>Your answer: Yes / No</p>
<p>If &quot;No,&quot; your root cause may be a proximate cause. Keep asking &quot;why&quot; until you reach a systemic issue.</p>
<h4>Success Criteria</h4>
<ul>
<li>✅ Root cause is systemic (process gap, tool limitation, documentation gap)</li>
<li>✅ Root cause is NOT individual blame (&quot;operator chose wrong VLAN&quot;)</li>
<li>✅ Contributing factors identified</li>
<li>✅ Root cause passes verification test (fixing it would prevent recurrence)</li>
</ul>
<hr>
<h3>Task 3: Create SMART Action Items</h3>
<p><strong>Estimated Time:</strong> 1-2 minutes</p>
<p><strong>Objective:</strong> Define 2-3 specific, actionable improvements.</p>
<h4>Step 3.1: Brainstorm Improvements</h4>
<p>What could prevent this incident from recurring?</p>
<p><strong>Your Ideas:</strong></p>
<ol>
<li><hr>
</li>
<li><hr>
</li>
<li><hr>
</li>
<li><hr>
</li>
</ol>
<p><strong>Categories to Consider:</strong></p>
<ul>
<li>Documentation updates (runbook, troubleshooting guide)</li>
<li>Scripts or tools (kubectl helpers, validation scripts)</li>
<li>Process changes (checklist, peer review)</li>
<li>Feature requests (product improvements)</li>
</ul>
<h4>Step 3.2: Make Action Items SMART</h4>
<p>Choose 2-3 improvements and apply SMART criteria. For each action item, define:</p>
<ul>
<li><strong>Title:</strong> Clear, specific description</li>
<li><strong>Owner:</strong> Specific person responsible</li>
<li><strong>Due Date:</strong> Realistic deadline</li>
<li><strong>Description:</strong> What will be done, including specific steps</li>
<li><strong>Success Criteria:</strong> 2-3 measurable, verifiable outcomes</li>
</ul>
<h4>Success Criteria</h4>
<ul>
<li>✅ Each action item has specific owner</li>
<li>✅ Each action item has realistic due date</li>
<li>✅ Description is specific (not vague like &quot;be more careful&quot;)</li>
<li>✅ Success criteria are measurable and verifiable</li>
<li>✅ Action items address root cause or contributing factors</li>
</ul>
<hr>
<h3>Task 4: Update Personal Runbook (Optional)</h3>
<p><strong>Estimated Time:</strong> 1 minute</p>
<p><strong>Objective:</strong> Document VLAN selection procedure for future reference.</p>
<h4>Step 4.1: Create Runbook Section</h4>
<p>Add to your personal troubleshooting runbook:</p>
<pre><code class=""language-markdown"">## VLAN Selection for New VPCs

**Before Creating VPC:**
1. Check VLANNamespace range: `kubectl get vlannamespace default -o jsonpath=&#39;{.spec.ranges}&#39;`
2. Check existing VPC VLANs: `kubectl get vpc -A -o yaml | grep &quot;vlan:&quot; | sort | uniq`
3. Avoid reserved VLANs: 1020-1029 (system reserved)
4. Choose available VLAN from range, not in use, not reserved
5. Document chosen VLAN in VPC design notes

**Troubleshooting:**
- &quot;VLAN conflict&quot;: Choose different VLAN
- &quot;VLAN reserved&quot;: Avoid 1020-1029 range
- Use show-available-vlans.sh script or escalate if unclear
</code></pre>
<h4>Success Criteria</h4>
<ul>
<li>✅ Runbook section created with clear title</li>
<li>✅ Procedure has numbered steps</li>
<li>✅ kubectl commands included with examples</li>
<li>✅ Reserved VLANs documented</li>
<li>✅ Troubleshooting tips included</li>
</ul>
<hr>
<h3>Lab Summary</h3>
<p><strong>What You Accomplished:</strong></p>
<p>You conducted a blameless post-incident review with:</p>
<ul>
<li>✅ Factual timeline with timestamps and metrics</li>
<li>✅ Root cause identified using 5 Whys technique</li>
<li>✅ Systemic root cause (not individual blame)</li>
<li>✅ 2-3 SMART action items created</li>
<li>✅ Personal runbook updated with learnings</li>
</ul>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Blameless reviews focus on systems, not people</strong> - &quot;What allowed this?&quot; not &quot;Who did this?&quot;</li>
<li><strong>5 Whys reveals root cause</strong> - Keep asking &quot;why&quot; until you reach a systemic issue</li>
<li><strong>SMART action items are actionable</strong> - Specific, Measurable, Assigned, Relevant, Time-bound</li>
<li><strong>Operational knowledge compounds</strong> - Each PIR improves team competency</li>
<li><strong>Continuous improvement is iterative</strong> - Small improvements add up over time</li>
</ol>
<p><strong>This PIR Would:</strong></p>
<ul>
<li>Prevent VLAN conflict recurrence through documentation and tooling</li>
<li>Improve team knowledge about VLAN selection</li>
<li>Create reusable procedures for future operators</li>
<li>Demonstrate professional operational practices</li>
</ul>
<hr>
<h2>Troubleshooting</h2>
<h3>Common Lab Challenges</h3>
<h4>Challenge: &quot;I can&#39;t identify a systemic root cause&quot;</h4>
<p><strong>Symptom:</strong> Your root cause is individual action, not systemic.</p>
<p><strong>Solution:</strong> Keep asking &quot;why&quot; until you reach system issue. Test: &quot;If we fix this, can the same mistake happen again?&quot; If YES, keep digging.</p>
<hr>
<h4>Challenge: &quot;My action items are vague&quot;</h4>
<p><strong>Symptom:</strong> Action items like &quot;Improve documentation&quot; or &quot;Be more careful.&quot;</p>
<p><strong>Solution:</strong> Apply SMART criteria: Specific owner, deadline, measurable success criteria, actionable steps.</p>
<hr>
<h4>Challenge: &quot;Timeline is incomplete&quot;</h4>
<p><strong>Solution:</strong> Include creation, detection, investigation, diagnosis, resolution attempts, escalation, and final resolution with timestamps.</p>
<hr>
<h2>Resources</h2>
<h3>Reference Documentation</h3>
<p><strong>Related Modules:</strong></p>
<ul>
<li>Module 4.1: Diagnosing Fabric Issues (incident details)</li>
<li>Module 4.2: Rollback &amp; Recovery (resolution actions)</li>
<li>Module 4.3: Coordinating with Support (escalation timeline)</li>
</ul>
<p><strong>PIR Templates:</strong></p>
<ul>
<li>Complete template (provided in Concept 2)</li>
<li>SMART action item template (provided in Concept 3)</li>
</ul>
<p><strong>SRE Best Practices:</strong></p>
<ul>
<li>Google SRE Book: &quot;Postmortem Culture: Learning from Failure&quot;</li>
<li>Etsy&#39;s &quot;Blameless PostMortems and Just Culture&quot;</li>
<li>Atlassian Incident Handbook: &quot;Post-Incident Reviews&quot;</li>
</ul>
<h3>Quick Reference: PIR Checklist</h3>
<p><strong>Before PIR:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> Gather incident timeline from logs, tickets, chat</li>
<li><input disabled="""" type=""checkbox""> Collect diagnostic evidence and resolution steps</li>
<li><input disabled="""" type=""checkbox""> Schedule PIR meeting within 1-3 days of resolution</li>
<li><input disabled="""" type=""checkbox""> Invite participants (operators, support, stakeholders)</li>
</ul>
<p><strong>During PIR:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> Set blameless tone at opening</li>
<li><input disabled="""" type=""checkbox""> Document factual timeline</li>
<li><input disabled="""" type=""checkbox""> Conduct 5 Whys for root cause</li>
<li><input disabled="""" type=""checkbox""> Brainstorm improvements (defer judgment)</li>
<li><input disabled="""" type=""checkbox""> Create SMART action items with owners</li>
</ul>
<p><strong>After PIR:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> Publish PIR document to team repository</li>
<li><input disabled="""" type=""checkbox""> Track action items to completion</li>
<li><input disabled="""" type=""checkbox""> Update runbooks and procedures</li>
<li><input disabled="""" type=""checkbox""> Share learnings with team</li>
<li><input disabled="""" type=""checkbox""> Monitor for recurrence</li>
</ul>
<h3>5 Whys Quick Reference</h3>
<p><strong>How to use:</strong></p>
<ol>
<li>Start with symptom/problem statement</li>
<li>Ask &quot;Why did this happen?&quot; → Record answer</li>
<li>Ask &quot;Why [answer to 2]?&quot; → Record answer</li>
<li>Ask &quot;Why [answer to 3]?&quot; → Record answer</li>
<li>Ask &quot;Why [answer to 4]?&quot; → Record answer</li>
<li>Ask &quot;Why [answer to 5]?&quot; → Record answer ← ROOT CAUSE</li>
</ol>
<p><strong>Root cause characteristics:</strong></p>
<ul>
<li>Systemic (process, tool, design issue)</li>
<li>Actionable (can be addressed with improvements)</li>
<li>Preventive (fixing it prevents recurrence)</li>
</ul>
<p><strong>If you reach individual blame (&quot;operator mistake&quot;), keep asking &quot;why&quot; - you haven&#39;t reached root cause yet.</strong></p>
<h3>SMART Action Item Checklist</h3>
<p><strong>Before finalizing action item, verify:</strong></p>
<ul>
<li><input disabled="""" type=""checkbox""> <strong>Specific:</strong> What exactly will be done? Where?</li>
<li><input disabled="""" type=""checkbox""> <strong>Measurable:</strong> How will we verify completion?</li>
<li><input disabled="""" type=""checkbox""> <strong>Actionable:</strong> Can assigned person do this? Do they have access/tools?</li>
<li><input disabled="""" type=""checkbox""> <strong>Relevant:</strong> Does this address root cause or contributing factor?</li>
<li><input disabled="""" type=""checkbox""> <strong>Time-bound:</strong> When will this be done? Is deadline realistic?</li>
</ul>
<p><strong>If any criteria not met, refine action item until all criteria satisfied.</strong></p>
<hr>
<h2>Assessment</h2>
<h3>Question 1: Blameless Culture</h3>
<p><strong>Scenario:</strong> During a PIR, someone says: &quot;This outage happened because Alice pushed wrong YAML. She should have tested it first.&quot;</p>
<p>What is the BEST blameless response?</p>
<ul>
<li>A) &quot;You&#39;re right, Alice should have been more careful.&quot;</li>
<li>B) &quot;Let&#39;s focus on Alice&#39;s training plan.&quot;</li>
<li>C) &quot;This isn&#39;t Alice&#39;s fault, it&#39;s the system&#39;s fault.&quot;</li>
<li>D) &quot;Let&#39;s discuss what process or validation would have caught this error before production.&quot;</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> D</p>
<p><strong>Why D is correct:</strong> Focuses on systems thinking (process gap), forward-looking improvements, and actionable solutions like pre-commit validation, staging environments, or peer review workflows.</p>
<p><strong>Why others wrong:</strong> A and B focus on individual blame, C is too vague without actionable steps.</p>
<p><strong>Blameless principle:</strong> &quot;Human error is a symptom of systemic issues. Fix the system, not the human.&quot;</p>
</details>

<hr>
<h3>Question 2: Root Cause Analysis</h3>
<p><strong>Scenario:</strong> Which statement represents a ROOT CAUSE (vs. proximate cause)?</p>
<ul>
<li>A) &quot;Server had no connectivity because VLAN was wrong.&quot;</li>
<li>B) &quot;VLAN was wrong because operator chose VLAN 1025.&quot;</li>
<li>C) &quot;Operator chose VLAN 1025 because they didn&#39;t check existing VPCs.&quot;</li>
<li>D) &quot;VLANNamespace doesn&#39;t validate VLAN conflicts at creation time.&quot;</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> D</p>
<p><strong>Why D is root cause:</strong> Systemic (system design limitation), actionable (feature request), preventive (fixes all similar incidents). System would validate and prevent conflicts automatically.</p>
<p><strong>Why others are proximate:</strong> A describes symptom, B and C are human actions. Fixing these doesn&#39;t prevent next operator from same mistake.</p>
<p><strong>Root Cause Test:</strong> &quot;If we fix this, would same incident be impossible?&quot; Only D passes: YES (system prevents mistake).</p>
</details>

<hr>
<h3>Question 3: SMART Action Items</h3>
<p><strong>Scenario:</strong> Which action item is BEST (most SMART)?</p>
<ul>
<li>A) &quot;Be more careful when selecting VLANs.&quot;</li>
<li>B) &quot;Update documentation about VLANs.&quot;</li>
<li>C) &quot;Document reserved VLAN ranges (1020-1029) in operator runbook section &#39;VLAN Selection&#39;, including kubectl command. Owner: Alice. Due: Oct 20, 2025.&quot;</li>
<li>D) &quot;Someone should write down VLAN stuff somewhere.&quot;</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> C</p>
<p><strong>Why C is SMART:</strong> Specific (what/where), Measurable (clear success criteria), Actionable (Alice can do it), Relevant (prevents VLAN conflicts), Time-bound (Oct 20 deadline).</p>
<p><strong>Why others fail:</strong> A is vague and not measurable. B lacks specifics on what/where/who/when. D has no owner, deadline, or specific scope.</p>
</details>

<hr>
<h3>Question 4: Continuous Improvement</h3>
<p><strong>Scenario:</strong> Your team has 5 PIRs this month. All 5 were VLAN-related config errors. What&#39;s the BEST next step?</p>
<ul>
<li>A) Accept that VLAN issues are common</li>
<li>B) Recognize pattern and prioritize systemic improvement (VLAN validation tooling)</li>
<li>C) Require operators to re-read VLAN documentation</li>
<li>D) Add more warning labels to Gitea</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B</p>
<p><strong>Why B is correct:</strong> 5 incidents with same root cause = systemic issue. Requires systemic fix (validation tooling, pre-commit hooks, staging environment) not individual training. High ROI: tooling investment &lt; ongoing incident response time.</p>
<p><strong>Why others wrong:</strong> A is defeatist (not continuous improvement). C didn&#39;t prevent first 5 incidents (manual process still error-prone). D is weakest error prevention (warnings don&#39;t change behavior).</p>
<p><strong>SRE principle:</strong> &quot;Recurring incidents indicate systemic issues. Fix the system, not symptoms.&quot;</p>
</details>

<hr>
<h2>Conclusion</h2>
<p>🎉 <strong>You&#39;ve completed Module 4.4: Post-Incident Review!</strong></p>
<p>🏆 <strong>You&#39;ve completed Course 4: Troubleshooting, Recovery &amp; Escalation!</strong></p>
<p>🎊 <strong>You&#39;ve completed the entire Network Like a Hyperscaler pathway!</strong></p>
<h3>What You Learned in Module 4.4</h3>
<p><strong>Blameless Culture:</strong></p>
<ul>
<li>Focus on systems and processes, not individuals</li>
<li>Use forward-looking questions</li>
<li>Build psychological safety for honest reporting</li>
</ul>
<p><strong>Post-Incident Review Structure:</strong></p>
<ul>
<li>Section 1: What Happened? (Timeline)</li>
<li>Section 2: Why Did It Happen? (Root Cause via 5 Whys)</li>
<li>Section 3: How Was It Resolved? (Actions Taken)</li>
<li>Section 4: How Do We Prevent Recurrence? (Improvements)</li>
</ul>
<p><strong>SMART Action Items:</strong></p>
<ul>
<li>Specific, Measurable, Actionable, Relevant, Time-bound</li>
<li>Owned by specific person with realistic due date</li>
<li>Verifiable completion criteria</li>
</ul>
<p><strong>Operational Knowledge Management:</strong></p>
<ul>
<li>Build team knowledge through PIR documentation</li>
<li>Update runbooks and troubleshooting guides</li>
<li>Share learnings through team meetings</li>
<li>Track patterns for systemic improvements</li>
</ul>
<h3>What You Learned in Course 4</h3>
<p><strong>Module 4.1: Diagnosing Fabric Issues</strong></p>
<ul>
<li>Systematic troubleshooting using hypothesis-driven investigation</li>
<li>Common failure modes and decision trees</li>
<li>Layered diagnostic approach (Events → Agent CRD → Grafana → Logs)</li>
</ul>
<p><strong>Module 4.2: Rollback &amp; Recovery</strong></p>
<ul>
<li>GitOps rollback workflows with git revert and ArgoCD</li>
<li>Safe deletion order (VPCAttachment → VPCPeering → VPC)</li>
<li>Kubernetes finalizers and stuck resource troubleshooting</li>
<li>Partial failure recovery scenarios</li>
</ul>
<p><strong>Module 4.3: Coordinating with Support</strong></p>
<ul>
<li>Effective support ticket writing with complete diagnostics</li>
<li>Communication best practices (prompt, contextual, professional)</li>
<li>Support ticket lifecycle understanding</li>
<li>Strategic escalation (when to escalate vs. self-resolve)</li>
</ul>
<p><strong>Module 4.4: Post-Incident Review</strong></p>
<ul>
<li>Blameless culture and facilitation</li>
<li>Root cause analysis with 5 Whys</li>
<li>SMART action item creation</li>
<li>Continuous improvement mindset</li>
</ul>
<h3>What You Accomplished in the Entire Pathway</h3>
<p><strong>Course 1: Foundations &amp; Interfaces</strong></p>
<ul>
<li>Understood Hedgehog architecture and declarative control model</li>
<li>Mastered three interfaces: Gitea (Git), kubectl (API), Grafana (observability)</li>
<li>Experienced GitOps workflow firsthand</li>
</ul>
<p><strong>Course 2: Provisioning &amp; Connectivity</strong></p>
<ul>
<li>Designed and deployed VPCs with subnets and DHCP</li>
<li>Attached servers to VPCs with proper connection references</li>
<li>Validated connectivity through multiple methods</li>
<li>Decommissioned resources safely in correct order</li>
</ul>
<p><strong>Course 3: Observability &amp; Fabric Health</strong></p>
<ul>
<li>Queried Prometheus metrics for fabric state</li>
<li>Interpreted Grafana dashboards (Fabric, Interfaces, Logs)</li>
<li>Monitored Agent CRD status for detailed switch state</li>
<li>Collected comprehensive diagnostics for support</li>
</ul>
<p><strong>Course 4: Troubleshooting, Recovery &amp; Escalation</strong></p>
<ul>
<li>Diagnosed fabric issues with systematic methodology</li>
<li>Rolled back changes safely using GitOps</li>
<li>Coordinated with support effectively and professionally</li>
<li>Conducted blameless post-incident reviews</li>
</ul>
<hr>
<h2>You Are Now a Hedgehog Certified Fabric Operator (HCFO)</h2>
<h3>What This Means</h3>
<p><strong>You can confidently operate a Hedgehog fabric:</strong></p>
<p>✅ <strong>Provision network resources</strong> (VPCs, attachments, peerings) using declarative GitOps</p>
<p>✅ <strong>Monitor fabric health</strong> (Grafana dashboards, kubectl events, Agent CRD inspection)</p>
<p>✅ <strong>Troubleshoot issues</strong> (systematic diagnosis, decision trees, layered investigation)</p>
<p>✅ <strong>Recover from failures</strong> (GitOps rollback, safe deletion, finalizer troubleshooting)</p>
<p>✅ <strong>Collaborate with support</strong> (effective escalation, clear communication, professional tickets)</p>
<p>✅ <strong>Improve operations</strong> (post-incident reviews, runbook updates, knowledge sharing)</p>
<h3>What You Are NOT (Yet)</h3>
<p><strong>You are not:</strong></p>
<ul>
<li>❌ A fabric architect (designing large fabrics from scratch)</li>
<li>❌ A Hedgehog developer (contributing to Hedgehog codebase)</li>
<li>❌ A networking expert (deep BGP/EVPN/VXLAN protocol knowledge)</li>
</ul>
<p><strong>But you ARE:</strong></p>
<ul>
<li>✅ <strong>Competent</strong> - Can perform daily operations independently</li>
<li>✅ <strong>Confident</strong> - Trust your systematic troubleshooting methodology</li>
<li>✅ <strong>Collaborative</strong> - Know when and how to engage support effectively</li>
<li>✅ <strong>Continuous learner</strong> - Improve operations with each incident</li>
<li>✅ <strong>Blameless</strong> - Focus on systems, not individuals</li>
<li>✅ <strong>GitOps-native</strong> - Understand declarative infrastructure as code</li>
</ul>
<h3>Your Operational Readiness</h3>
<p><strong>Day 1 (Ready Now):</strong></p>
<ul>
<li>Provision VPCs and VPCAttachments using Gitea/kubectl</li>
<li>Monitor fabric health using Grafana dashboards</li>
<li>Respond to basic connectivity issues</li>
<li>Collect diagnostics and escalate when appropriate</li>
</ul>
<p><strong>Week 1:</strong></p>
<ul>
<li>Handle most common operational tasks independently</li>
<li>Use troubleshooting decision trees for diagnosis</li>
<li>Perform GitOps rollbacks when needed</li>
<li>Conduct post-incident reviews with team</li>
</ul>
<p><strong>Month 1:</strong></p>
<ul>
<li>Build personal runbook from incidents encountered</li>
<li>Contribute to team knowledge sharing</li>
<li>Mentor new operators using PIR documentation</li>
<li>Identify patterns and propose systemic improvements</li>
</ul>
<p><strong>Ongoing:</strong></p>
<ul>
<li>Continuous learning through incidents and PIRs</li>
<li>Stay current with Hedgehog releases and features</li>
<li>Participate in operations community</li>
<li>Share learnings and best practices</li>
</ul>
<hr>
<h2>Next Steps</h2>
<h3>Immediate Actions (This Week)</h3>
<ol>
<li><p><strong>Practice in lab environment</strong></p>
<ul>
<li>Repeat key modules if needed (especially 4.1 troubleshooting)</li>
<li>Experiment with intentional failures to practice diagnosis</li>
<li>Build muscle memory for kubectl commands</li>
</ul>
</li>
<li><p><strong>Create personal runbook</strong></p>
<ul>
<li>Document procedures learned in pathway</li>
<li>Add VLAN selection guidelines from Module 4.4</li>
<li>Include kubectl cheat sheet and common commands</li>
<li>Add troubleshooting decision trees from Module 4.1</li>
</ul>
</li>
<li><p><strong>Set up diagnostic tools</strong></p>
<ul>
<li>Save diagnostic collection script from Module 3.4</li>
<li>Bookmark Grafana dashboards (Fabric, Interfaces, Logs)</li>
<li>Configure kubectl aliases for common commands</li>
<li>Create show-available-vlans.sh helper script</li>
</ul>
</li>
<li><p><strong>Review support templates</strong></p>
<ul>
<li>Save support ticket template from Module 4.3</li>
<li>Save PIR template from Module 4.4</li>
<li>Practice writing both for lab incidents</li>
</ul>
</li>
</ol>
<h3>First 30 Days (Building Experience)</h3>
<ol>
<li><p><strong>Shadow senior operators</strong></p>
<ul>
<li>Observe during incidents and learn their approach</li>
<li>Ask questions about decision-making process</li>
<li>Compare their methodology to modules learned</li>
</ul>
</li>
<li><p><strong>Conduct PIRs for all issues</strong></p>
<ul>
<li>Even small issues deserve brief PIRs (5-10 min)</li>
<li>Practice 5 Whys technique until it becomes natural</li>
<li>Build habit of documentation and learning</li>
</ul>
</li>
<li><p><strong>Build personal knowledge base</strong></p>
<ul>
<li>Document every issue encountered (even if trivial)</li>
<li>Create troubleshooting tips section</li>
<li>Track learnings from support interactions</li>
<li>Organize by topic (VPCs, connectivity, BGP, etc.)</li>
</ul>
</li>
<li><p><strong>Contribute to team knowledge</strong></p>
<ul>
<li>Share interesting findings in team channels</li>
<li>Update team runbooks with new learnings</li>
<li>Present PIR findings in team meetings</li>
<li>Help onboard new operators</li>
</ul>
</li>
</ol>
<h3>Ongoing (Continuous Improvement)</h3>
<ol>
<li><p><strong>Stay current with Hedgehog</strong></p>
<ul>
<li>Review release notes for new features</li>
<li>Test new functionality in lab before production</li>
<li>Update runbooks when procedures change</li>
<li>Participate in Hedgehog community discussions</li>
</ul>
</li>
<li><p><strong>Deepen expertise</strong></p>
<ul>
<li>Read Hedgehog architecture documentation</li>
<li>Learn more about BGP/EVPN/VXLAN (networking foundation)</li>
<li>Understand Kubernetes controllers and CRDs deeper</li>
<li>Study SRE best practices (Google SRE book)</li>
</ul>
</li>
<li><p><strong>Participate in operational excellence</strong></p>
<ul>
<li>Attend or lead post-incident reviews</li>
<li>Propose and implement systemic improvements</li>
<li>Track incident metrics (frequency, MTTR, recurring patterns)</li>
<li>Contribute to automation and tooling</li>
</ul>
</li>
<li><p><strong>Share knowledge</strong></p>
<ul>
<li>Mentor new operators</li>
<li>Write blog posts or documentation</li>
<li>Present learnings to broader team</li>
<li>Contribute to open source (if comfortable)</li>
</ul>
</li>
</ol>
<hr>
<h2>The Hedgehog Learning Philosophy in Action</h2>
<p>Throughout this pathway, you experienced:</p>
<p>✅ <strong>Train for Reality, Not Rote</strong></p>
<ul>
<li>Production-like scenarios throughout (VLAN conflicts, BGP issues, support escalation)</li>
<li>Real commands, real troubleshooting, real documentation</li>
</ul>
<p>✅ <strong>Focus on What Matters Most</strong></p>
<ul>
<li>Common, high-impact operations (VPC provisioning, connectivity validation, troubleshooting)</li>
<li>Not comprehensive (didn&#39;t cover every CRD), but confident (can handle most scenarios)</li>
</ul>
<p>✅ <strong>Confidence Before Comprehensiveness</strong></p>
<ul>
<li>Small wins leading to competence</li>
<li>Each module built on previous success</li>
<li>Lab success criteria celebrated progress</li>
</ul>
<p>✅ <strong>Learn by Doing, Not Watching</strong></p>
<ul>
<li>Hands-on labs in every module (no passive video watching)</li>
<li>Practice actual kubectl commands and GitOps workflows</li>
<li>Build muscle memory through repetition</li>
</ul>
<p>✅ <strong>Teach the Why Behind the How</strong></p>
<ul>
<li>Explained GitOps benefits (not just &quot;use Git&quot;)</li>
<li>Explained finalizers purpose (not just &quot;don&#39;t remove them&quot;)</li>
<li>Explained blameless culture rationale (not just &quot;don&#39;t blame&quot;)</li>
</ul>
<p>✅ <strong>Support as Part of Learning</strong></p>
<ul>
<li>Normalized escalation as professional practice (Module 4.3)</li>
<li>Blameless culture encourages asking for help (Module 4.4)</li>
<li>Psychological safety emphasized throughout</li>
</ul>
<p>✅ <strong>Continuous Learning Over Static Mastery</strong></p>
<ul>
<li>Post-incident reviews for ongoing improvement (Module 4.4)</li>
<li>Runbook updates based on experience</li>
<li>Growth mindset: &quot;Each incident makes me better&quot;</li>
</ul>
<hr>
<h2>Thank You for Completing This Pathway!</h2>
<p>You&#39;ve learned to <strong>network like a hyperscaler</strong>—not by memorizing switch commands, but by mastering:</p>
<ul>
<li>Declarative infrastructure as code (GitOps)</li>
<li>Observability and data-driven operations (Grafana, metrics)</li>
<li>Systematic troubleshooting (hypothesis-driven investigation)</li>
<li>Operational excellence (blameless culture, continuous improvement)</li>
</ul>
<p><strong>You&#39;re ready to operate Hedgehog fabrics with confidence.</strong></p>
<p><strong>Welcome to the community of Hedgehog Certified Fabric Operators!</strong></p>
<p>🎉 <strong>Congratulations on completing all 16 modules!</strong> 🎉</p>
<hr>
<p><strong>Course 4 Complete</strong> | <strong>Entire Pathway Complete</strong> | <strong>HCFO Certified</strong> ✅</p>
",404,12,,,"hedgehog,fabric,post-incident,review,continuous-improvement,sre,blameless-culture"
198563771476,Systematic Troubleshooting Framework,fabric-operations-troubleshooting-framework,[object Object],"Learn systematic troubleshooting methodology using hypothesis-driven investigation, decision trees, and integrated diagnostic workflows for Hedgehog fabric operations.","<h2>Introduction</h2>
<p>You&#39;ve designed VPCs, attached servers, validated connectivity, and monitored fabric health. Your Hedgehog fabric is operational.</p>
<p>Then one morning: <strong>&quot;Server-07 can&#39;t reach the database. The application is down.&quot;</strong></p>
<p>You could panic. Or you could apply <strong>systematic troubleshooting methodology</strong>—the difference between experienced operators and beginners.</p>
<p>Beginners randomly check things, hoping to stumble on the issue. Experts use <strong>hypothesis-driven investigation</strong>, eliminating possibilities systematically until they identify the root cause.</p>
<h3>Building on Pre-Escalation Diagnostics</h3>
<p>In Module 3.4, you learned the <strong>pre-escalation diagnostic checklist</strong>—collecting evidence for support tickets. That checklist taught you <em>what</em> to collect: events, Agent CRD status, Grafana metrics, and logs.</p>
<p>Module 4.1a teaches you <strong>how to diagnose issues independently</strong> using that evidence:</p>
<ul>
<li><strong>Form hypotheses</strong> based on symptoms</li>
<li><strong>Test hypotheses</strong> with kubectl and Grafana</li>
<li><strong>Eliminate possibilities</strong> systematically</li>
<li><strong>Identify root cause</strong> with confidence</li>
</ul>
<h3>What You&#39;ll Learn</h3>
<p><strong>Troubleshooting Methodology:</strong></p>
<ul>
<li>Hypothesis-driven investigation framework</li>
<li>Systematic evidence collection order</li>
<li>Root cause identification through elimination</li>
</ul>
<p><strong>Common Failure Modes:</strong></p>
<ul>
<li>VPC attachment issues (incorrect subnet, connection, or VLAN)</li>
<li>BGP peering problems (Agent CRD state inspection)</li>
<li>Interface errors (Grafana Interface Dashboard analysis)</li>
<li>Configuration drift (GitOps reconciliation failures)</li>
</ul>
<p><strong>Diagnostic Workflow:</strong></p>
<ul>
<li>Layer 1: Kubernetes events (fastest check)</li>
<li>Layer 2: Agent CRD status (detailed switch state)</li>
<li>Layer 3: Grafana dashboards (visual metrics and trends)</li>
<li>Layer 4: Controller logs (reconciliation details)</li>
</ul>
<p><strong>Decision Trees:</strong></p>
<ul>
<li>Server cannot communicate in VPC</li>
<li>Cross-VPC connectivity fails</li>
<li>VPCAttachment shows success but doesn&#39;t work</li>
</ul>
<h3>The Scenario</h3>
<p>You&#39;ll learn systematic methodology for diagnosing connectivity failures. The framework you master here applies to any fabric issue you encounter.</p>
<p>This module focuses on <strong>the methodology itself</strong>—how to think systematically about troubleshooting. In Module 4.1b, you&#39;ll apply this methodology in a hands-on troubleshooting lab.</p>
<hr>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Apply systematic troubleshooting methodology</strong> - Use hypothesis-driven investigation to diagnose fabric issues</li>
<li><strong>Identify common failure modes</strong> - Recognize VPC attachment issues, BGP problems, interface errors, and configuration drift</li>
<li><strong>Construct diagnostic workflows</strong> - Build investigation paths from events → Agent CRDs → Grafana metrics → logs</li>
<li><strong>Differentiate issue types</strong> - Distinguish between configuration, connectivity, and platform issues</li>
<li><strong>Use decision trees for diagnosis</strong> - Follow structured decision paths to identify root causes</li>
</ol>
<hr>
<h2>Prerequisites</h2>
<p>Before starting this module, you should have:</p>
<p><strong>Completed Courses:</strong></p>
<ul>
<li>Course 1: Foundations &amp; Interfaces (kubectl proficiency, GitOps workflow)</li>
<li>Course 2: Provisioning &amp; Connectivity (VPC and VPCAttachment creation experience)</li>
<li>Course 3: Observability &amp; Fabric Health (Prometheus metrics, Grafana dashboards, Agent CRD inspection, diagnostic checklist)</li>
</ul>
<p><strong>Understanding:</strong></p>
<ul>
<li>How VPCs and VPCAttachments should work (expected behavior)</li>
<li>Agent CRD structure and status fields</li>
<li>Grafana dashboard navigation</li>
<li>kubectl events and describe commands</li>
</ul>
<p><strong>Environment:</strong></p>
<ul>
<li>kubectl configured and authenticated</li>
<li>Grafana access (<a href=""http://localhost:3000"">http://localhost:3000</a>)</li>
<li>Hedgehog fabric with at least one VPC deployed</li>
</ul>
<hr>
<h2>Scenario</h2>
<p><strong>Incident Report:</strong></p>
<p>A developer reports that <strong>server-07</strong> in VPC <strong>customer-app-vpc</strong> cannot reach the gateway or other servers in the VPC. The VPCAttachment was created this morning using the standard GitOps workflow.</p>
<p><strong>Initial Investigation:</strong></p>
<p>You run <code>kubectl describe vpcattachment customer-app-vpc-server-07</code> and see no error events. The resource exists, the controller processed it successfully, and there are no warnings.</p>
<p>But the server still has no connectivity.</p>
<p><strong>Your Task:</strong></p>
<p>Use systematic troubleshooting methodology to identify the root cause and document a solution.</p>
<p><strong>Known Information:</strong></p>
<ul>
<li>VPC: <code>customer-app-vpc</code></li>
<li>Subnet: <code>frontend</code> (10.20.10.0/24, gateway 10.20.10.1, VLAN 1025)</li>
<li>Server: <code>server-07</code></li>
<li>Expected Connection: <code>server-07--unbundled--leaf-04</code></li>
<li>VPCAttachment: <code>customer-app-vpc-server-07</code></li>
<li>Symptom: Server cannot ping gateway 10.20.10.1</li>
</ul>
<hr>
<h2>Core Concepts &amp; Deep Dive</h2>
<h3>Concept 1: Troubleshooting Methodology (Hypothesis-Driven Investigation)</h3>
<h4>The Problem with Random Checking</h4>
<p>Beginners troubleshoot by checking things randomly:</p>
<ul>
<li>&quot;Let me check the controller logs&quot;</li>
<li>&quot;Maybe it&#39;s a BGP issue&quot;</li>
<li>&quot;Did the switch reboot?&quot;</li>
<li>&quot;Is DNS working?&quot;</li>
</ul>
<p>This wastes time and misses systematic patterns. Worse, you might check dozens of things without finding the root cause, or accidentally stumble on the solution without understanding <em>why</em> it works.</p>
<h4>Systematic Approach: Hypothesis-Driven Investigation</h4>
<p>Professional troubleshooting follows a structured methodology:</p>
<p><strong>Step 1: Gather Symptoms</strong></p>
<p>Document what you know:</p>
<ul>
<li><strong>What is not working?</strong> (Specific behavior: &quot;server-07 cannot ping 10.20.10.1&quot;)</li>
<li><strong>What is the expected behavior?</strong> (Server should ping gateway successfully)</li>
<li><strong>When did it start?</strong> (This morning after VPCAttachment creation)</li>
<li><strong>What changed recently?</strong> (VPCAttachment created, committed to Git, synced by ArgoCD)</li>
</ul>
<p><strong>Step 2: Form Hypotheses</strong></p>
<p>Based on symptoms, generate possible causes:</p>
<ul>
<li><strong>Configuration error:</strong> VLAN mismatch, wrong connection reference, incorrect subnet</li>
<li><strong>Connectivity issue:</strong> BGP session down, interface down, physical layer problem</li>
<li><strong>Platform issue:</strong> Switch failure, resource exhaustion, controller error</li>
</ul>
<p><strong>Step 3: Test Hypotheses</strong></p>
<p>For each hypothesis, design a specific test:</p>
<ul>
<li><strong>If VLAN mismatch</strong> → Check Agent CRD interface configuration</li>
<li><strong>If BGP down</strong> → Check Agent CRD bgpNeighbors state</li>
<li><strong>If interface down</strong> → Check Grafana Interface Dashboard</li>
</ul>
<p><strong>Step 4: Eliminate or Confirm</strong></p>
<p>Each test either:</p>
<ul>
<li><strong>Eliminates</strong> hypothesis (BGP is established → not a BGP issue)</li>
<li><strong>Confirms</strong> hypothesis (VLAN 1010 expected, VLAN 1020 configured → VLAN mismatch confirmed!)</li>
</ul>
<p><strong>Step 5: Identify Root Cause</strong></p>
<p>When one hypothesis is confirmed and others eliminated:</p>
<ul>
<li>Root cause identified</li>
<li>Solution becomes clear</li>
<li>You can document findings with confidence</li>
</ul>
<h4>Example Walkthrough</h4>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Server-07 in VPC <code>webapp-vpc</code>, subnet <code>frontend</code></li>
<li>Expected: ping 10.0.10.1 (gateway)</li>
<li>Actual: &quot;Destination Host Unreachable&quot;</li>
<li>Recent change: VPCAttachment created today</li>
</ul>
<p><strong>Hypotheses:</strong></p>
<ol>
<li>VLAN not configured on switch interface (configuration issue)</li>
<li>Interface is down (connectivity issue)</li>
<li>Server network interface misconfigured (server issue)</li>
<li>BGP session down (routing issue)</li>
</ol>
<p><strong>Tests:</strong></p>
<p><strong>Hypothesis 1: VLAN not configured</strong></p>
<pre><code class=""language-bash""># Check Agent CRD for leaf-04 (server-07 connects to leaf-04)
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8}&#39; | jq

# Look for: vlans field contains 1010
</code></pre>
<p><strong>Result:</strong> VLAN 1010 is configured on Ethernet8 ✅ (Hypothesis 1 eliminated)</p>
<p><strong>Hypothesis 2: Interface down</strong></p>
<pre><code class=""language-bash""># Check Grafana Interface Dashboard for leaf-04/Ethernet8
# Or check Agent CRD:
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8.oper}&#39;

# Look for: oper=up
</code></pre>
<p><strong>Result:</strong> Ethernet8 oper=up ✅ (Hypothesis 2 eliminated)</p>
<p><strong>Hypothesis 3: Server misconfigured</strong></p>
<pre><code class=""language-bash""># SSH to server-07
ip link show enp2s1.1010

# Look for: interface exists and is up
</code></pre>
<p><strong>Result:</strong> Server interface enp2s1.1010 is up and tagged ✅ (Hypothesis 3 eliminated)</p>
<p><strong>Hypothesis 4: BGP down</strong></p>
<pre><code class=""language-bash""># Check Agent CRD bgpNeighbors
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors.default}&#39; | jq

# Look for: all neighbors state=established
</code></pre>
<p><strong>Result:</strong> All BGP sessions established ✅ (Hypothesis 4 eliminated)</p>
<p><strong>Wait—all tests passed, but still broken?</strong></p>
<p>When all initial hypotheses are eliminated, revise your hypothesis list. This is where systematic methodology shines: you don&#39;t give up or escalate prematurely. You form <em>new</em> hypotheses based on evidence.</p>
<p><strong>Revised Hypothesis:</strong> VPCAttachment references <strong>wrong connection</strong>.</p>
<p><strong>Test:</strong></p>
<pre><code class=""language-bash"">kubectl get vpcattachment webapp-vpc-server-07 -o jsonpath=&#39;{.spec.connection}&#39;
# Output: server-07--unbundled--leaf-05
</code></pre>
<p><strong>Root Cause Found:</strong> VPCAttachment references <code>leaf-05</code> but server-07 actually connects to <code>leaf-04</code>.</p>
<p><strong>Solution:</strong> Update VPCAttachment connection reference in Gitea to <code>server-07--unbundled--leaf-04</code>, commit, wait for ArgoCD sync.</p>
<h4>Why This Methodology Works</h4>
<ul>
<li><strong>Eliminates guesswork:</strong> You test, don&#39;t guess</li>
<li><strong>Builds evidence:</strong> Each test adds to your knowledge</li>
<li><strong>Enables explanation:</strong> You can document <em>why</em> the issue occurred</li>
<li><strong>Transfers to new scenarios:</strong> The methodology applies to issues you&#39;ve never seen before</li>
<li><strong>Supports escalation:</strong> If you must escalate, you provide tested hypotheses and evidence</li>
</ul>
<hr>
<h3>Concept 2: Common Failure Modes</h3>
<p>Knowing common failure patterns helps you form hypotheses quickly. Here are the most frequent issues in Hedgehog fabrics:</p>
<h4>Failure Mode 1: VPC Attachment Issues</h4>
<p><strong>Symptoms:</strong></p>
<ul>
<li>VPCAttachment created, no error events in kubectl describe</li>
<li>Server cannot communicate within VPC (cannot ping gateway or other servers)</li>
</ul>
<p><strong>Common Root Causes:</strong></p>
<ol>
<li><p><strong>Wrong connection name</strong></p>
<ul>
<li>VPCAttachment references incorrect Connection CRD</li>
<li>Example: VPCAttachment specifies <code>leaf-05</code> but server connects to <code>leaf-04</code></li>
</ul>
</li>
<li><p><strong>Wrong subnet</strong></p>
<ul>
<li>VPCAttachment references non-existent subnet</li>
<li>Example: VPCAttachment specifies <code>vpc-1/database</code> but VPC only has <code>vpc-1/frontend</code></li>
</ul>
</li>
<li><p><strong>VLAN conflict</strong></p>
<ul>
<li>VPC subnet specifies VLAN already in use</li>
<li>VLANNamespace allocates different VLAN</li>
<li>Configuration and actual allocation mismatch</li>
</ul>
</li>
<li><p><strong>nativeVLAN mismatch</strong></p>
<ul>
<li>VPCAttachment expects untagged VLAN (<code>nativeVLAN: true</code>)</li>
<li>Server expects tagged VLAN (interface enp2s1.1010)</li>
<li>Or vice versa</li>
</ul>
</li>
</ol>
<p><strong>Diagnostic Path:</strong></p>
<pre><code>Check VPCAttachment spec
  ↓
Verify connection exists: kubectl get connection &lt;name&gt; -n fab
  ↓
Verify subnet exists in VPC: kubectl get vpc &lt;vpc&gt; -o yaml | grep &lt;subnet&gt;
  ↓
Check Agent CRD for switch interface: Is VLAN configured?
  ↓
Check Grafana Interface Dashboard: Is interface up?
  ↓
Check VLAN ID matches expected
</code></pre>
<p><strong>Resolution:</strong></p>
<ul>
<li>Update VPCAttachment connection or subnet reference in Git</li>
<li>Fix VPC VLAN assignment if conflict exists</li>
<li>Adjust nativeVLAN setting to match server configuration</li>
</ul>
<hr>
<h4>Failure Mode 2: BGP Peering Problems</h4>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Cross-VPC connectivity fails (VPCPeering exists)</li>
<li>External connectivity fails (ExternalPeering exists)</li>
<li>Grafana Fabric Dashboard shows BGP sessions down</li>
</ul>
<p><strong>Common Root Causes:</strong></p>
<ol>
<li><p><strong>BGP session not established</strong></p>
<ul>
<li>Neighbor IP unreachable</li>
<li>ASN mismatch</li>
<li>Interface IP misconfigured</li>
</ul>
</li>
<li><p><strong>Route not advertised</strong></p>
<ul>
<li>VPCPeering permit list misconfiguration</li>
<li>ExternalPeering prefix filter blocks routes</li>
</ul>
</li>
<li><p><strong>Community mismatch</strong></p>
<ul>
<li>External community filter blocks routes</li>
<li>Inbound/outbound community misconfigured</li>
</ul>
</li>
</ol>
<p><strong>Diagnostic Path:</strong></p>
<pre><code>Check Grafana Fabric Dashboard: Which BGP sessions down?
  ↓
Check Agent CRD bgpNeighbors for affected switch
  ↓
Identify neighbor IP and state (idle, active, established)
  ↓
If state != established:
  - Check ExternalAttachment neighbor IP/ASN
  - Check switch interface IP configured correctly
  - Check external router reachability
  ↓
If state = established but routes missing:
  - Check VPCPeering permit list
  - Check ExternalPeering prefix filters
  - Check community configuration
</code></pre>
<p><strong>Resolution:</strong></p>
<ul>
<li>Fix neighbor IP or ASN in ExternalAttachment</li>
<li>Update VPCPeering permit list to include required subnets</li>
<li>Verify ExternalPeering prefix filters allow expected routes</li>
<li>Correct community configuration in External resource</li>
</ul>
<hr>
<h4>Failure Mode 3: Interface Errors</h4>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Intermittent connectivity failures</li>
<li>Packet loss</li>
<li>Grafana Interface Dashboard shows errors</li>
</ul>
<p><strong>Common Root Causes:</strong></p>
<ol>
<li><p><strong>Physical layer issue</strong></p>
<ul>
<li>Bad cable, SFP, or switch port</li>
<li>Dirty fiber optic connector</li>
</ul>
</li>
<li><p><strong>MTU mismatch</strong></p>
<ul>
<li>Server configured with MTU 9000</li>
<li>Switch configured with MTU 1500</li>
<li>Fragmentation issues</li>
</ul>
</li>
<li><p><strong>Congestion</strong></p>
<ul>
<li>Traffic exceeds interface capacity</li>
<li>No QoS configured</li>
</ul>
</li>
<li><p><strong>Configuration error</strong></p>
<ul>
<li>Speed/duplex mismatch</li>
<li>LACP misconfiguration on bundled/MCLAG connection</li>
</ul>
</li>
</ol>
<p><strong>Diagnostic Path:</strong></p>
<pre><code>Check Grafana Interface Dashboard: Which interface has errors?
  ↓
Check Agent CRD interface counters: ine (input errors), oute (output errors)
  ↓
Check interface speed and MTU configuration
  ↓
Check physical layer: SFP status, cable integrity
  ↓
If congestion: Check traffic patterns in Grafana
  ↓
If LACP issue: Check server LACP configuration
</code></pre>
<p><strong>Resolution:</strong></p>
<ul>
<li>Replace faulty cable or SFP</li>
<li>Align MTU configuration across server and switch</li>
<li>Implement QoS or upgrade link capacity</li>
<li>Fix speed/duplex or LACP configuration</li>
</ul>
<hr>
<h4>Failure Mode 4: Configuration Drift (GitOps Reconciliation Failures)</h4>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Git shows correct configuration</li>
<li>kubectl shows different configuration</li>
<li>ArgoCD shows &quot;OutOfSync&quot; status</li>
</ul>
<p><strong>Common Root Causes:</strong></p>
<ol>
<li><p><strong>Manual kubectl changes</strong></p>
<ul>
<li>Someone applied changes directly (bypassing Git)</li>
<li>ArgoCD detects drift</li>
</ul>
</li>
<li><p><strong>ArgoCD sync disabled</strong></p>
<ul>
<li>Auto-sync turned off manually</li>
<li>Changes in Git not applied</li>
</ul>
</li>
<li><p><strong>Git webhook broken</strong></p>
<ul>
<li>ArgoCD not notified of commits</li>
<li>Manual sync required</li>
</ul>
</li>
<li><p><strong>Reconciliation error</strong></p>
<ul>
<li>Controller failed to apply configuration</li>
<li>Dependency missing or validation failed</li>
</ul>
</li>
</ol>
<p><strong>Diagnostic Path:</strong></p>
<pre><code>Check ArgoCD application status: Synced or OutOfSync?
  ↓
Compare Git YAML to kubectl get &lt;resource&gt; -o yaml
  ↓
Check ArgoCD sync history: Last successful sync?
  ↓
Check controller logs for reconciliation errors
  ↓
Check kubectl events for VPC/VPCAttachment errors
</code></pre>
<p><strong>Resolution:</strong></p>
<ul>
<li>Revert manual kubectl changes, re-sync from Git</li>
<li>Re-enable ArgoCD auto-sync</li>
<li>Fix Git webhook configuration</li>
<li>Resolve dependency or validation errors in Git configuration</li>
</ul>
<hr>
<h3>Concept 3: Diagnostic Workflow (Evidence Collection Order)</h3>
<p>When facing an issue, collect evidence in this order for maximum efficiency:</p>
<h4>Layer 1: Kubernetes Events (Fastest Check)</h4>
<p><strong>Why first?</strong> Events quickly reveal configuration errors, validation failures, and reconciliation issues.</p>
<pre><code class=""language-bash""># Check for recent Warning events
kubectl get events --field-selector type=Warning --sort-by=&#39;.lastTimestamp&#39;

# Check events for specific resource
kubectl describe vpc webapp-vpc
kubectl describe vpcattachment webapp-vpc-server-07
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li><code>VLANConflict</code>, <code>SubnetOverlap</code> (configuration errors)</li>
<li><code>DependencyMissing</code> (VPC/Connection not found)</li>
<li><code>ReconcileFailed</code> (controller errors)</li>
<li><code>ValidationFailed</code> (spec validation errors)</li>
</ul>
<p><strong>What events tell you:</strong></p>
<ul>
<li>Whether the controller accepted the configuration</li>
<li>If validation passed</li>
<li>If dependencies exist</li>
<li>If reconciliation succeeded</li>
</ul>
<p><strong>Time investment:</strong> 10-30 seconds</p>
<p><strong>When to move on:</strong> If no Warning events exist, proceed to Layer 2.</p>
<hr>
<h4>Layer 2: Agent CRD Status (Detailed Switch State)</h4>
<p><strong>Why second?</strong> Agent CRD is the source of truth for actual switch configuration and operational state.</p>
<pre><code class=""language-bash""># Check agent readiness
kubectl get agents -n fab

# View detailed switch state
kubectl get agent leaf-04 -n fab -o yaml

# Check BGP neighbors
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq

# Check interface state
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8}&#39; | jq
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li><strong>BGP neighbor state:</strong> <code>established</code> vs <code>idle</code> or <code>active</code></li>
<li><strong>Interface oper state:</strong> <code>up</code> vs <code>down</code></li>
<li><strong>VLAN configuration:</strong> Which VLANs are configured on which interfaces</li>
<li><strong>Interface counters:</strong> <code>ine</code> (input errors), <code>oute</code> (output errors)</li>
<li><strong>Platform health:</strong> PSU status, fan speeds, temperature</li>
</ul>
<p><strong>What Agent CRD tells you:</strong></p>
<ul>
<li>Actual switch configuration (not just desired state)</li>
<li>Operational status (up/down, established/idle)</li>
<li>Performance metrics (errors, counters)</li>
<li>Hardware health</li>
</ul>
<p><strong>Time investment:</strong> 1-2 minutes</p>
<p><strong>When to move on:</strong> If Agent CRD shows expected configuration and healthy state, proceed to Layer 3.</p>
<hr>
<h4>Layer 3: Grafana Dashboards (Visual Metrics and Trends)</h4>
<p><strong>Why third?</strong> Grafana provides visual trends and historical context that kubectl cannot.</p>
<p><strong>Fabric Dashboard:</strong></p>
<ul>
<li>BGP session health (all established?)</li>
<li>BGP session flapping (repeated up/down transitions)</li>
<li>VPC count (as expected?)</li>
</ul>
<p><strong>Interfaces Dashboard:</strong></p>
<ul>
<li>Interface operational state (up/down over time)</li>
<li>Error rates (input errors, output errors)</li>
<li>Traffic patterns (bandwidth utilization, spikes, drops)</li>
<li>Packet counters (unicast, broadcast, multicast)</li>
</ul>
<p><strong>Logs Dashboard:</strong></p>
<ul>
<li>Recent ERROR logs (switch or controller)</li>
<li>Filter by switch name for targeted investigation</li>
<li>Syslog patterns (BGP state changes, interface flaps)</li>
</ul>
<p><strong>Look for:</strong></p>
<ul>
<li>Interfaces with high error rates (physical layer issues)</li>
<li>BGP sessions flapping (routing instability)</li>
<li>Traffic patterns indicating congestion or no traffic</li>
<li>Log patterns correlating with symptom timing</li>
</ul>
<p><strong>What Grafana tells you:</strong></p>
<ul>
<li>Historical trends (when did it start?)</li>
<li>Intermittent issues (not visible in current state)</li>
<li>Correlation between events (BGP flap coincides with interface errors)</li>
</ul>
<p><strong>Time investment:</strong> 2-3 minutes</p>
<p><strong>When to move on:</strong> If Grafana confirms hypotheses or reveals new patterns, validate with logs.</p>
<hr>
<h4>Layer 4: Controller Logs (Reconciliation Details)</h4>
<p><strong>Why fourth?</strong> Controller logs explain <em>why</em> reconciliation succeeded or failed.</p>
<pre><code class=""language-bash""># Check controller logs for specific resource
kubectl logs -n fab deployment/fabric-controller-manager | grep webapp-vpc-server-07

# Check for errors
kubectl logs -n fab deployment/fabric-controller-manager --tail=200 | grep -i error

# Follow logs live
kubectl logs -n fab deployment/fabric-controller-manager -f
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li>Reconciliation loops (same resource reconciling repeatedly)</li>
<li>Errors during configuration application</li>
<li>Validation failures</li>
<li>Dependency resolution issues</li>
</ul>
<p><strong>What controller logs tell you:</strong></p>
<ul>
<li>Why controller made specific decisions</li>
<li>Why configuration wasn&#39;t applied</li>
<li>Timing of reconciliation attempts</li>
</ul>
<p><strong>Time investment:</strong> 2-5 minutes</p>
<p><strong>When to use:</strong></p>
<ul>
<li>Events show <code>ReconcileFailed</code> but reason unclear</li>
<li>Configuration should be applied but isn&#39;t</li>
<li>Debugging GitOps sync issues</li>
</ul>
<hr>
<h4>Summary: Layered Diagnostic Approach</h4>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Tool</th>
<th>Purpose</th>
<th>Time</th>
<th>When to Use</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>kubectl events</td>
<td>Quick error check</td>
<td>10-30s</td>
<td>Always first</td>
</tr>
<tr>
<td>2</td>
<td>Agent CRD</td>
<td>Switch state verification</td>
<td>1-2min</td>
<td>After events</td>
</tr>
<tr>
<td>3</td>
<td>Grafana</td>
<td>Trends and historical context</td>
<td>2-3min</td>
<td>For intermittent issues or validation</td>
</tr>
<tr>
<td>4</td>
<td>Controller logs</td>
<td>Reconciliation debugging</td>
<td>2-5min</td>
<td>When reconciliation fails or behavior unclear</td>
</tr>
</tbody></table>
<p><strong>Total time for full diagnostic:</strong> 5-10 minutes</p>
<p>This layered approach ensures you:</p>
<ul>
<li>Start with fastest checks (events)</li>
<li>Progress to detailed state (Agent CRD)</li>
<li>Validate with trends (Grafana)</li>
<li>Deep-dive only when necessary (logs)</li>
</ul>
<hr>
<h3>Concept 4: Decision Trees for Common Scenarios</h3>
<p>Decision trees provide structured diagnostic paths for frequent issues. Follow the tree from top to bottom, testing at each decision point.</p>
<h4>Decision Tree 1: Server Cannot Communicate in VPC</h4>
<pre><code>Server cannot ping gateway or other servers in VPC
  │
  ├─ Check kubectl events for VPCAttachment
  │    │
  │    ├─ Warning events present?
  │    │    ├─ YES → Read error message, fix configuration
  │    │    └─ NO → Continue investigation
  │
  ├─ Check Agent CRD for switch
  │    │
  │    ├─ Is interface oper=up?
  │    │    ├─ NO → Physical layer issue (cable, SFP, port)
  │    │    └─ YES → Continue
  │    │
  │    ├─ Is VLAN configured on interface?
  │    │    ├─ NO → VPCAttachment wrong connection or reconciliation failed
  │    │    └─ YES → Continue
  │    │
  │    ├─ Is VLAN ID correct?
  │    │    ├─ NO → Configuration error (VLAN mismatch or conflict)
  │    │    └─ YES → Continue
  │
  ├─ Check server network interface
  │    │
  │    ├─ Is interface up and tagged with VLAN?
  │    │    ├─ NO → Server configuration issue
  │    │    └─ YES → Continue
  │
  ├─ Check Grafana Interface Dashboard
  │    │
  │    ├─ High error rates on interface?
  │    │    ├─ YES → Physical layer issue or MTU mismatch
  │    │    └─ NO → Escalate (unexpected issue)
</code></pre>
<p><strong>Example Usage:</strong></p>
<ol>
<li>Server-07 cannot ping gateway 10.20.10.1</li>
<li>Check events: No warnings → Continue</li>
<li>Check Agent CRD leaf-04 Ethernet8: oper=up → Continue</li>
<li>Check VLAN on Ethernet8: VLAN 1020 configured</li>
<li>VPC expects VLAN 1025 → <strong>VLAN mismatch identified!</strong></li>
</ol>
<hr>
<h4>Decision Tree 2: Cross-VPC Connectivity Fails (VPCPeering Exists)</h4>
<pre><code>Server in VPC-1 cannot reach server in VPC-2
  │
  ├─ Check kubectl events for VPCPeering
  │    │
  │    ├─ Warning events?
  │    │    ├─ YES → Configuration error in permit list
  │    │    └─ NO → Continue
  │
  ├─ Verify VPCPeering permit includes both subnets
  │    │
  │    ├─ Permit list missing subnet?
  │    │    ├─ YES → Add subnet to permit list
  │    │    └─ NO → Continue
  │
  ├─ Check both VPCs in same IPv4Namespace
  │    │
  │    ├─ Different namespaces?
  │    │    ├─ YES → Configuration error (VPCPeering requires same namespace)
  │    │    └─ NO → Continue
  │
  ├─ Check BGP sessions in Grafana Fabric Dashboard
  │    │
  │    ├─ BGP sessions down?
  │    │    ├─ YES → Investigate BGP issue (Agent CRD bgpNeighbors)
  │    │    └─ NO → Continue
  │
  ├─ Check VPC subnet isolation flags
  │    │
  │    ├─ isolated: true but no permit?
  │    │    ├─ YES → Add permit list entry
  │    │    └─ NO → Escalate (unexpected issue)
</code></pre>
<p><strong>Example Usage:</strong></p>
<ol>
<li>VPC-1 server cannot reach VPC-2 server</li>
<li>Check events: No warnings → Continue</li>
<li>Check VPCPeering permit list: Only includes VPC-1 frontend, missing VPC-2 backend</li>
<li><strong>Permit list incomplete → Add VPC-2 backend to permit</strong></li>
</ol>
<hr>
<h4>Decision Tree 3: VPCAttachment Shows Success But Doesn&#39;t Work</h4>
<pre><code>kubectl describe shows no errors, but server has no connectivity
  │
  ├─ Verify VPCAttachment references correct connection
  │    │
  │    ├─ kubectl get vpcattachment &lt;name&gt; -o jsonpath=&#39;{.spec.connection}&#39;
  │    │
  │    ├─ Connection matches server&#39;s actual connection?
  │    │    ├─ NO → Root cause found: Wrong connection reference
  │    │    └─ YES → Continue
  │
  ├─ Verify subnet exists in VPC
  │    │
  │    ├─ kubectl get vpc &lt;vpc&gt; -o yaml | grep &lt;subnet&gt;
  │    │
  │    ├─ Subnet exists?
  │    │    ├─ NO → Root cause found: Subnet doesn&#39;t exist
  │    │    └─ YES → Continue
  │
  ├─ Check Agent CRD for switch
  │    │
  │    ├─ VLAN configured on interface?
  │    │    ├─ NO → Reconciliation failed (check controller logs)
  │    │    └─ YES → Continue
  │
  ├─ Check nativeVLAN setting
  │    │
  │    ├─ VPCAttachment nativeVLAN matches server expectation?
  │    │    ├─ NO → Root cause found: nativeVLAN mismatch
  │    │    └─ YES → Escalate (unexpected issue)
</code></pre>
<p><strong>Example Usage:</strong></p>
<ol>
<li>VPCAttachment created, no events, but server has no connectivity</li>
<li>Check connection: <code>server-07--unbundled--leaf-04</code> → Matches</li>
<li>Check subnet exists: <code>customer-app-vpc/frontend</code> → Exists</li>
<li>Check Agent CRD: VLAN 1020 configured</li>
<li>VPC expects VLAN 1025 → <strong>VLAN mismatch (allocation conflict)</strong></li>
</ol>
<hr>
<h2>Conclusion</h2>
<p>You&#39;ve mastered the systematic troubleshooting framework for Hedgehog fabric operations!</p>
<h3>What You Learned</h3>
<p><strong>Troubleshooting Methodology:</strong></p>
<ul>
<li>Hypothesis-driven investigation framework</li>
<li>Evidence-based elimination of possibilities</li>
<li>Root cause identification with confidence</li>
</ul>
<p><strong>Common Failure Modes:</strong></p>
<ul>
<li>VPC attachment issues (VLAN conflicts, wrong connections)</li>
<li>BGP peering problems (permit lists, communities)</li>
<li>Interface errors (physical layer, MTU, congestion)</li>
<li>Configuration drift (GitOps sync issues)</li>
</ul>
<p><strong>Diagnostic Workflow:</strong></p>
<ul>
<li>Layer 1: kubectl events (fastest check)</li>
<li>Layer 2: Agent CRD (detailed switch state)</li>
<li>Layer 3: Grafana (trends and historical context)</li>
<li>Layer 4: Controller logs (reconciliation details)</li>
</ul>
<p><strong>Decision Trees:</strong></p>
<ul>
<li>Server cannot communicate in VPC</li>
<li>Cross-VPC connectivity fails</li>
<li>VPCAttachment shows success but doesn&#39;t work</li>
</ul>
<h3>Key Takeaways</h3>
<ol>
<li><p><strong>Systematic approach beats random checking</strong> - Hypothesis-driven investigation saves time and ensures thorough diagnosis</p>
</li>
<li><p><strong>Evidence collection order matters</strong> - Start fast (events), then detailed (Agent CRD), then historical (Grafana)</p>
</li>
<li><p><strong>&quot;Success&quot; doesn&#39;t mean &quot;working&quot;</strong> - No error events doesn&#39;t guarantee correct configuration (e.g., VLAN mismatch)</p>
</li>
<li><p><strong>Decision trees provide structure</strong> - Follow diagnostic paths for common scenarios to avoid missing steps</p>
</li>
<li><p><strong>Root cause identification requires testing</strong> - Eliminate possibilities until one remains, don&#39;t assume</p>
</li>
</ol>
<h3>Next Steps</h3>
<p>In <strong>Module 4.1b: Hands-On Fabric Diagnosis Lab</strong>, you&#39;ll apply this methodology in a real troubleshooting scenario. You&#39;ll:</p>
<ul>
<li>Use the layered diagnostic approach</li>
<li>Apply decision trees to identify root cause</li>
<li>Practice hypothesis-driven investigation</li>
<li>Document findings and solutions</li>
</ul>
<p>The framework you learned here is the foundation for all troubleshooting work. Master it now, and you&#39;ll diagnose issues confidently in any scenario.</p>
<hr>
<p><strong>You&#39;re now equipped with systematic troubleshooting methodology. See you in Module 4.1b for hands-on practice!</strong></p>
",401,10,,,"hedgehog,fabric,troubleshooting,diagnostics,methodology,decision-trees"
198565243491,Pre-Support Diagnostic Checklist,fabric-operations-pre-support-diagnostics,[object Object],Execute systematic diagnostic collection and write effective support tickets with comprehensive evidence for fabric issue escalation.,"<h2>Introduction</h2>
<p>You&#39;ve completed Modules 3.1-3.3, learning to:</p>
<ul>
<li>Query Prometheus metrics</li>
<li>Interpret Grafana dashboards</li>
<li>Check kubectl events and Agent CRD status</li>
</ul>
<p>These skills enable you to self-resolve <strong>many</strong> common issues: VLAN conflicts, connection misconfigurations, subnet overlaps, and invalid CIDRs. When Grafana shows no traffic and kubectl events reveal a VLAN conflict, you know to choose a different VLAN. When a VPCAttachment fails because a connection doesn&#39;t exist, you fix the connection name. These are configuration errors—fixable with your observability toolkit.</p>
<p>But sometimes, despite your troubleshooting, you&#39;ll encounter issues that require <strong>support escalation</strong>:</p>
<ul>
<li>Controller crashes repeatedly</li>
<li>Agent disconnects without clear cause</li>
<li>Switch hardware failures</li>
<li>VPC appears successful but traffic doesn&#39;t flow</li>
<li>Performance degradation with unknown root cause</li>
<li>Unexpected fabric behavior that doesn&#39;t match configuration</li>
</ul>
<h3>The Critical Question</h3>
<p><strong>When do I escalate versus keep troubleshooting?</strong></p>
<p><strong>And when I escalate, what information does support need?</strong></p>
<p>This module answers both questions. You&#39;ll learn a systematic approach to pre-escalation diagnostics that ensures you&#39;ve tried basic troubleshooting, collected comprehensive evidence, and can make confident escalation decisions. You&#39;ll practice the complete workflow: checklist execution, diagnostic collection, escalation decision-making, and support ticket writing.</p>
<h3>The Support Philosophy</h3>
<blockquote>
<p><strong>&quot;Support is not a last resort—it&#39;s a strategic resource.&quot;</strong></p>
</blockquote>
<p>Escalating early with good diagnostics is <strong>better</strong> than spending hours on an unsolvable problem. Support teams are partners in reliability, not judges of your troubleshooting ability. The key is knowing when to escalate and providing the right data when you do.</p>
<p>This module teaches you to:</p>
<ol>
<li><strong>Try first</strong> - Execute systematic health check to identify configuration issues</li>
<li><strong>Collect diagnostics</strong> - Gather comprehensive evidence (kubectl + Grafana)</li>
<li><strong>Escalate confidently</strong> - Make informed escalation decisions with complete data</li>
</ol>
<h3>What You&#39;ll Learn</h3>
<ul>
<li><strong>Pre-escalation diagnostic checklist</strong> - Systematic 6-step health check before escalation</li>
<li><strong>Evidence collection</strong> - What to gather (kubectl, logs, events, Agent status, Grafana screenshots)</li>
<li><strong>Escalation triggers</strong> - When to escalate versus self-resolve</li>
<li><strong>Support ticket best practices</strong> - Clear problem statement with relevant diagnostics</li>
<li><strong>Diagnostic package creation</strong> - Bundle diagnostics for support team</li>
<li><strong>Support boundaries</strong> - Configuration issues versus bugs</li>
</ul>
<h3>The Integration</h3>
<p>This module brings together all Course 3 skills:</p>
<pre><code>Module 3.1 (Prometheus)  ──┐
Module 3.2 (Grafana)     ──┼──&gt; Complete
Module 3.3 (kubectl)     ──┘     Diagnostic
                                 Workflow
</code></pre>
<p>You&#39;ll use Prometheus queries, Grafana dashboards, kubectl events, and Agent CRD status together in a systematic escalation workflow. This integrated approach is what separates novice operators from experienced ones who can confidently navigate the boundary between self-resolution and escalation.</p>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Execute diagnostic checklist</strong> - Run comprehensive 6-step health check before escalation</li>
<li><strong>Collect fabric diagnostics systematically</strong> - Gather kubectl outputs, logs, events, Agent status, and Grafana screenshots</li>
<li><strong>Identify escalation triggers</strong> - Determine when to escalate versus self-resolve</li>
<li><strong>Write effective support tickets</strong> - Document issues clearly with relevant diagnostics</li>
<li><strong>Package diagnostics</strong> - Create diagnostic bundle for support team</li>
<li><strong>Understand support boundaries</strong> - Know what support can help with versus operational configuration issues</li>
</ol>
<h2>Prerequisites</h2>
<ul>
<li>Module 3.1 completion (Fabric Telemetry Overview)</li>
<li>Module 3.2 completion (Dashboard Interpretation)</li>
<li>Module 3.3 completion (Events &amp; Status Monitoring)</li>
<li>Understanding of kubectl, Grafana, and Prometheus</li>
<li>Experience with VPC troubleshooting from Course 2</li>
</ul>
<h2>Scenario: Complete Pre-Escalation Diagnostic Collection</h2>
<p>A user reports that their newly created VPC <code>customer-app-vpc</code> with VPCAttachment for <code>server-07</code> isn&#39;t working. Server-07 cannot reach other servers in the VPC. You&#39;ll execute the full diagnostic checklist, collect evidence, and determine if this requires escalation or self-resolution.</p>
<p><strong>Environment Access:</strong></p>
<ul>
<li><strong>Grafana:</strong> <a href=""http://localhost:3000"">http://localhost:3000</a></li>
<li><strong>Prometheus:</strong> <a href=""http://localhost:9090"">http://localhost:9090</a></li>
<li><strong>kubectl:</strong> Already configured</li>
</ul>
<h3>Task 1: Execute Pre-Escalation Checklist (3 minutes)</h3>
<p><strong>Objective:</strong> Systematically check fabric health using the 6-step checklist</p>
<p><strong>Step 1.1: Kubernetes Resource Health</strong></p>
<pre><code class=""language-bash""># Check VPC and VPCAttachment exist
kubectl get vpc customer-app-vpc
kubectl get vpcattachment -A | grep server-07

# Check controller pods
kubectl get pods -n fab
</code></pre>
<p><strong>Look for:</strong> All pods in <code>Running</code> state. Pods in <code>CrashLoopBackOff</code> or <code>Error</code> → escalation trigger. <strong>If controller is crashing, escalate immediately.</strong></p>
<hr>
<p><strong>Step 1.2: Check Events</strong></p>
<pre><code class=""language-bash""># Check VPC and VPCAttachment events
kubectl get events --field-selector involvedObject.name=customer-app-vpc
kubectl describe vpcattachment customer-app-vpc--server-07
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li>✅ <code>Normal</code> events: Created, VNIAllocated, SubnetsConfigured, Ready</li>
<li>❌ <code>Warning</code> events: ValidationFailed, VLANConflict, SubnetOverlap, DependencyMissing</li>
</ul>
<p><strong>Decision:</strong> Warning events with clear config errors → self-resolve. No error events but issue persists → escalate.</p>
<hr>
<p><strong>Step 1.3: Agent Status</strong></p>
<pre><code class=""language-bash""># Find switch for server-07
kubectl get connections -n fab | grep server-07

# Check agent and interface
kubectl get agent leaf-04 -n fab
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8}&#39; | jq
</code></pre>
<p><strong>Look for:</strong></p>
<ul>
<li>✅ <code>oper: &quot;up&quot;</code>, low error counters</li>
<li>❌ <code>oper: &quot;down&quot;</code> or high errors → check cabling</li>
</ul>
<hr>
<p><strong>Step 1.4: BGP Health</strong></p>
<pre><code class=""language-bash""># Check BGP neighbors
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq

# Alternative: Query Prometheus
curl -s &#39;http://localhost:9090/api/v1/query?query=bgp_neighbor_state{state!=&quot;established&quot;}&#39; | jq
</code></pre>
<p><strong>Look for:</strong> All neighbors <code>state: &quot;established&quot;</code>. If down, check spine connectivity.</p>
<hr>
<p><strong>Step 1.5: Grafana Dashboard Review</strong></p>
<p>Review dashboards at <a href=""http://localhost:3000"">http://localhost:3000</a>:</p>
<ul>
<li><strong>Fabric Dashboard:</strong> BGP sessions, VPC count</li>
<li><strong>Interfaces Dashboard:</strong> leaf-04/Ethernet8 state, VLAN, traffic</li>
<li><strong>Platform Dashboard:</strong> CPU/memory, temperature, PSU/fans</li>
<li><strong>Logs Dashboard:</strong> ERROR logs</li>
<li><strong>Critical Resources Dashboard:</strong> ASIC resource utilization</li>
</ul>
<hr>
<p><strong>Step 1.6: Controller and Agent Logs</strong></p>
<pre><code class=""language-bash""># Controller logs
kubectl logs -n fab deployment/fabric-controller-manager --tail=200 | grep -i error

# If crashed, get previous logs
kubectl logs -n fab deployment/fabric-controller-manager --previous
</code></pre>
<p><strong>Look for:</strong> ERROR/PANIC messages, reconciliation loops. Escalate if found.</p>
<hr>
<p><strong>Checklist Outcomes:</strong></p>
<p><strong>A) Self-Resolve:</strong> Clear config errors (VLAN conflict, connection not found, subnet overlap) → Fix in Gitea</p>
<p><strong>B) Escalate:</strong> Controller crashes, agent issues, no errors but doesn&#39;t work, hardware failures → Collect diagnostics</p>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Executed all 6 steps</li>
<li>✅ Identified self-resolve vs escalation</li>
<li>✅ Documented findings</li>
</ul>
<hr>
<h3>Task 2: Collect Diagnostic Evidence (2-3 minutes)</h3>
<p><strong>Objective:</strong> Gather complete diagnostic bundle for support</p>
<p><strong>Quick Collection Script:</strong></p>
<pre><code class=""language-bash"">TIMESTAMP=$(date +%Y%m%d-%H%M%S)
OUTDIR=&quot;hedgehog-diagnostics-${TIMESTAMP}&quot;
mkdir -p ${OUTDIR}

# Problem description
echo &quot;Issue: server-07 cannot reach other servers in customer-app-vpc&quot; &gt; ${OUTDIR}/diagnostic-timestamp.txt
date &gt;&gt; ${OUTDIR}/diagnostic-timestamp.txt

# Collect CRDs
kubectl get vpc,vpcattachment,vpcpeering -A -o yaml &gt; ${OUTDIR}/crds-vpc.yaml
kubectl get switches,servers,connections -n fab -o yaml &gt; ${OUTDIR}/crds-wiring.yaml
kubectl get agents -n fab -o yaml &gt; ${OUTDIR}/crds-agents.yaml

# Collect events and logs
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39; &gt; ${OUTDIR}/events.log
kubectl logs -n fab deployment/fabric-controller-manager --tail=2000 &gt; ${OUTDIR}/controller.log
kubectl logs -n fab deployment/fabric-controller-manager --previous &gt; ${OUTDIR}/controller-previous.log 2&gt;/dev/null || echo &quot;No previous logs&quot; &gt; ${OUTDIR}/controller-previous.log

# Collect status and versions
kubectl get pods -n fab &gt; ${OUTDIR}/pods-fab.txt
kubectl version &gt; ${OUTDIR}/kubectl-version.txt
kubectl get agents -n fab -o jsonpath=&#39;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.status.version}{&quot;\n&quot;}{end}&#39; &gt; ${OUTDIR}/agent-versions.txt

# Collect agent details for affected switch (leaf-04)
kubectl get agent leaf-04 -n fab -o yaml &gt; ${OUTDIR}/agent-leaf-04.yaml
kubectl get agent leaf-04 -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq &gt; ${OUTDIR}/agent-leaf-04-interfaces.json

# Compress
tar czf ${OUTDIR}.tar.gz ${OUTDIR}
echo &quot;Created: ${OUTDIR}.tar.gz&quot;
</code></pre>
<p><strong>Collect Grafana Screenshots:</strong></p>
<ol>
<li>Navigate to &quot;Hedgehog Interfaces&quot; dashboard, filter leaf-04/Ethernet8, screenshot</li>
<li>Navigate to &quot;Hedgehog Logs&quot; dashboard, filter leaf-04, screenshot errors</li>
<li>Add to bundle: <code>cp grafana-*.png ${OUTDIR}/ &amp;&amp; tar czf ${OUTDIR}.tar.gz ${OUTDIR}</code></li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Diagnostic bundle created with CRDs, events, logs, Agent status, versions</li>
<li>✅ Grafana screenshots captured</li>
<li>✅ Bundle includes problem statement</li>
</ul>
<hr>
<h3>Task 3: Determine Escalation Decision (1 minute)</h3>
<p><strong>Objective:</strong> Decide if issue requires support escalation</p>
<p><strong>Self-Resolve (Configuration Errors):</strong></p>
<ul>
<li>VLAN conflict, subnet overlap, connection not found, invalid CIDR, missing VPC → Fix in Gitea, commit</li>
</ul>
<p><strong>Escalate (System/Unexpected Issues):</strong></p>
<ul>
<li>Controller crashes, agent disconnects, no errors but doesn&#39;t work, hardware failures, performance issues</li>
</ul>
<p><strong>Document Decision:</strong></p>
<pre><code class=""language-bash"">echo &quot;Decision: [Self-Resolve/Escalate]&quot; &gt;&gt; ${OUTDIR}/diagnostic-timestamp.txt
echo &quot;Reason: [Explanation]&quot; &gt;&gt; ${OUTDIR}/diagnostic-timestamp.txt
</code></pre>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Clear decision documented with reasoning</li>
</ul>
<hr>
<h3>Task 4: Write Support Ticket (2 minutes)</h3>
<p><strong>Objective:</strong> Document issue clearly for support</p>
<p><strong>Support Ticket Template:</strong></p>
<pre><code class=""language-markdown""># Issue Summary
**Problem:** [One-sentence description]
**Severity:** [P1-P4]
**Impacted Resources:** [VPCs, servers, switches]
**Started:** [Timestamp UTC]
**Recent Changes:** [What changed?]

# Environment
**Hedgehog Version:** Controller image, Agent versions
**Fabric Topology:** Spine/leaf count
**Kubernetes Version:** [version]

# Symptoms
**Observable:** [What you see in Grafana/kubectl/logs]
**Expected:** [What should happen]
**Actual:** [What is happening]

# Diagnostics Completed
**Checklist:** [✅ kubectl events, Agent status, BGP health, Grafana, logs, CRDs]
**Troubleshooting Attempted:** [What you tried and results]
**Findings:** [What you discovered]

# Attachments
**Diagnostic Bundle:** hedgehog-diagnostics-[timestamp].tar.gz
**Grafana Screenshots:** [filenames with descriptions]

# Impact
**Users Affected:** [number/type]
**Business Impact:** [critical/non-critical]
**Workaround:** [if any]
</code></pre>
<p><strong>Save ticket:</strong></p>
<pre><code class=""language-bash""># Add to bundle
cp support-ticket.md ${OUTDIR}/
tar czf ${OUTDIR}.tar.gz ${OUTDIR}
</code></pre>
<p><strong>Key Principles:</strong></p>
<ul>
<li>Be concise and specific</li>
<li>Show your troubleshooting work</li>
<li>Attach complete diagnostics</li>
<li>Avoid speculation, stick to observations</li>
<li>Professional tone</li>
</ul>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>✅ Clear problem statement with diagnostics</li>
<li>✅ Troubleshooting documented</li>
<li>✅ Professional, specific tone</li>
</ul>
<hr>
<h3>Lab Summary</h3>
<p><strong>What You Accomplished:</strong></p>
<p>You completed the full pre-escalation diagnostic workflow:</p>
<ul>
<li>✅ Executed systematic 6-step health check (Kubernetes, events, agents, BGP, Grafana, logs)</li>
<li>✅ Collected comprehensive diagnostics (CRDs, events, logs, Agent status, versions, topology)</li>
<li>✅ Made escalation decision (self-resolve vs escalate)</li>
<li>✅ Wrote effective support ticket (clear, concise, with evidence)</li>
<li>✅ Created diagnostic bundle for support (compressed, complete, well-organized)</li>
</ul>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Checklist ensures thoroughness</strong> - Don&#39;t skip steps, even if you think you know the issue</li>
<li><strong>Evidence collection is systematic</strong> - kubectl + Grafana + logs provide complete picture</li>
<li><strong>Escalation triggers are clear</strong> - Configuration errors = self-resolve, unexpected behavior = escalate</li>
<li><strong>Support tickets need specifics</strong> - Problem statement + diagnostics + troubleshooting attempts</li>
<li><strong>Diagnostic automation saves time</strong> - Script collection vs manual (see Concept 5)</li>
<li><strong>Early escalation with good data is better than endless troubleshooting</strong></li>
</ol>
<p><strong>Real-World Application:</strong></p>
<p>This workflow applies to production fabric operations:</p>
<ul>
<li>When on-call and encountering fabric issues</li>
<li>When users report VPC connectivity problems</li>
<li>When Grafana alerts fire for fabric components</li>
<li>When changes don&#39;t produce expected results</li>
</ul>
<p><strong>The diagnostic checklist and support ticket template are directly usable in production.</strong></p>
<hr>
<h2>Concepts &amp; Deep Dive</h2>
<h3>Concept 1: Pre-Escalation Diagnostic Checklist</h3>
<p>The 6-step checklist ensures systematic troubleshooting before escalation.</p>
<p><strong>Why Checklists?</strong> Prevent skipped steps, reduce cognitive load, ensure consistent quality.</p>
<p><strong>The 6 Steps:</strong></p>
<ol>
<li><strong>Kubernetes Resources:</strong> Verify VPCs, Agents, controller pods exist and running</li>
<li><strong>Events:</strong> Check for Warning events indicating config errors vs bugs</li>
<li><strong>Agent Status:</strong> Confirm agents connected, Ready, recent heartbeat</li>
<li><strong>BGP Health:</strong> All sessions established (use kubectl or Prometheus)</li>
<li><strong>Grafana:</strong> Check Fabric, Interfaces, Platform, Logs, Critical Resources dashboards</li>
<li><strong>Logs:</strong> Check controller/agent logs for ERROR/PANIC messages</li>
</ol>
<p><strong>Decision Matrix:</strong></p>
<table>
<thead>
<tr>
<th>Indicator</th>
<th>Self-Resolve</th>
<th>Escalate</th>
</tr>
</thead>
<tbody><tr>
<td>Events</td>
<td>VLAN conflict, connection not found</td>
<td>No events but issue persists</td>
</tr>
<tr>
<td>Pods</td>
<td>Running</td>
<td>CrashLoopBackOff</td>
</tr>
<tr>
<td>Agents</td>
<td>Ready, recent heartbeat</td>
<td>Disconnecting repeatedly</td>
</tr>
<tr>
<td>BGP</td>
<td>Established</td>
<td>Flapping without cause</td>
</tr>
<tr>
<td>Logs</td>
<td>Config validation errors</td>
<td>PANIC, fatal errors</td>
</tr>
</tbody></table>
<p><strong>Using the Checklist:</strong> Execute all steps → Document findings → Correlate data → Make decision → Act</p>
<hr>
<h3>Concept 2: Evidence Collection</h3>
<p>Complete diagnostic package includes:</p>
<ol>
<li><strong>Problem Statement:</strong> What broke, when, what changed, expected vs actual</li>
<li><strong>CRDs:</strong> Full YAML of VPCs, wiring, agents, namespaces</li>
<li><strong>Events:</strong> All events sorted by timestamp, Warning events separately</li>
<li><strong>Logs:</strong> Controller logs (current + previous if crashed)</li>
<li><strong>Agent Status:</strong> Per-switch YAML with BGP, interfaces, platform health</li>
<li><strong>Versions:</strong> Kubernetes, controller, agent versions</li>
<li><strong>Topology:</strong> Switches, connections, servers</li>
<li><strong>Grafana Screenshots:</strong> Interfaces, Logs, Fabric, Platform dashboards</li>
<li><strong>Timestamp:</strong> When diagnostics collected</li>
</ol>
<p><strong>Package it:</strong></p>
<pre><code class=""language-bash"">tar czf hedgehog-diagnostics-$(date +%Y%m%d-%H%M%S).tar.gz hedgehog-diagnostics-*/
</code></pre>
<p><strong>Don&#39;t include:</strong> Secrets/credentials (redact), unrelated resources, excessive logs, binaries</p>
<hr>
<h3>Concept 3: Escalation Triggers</h3>
<p><strong>ESCALATE When:</strong></p>
<ol>
<li><strong>Controller Issues:</strong> CrashLoopBackOff, PANIC/fatal errors, repeated restarts, reconciliation loops</li>
<li><strong>Agent Issues:</strong> Repeated disconnects (not reboot), can&#39;t connect (switch reachable), unexpected errors</li>
<li><strong>Hardware Failures:</strong> Switch not booting, kernel panics, persistent sensor errors</li>
<li><strong>Unexpected Behavior:</strong> Ready status but doesn&#39;t work, config not applied, impossible metrics, BGP flapping</li>
<li><strong>Performance Issues:</strong> Degradation without capacity issues, slow reconciliation, API unresponsive</li>
<li><strong>Data Plane:</strong> Traffic not forwarding despite correct config, unexplained packet loss, VXLAN issues</li>
</ol>
<p><strong>DO NOT ESCALATE (Self-Resolve):</strong></p>
<ol>
<li><strong>Config Errors:</strong> VLAN conflict, subnet overlap, invalid CIDR, connection not found, missing VPC</li>
<li><strong>Expected Warnings:</strong> Unused interfaces down, VPC deletion blocked by attachments, validation errors</li>
<li><strong>Operational Questions:</strong> How to configure, best practices, capacity → Check docs</li>
</ol>
<p><strong>Decision Tree:</strong></p>
<pre><code>Issue → Checklist → Clear config error? YES → Self-resolve
                 ↓ NO
                 → Controller/Agent crash? YES → Escalate
                 ↓ NO
                 → Hardware failure? YES → Escalate
                 ↓ NO
                 → Config correct but not working? YES → Escalate
                 ↓ NO
                 → Operational question? YES → Check docs
</code></pre>
<p><strong>Rule:</strong> If events/logs clearly explain (config error) → Self-resolve. If unexpected errors or no explanation → Escalate.</p>
<hr>
<h3>Concept 4: Support Ticket Best Practices</h3>
<p><strong>Essential Components:</strong></p>
<ol>
<li><strong>Clear Summary:</strong> One-sentence problem statement (specific, not vague)</li>
<li><strong>Severity:</strong> P1 (critical) → P4 (question) with business impact</li>
<li><strong>Impacted Resources:</strong> Exact VPCs, servers, switches</li>
<li><strong>Timeline:</strong> When started, what changed</li>
<li><strong>Symptoms:</strong> Observable evidence (Grafana/kubectl/logs)</li>
<li><strong>Expected vs Actual:</strong> What should vs does happen</li>
<li><strong>Troubleshooting:</strong> What you tried and results</li>
<li><strong>Diagnostics:</strong> Attached bundle with contents list</li>
<li><strong>Environment:</strong> Versions (controller, agents, k8s), topology</li>
<li><strong>Impact:</strong> Users affected, business impact, workarounds</li>
</ol>
<p><strong>Key Principles:</strong></p>
<ul>
<li>Be concise and specific</li>
<li>Show your work</li>
<li>Attach complete diagnostics</li>
<li>Report observations, not speculation</li>
<li>Professional tone</li>
</ul>
<p><strong>Common Mistakes to Avoid:</strong></p>
<ul>
<li>❌ Vague: &quot;It&#39;s not working&quot; → ✅ Specific: &quot;Server-03 cannot reach VPC gateway 10.10.1.1&quot;</li>
<li>❌ Missing diagnostics → ✅ Attached bundle</li>
<li>❌ No troubleshooting → ✅ Documented attempts</li>
<li>❌ Emotional language → ✅ Professional tone</li>
</ul>
<hr>
<h3>Concept 5: Diagnostic Collection Automation</h3>
<p>Automate collection for consistency and speed (30 seconds vs 5-10 minutes manual).</p>
<p><strong>Basic Script Structure:</strong></p>
<pre><code class=""language-bash"">#!/bin/bash
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
OUTDIR=&quot;hedgehog-diagnostics-${TIMESTAMP}&quot;
mkdir -p ${OUTDIR}

# Problem description prompt
read -p &quot;Problem description: &quot; DESC
echo &quot;Issue: ${DESC}&quot; &gt; ${OUTDIR}/diagnostic-timestamp.txt
date &gt;&gt; ${OUTDIR}/diagnostic-timestamp.txt

# Collect CRDs, events, logs
kubectl get vpc,vpcattachment,vpcpeering -A -o yaml &gt; ${OUTDIR}/crds-vpc.yaml 2&gt;&amp;1
kubectl get switches,servers,connections -n fab -o yaml &gt; ${OUTDIR}/crds-wiring.yaml 2&gt;&amp;1
kubectl get agents -n fab -o yaml &gt; ${OUTDIR}/crds-agents.yaml 2&gt;&amp;1
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39; &gt; ${OUTDIR}/events.log 2&gt;&amp;1
kubectl logs -n fab deployment/fabric-controller-manager --tail=2000 &gt; ${OUTDIR}/controller.log 2&gt;&amp;1

# Versions and status
kubectl version &gt; ${OUTDIR}/kubectl-version.txt 2&gt;&amp;1
kubectl get pods -n fab &gt; ${OUTDIR}/pods-fab.txt 2&gt;&amp;1
kubectl get agents -n fab -o jsonpath=&#39;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.status.version}{&quot;\n&quot;}{end}&#39; &gt; ${OUTDIR}/agent-versions.txt 2&gt;&amp;1

# Per-switch details (prompt for affected switches)
read -p &quot;Affected switches (comma-separated): &quot; SWITCHES
for switch in ${SWITCHES//,/ }; do
    kubectl get agent ${switch} -n fab -o yaml &gt; ${OUTDIR}/agent-${switch}.yaml 2&gt;&amp;1
    kubectl get agent ${switch} -n fab -o jsonpath=&#39;{.status.state.interfaces}&#39; | jq &gt; ${OUTDIR}/agent-${switch}-interfaces.json 2&gt;&amp;1
done

# Compress
tar czf ${OUTDIR}.tar.gz ${OUTDIR}
echo &quot;Created: ${OUTDIR}.tar.gz ($(du -h ${OUTDIR}.tar.gz | cut -f1))&quot;
</code></pre>
<p><strong>Benefits:</strong> Consistency, completeness, speed, error handling</p>
<p><strong>Redacting Secrets:</strong></p>
<pre><code class=""language-bash"">find ${OUTDIR} -type f -exec sed -i &#39;s/password:.*/password: REDACTED/g&#39; {} \;
find ${OUTDIR} -type f -exec sed -i &#39;s/token:.*/token: REDACTED/g&#39; {} \;
</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>Issue: Diagnostic Bundle Too Large</h3>
<p><strong>Symptom:</strong> Bundle exceeds upload limit (&gt;10 MB)</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Reduce log tail: <code>--tail=500</code> instead of <code>--tail=2000</code></li>
<li>Split into multiple bundles (CRDs, logs, agent status separately)</li>
<li>Upload to cloud storage and share link</li>
</ol>
<hr>
<h3>Issue: kubectl Commands Timing Out</h3>
<p><strong>Symptom:</strong> Script hangs on kubectl commands</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Add timeout: <code>kubectl get vpc -A -o yaml --request-timeout=30s</code></li>
<li>Collect only critical resources</li>
<li>Increase cluster timeout in kubeconfig</li>
</ol>
<hr>
<h3>Issue: Previous Controller Logs Not Available</h3>
<p><strong>Symptom:</strong> <code>kubectl logs --previous</code> fails</p>
<p><strong>Cause:</strong> Controller hasn&#39;t crashed (expected)</p>
<p><strong>Solution:</strong> Script handles gracefully with <code>2&gt;/dev/null || echo &quot;No previous logs&quot;</code></p>
<hr>
<h3>Issue: Agent CRD Has No Status</h3>
<p><strong>Symptom:</strong> <code>kubectl get agent &lt;switch&gt;</code> shows <code>status: {}</code></p>
<p><strong>Troubleshooting:</strong></p>
<ol>
<li>Check if agent pod exists: <code>kubectl get pods -n fab | grep agent-&lt;switch&gt;</code></li>
<li>Check switch registered: <code>kubectl get switch &lt;switch&gt; -n fab</code></li>
<li>Check boot logs: <code>hhfab vlab serial &lt;switch&gt;</code></li>
<li>Check fabric-boot: <code>kubectl logs -n fab deployment/fabric-boot</code></li>
</ol>
<p><strong>Solution:</strong> Wait for registration or investigate boot failure</p>
<hr>
<h3>Issue: Redacting Sensitive Information</h3>
<p><strong>Automated redaction:</strong></p>
<pre><code class=""language-bash"">find ${DIR} -type f -exec sed -i &#39;s/password:.*/password: REDACTED/g&#39; {} \;
find ${DIR} -type f -exec sed -i &#39;s/token:.*/token: REDACTED/g&#39; {} \;
</code></pre>
<p><strong>Manual review:</strong> <code>grep -ri &quot;password\|token\|secret&quot; hedgehog-diagnostics-*/</code></p>
<hr>
<h3>Issue: Support Requests Additional Data</h3>
<p><strong>Common requests:</strong></p>
<ol>
<li>Time-range metrics: <code>curl &#39;http://localhost:9090/api/v1/query_range?query=...&#39;</code></li>
<li>Switch CLI: <code>hhfab vlab ssh leaf-04 &quot;show running-config&quot;</code></li>
<li>Detailed interface stats: <code>kubectl get agent leaf-04 -o jsonpath=&#39;{.status.state.interfaces.Ethernet8}&#39;</code></li>
</ol>
<p><strong>Best practice:</strong> Keep original bundle, add supplemental files to ticket</p>
<hr>
<h2>Resources</h2>
<h3>Kubernetes Documentation</h3>
<ul>
<li><a href=""https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/"">Kubernetes Events</a></li>
<li><a href=""https://kubernetes.io/docs/reference/kubectl/"">kubectl Reference</a></li>
<li><a href=""https://kubernetes.io/docs/tasks/debug/"">Troubleshooting Applications</a></li>
</ul>
<h3>Hedgehog Documentation</h3>
<ul>
<li>Hedgehog CRD Reference (CRD_REFERENCE.md in research folder)</li>
<li>Hedgehog Observability Guide (OBSERVABILITY.md in research folder)</li>
<li>Hedgehog Fabric Controller Documentation</li>
</ul>
<h3>Related Modules</h3>
<ul>
<li>Previous: <a href=""module-3.3-events-status-monitoring.md"">Module 3.3: Events &amp; Status Monitoring</a></li>
<li>Pathway: Network Like a Hyperscaler</li>
</ul>
<h3>Diagnostic Collection Quick Reference</h3>
<p><strong>6-Step Checklist:</strong></p>
<pre><code class=""language-bash""># 1. Kubernetes Resource Health
kubectl get vpc,vpcattachment -A
kubectl get switches,agents -n fab
kubectl get pods -n fab

# 2. Events (Last Hour)
kubectl get events --all-namespaces --field-selector type=Warning --sort-by=&#39;.lastTimestamp&#39;

# 3. Switch Agent Status
kubectl get agents -n fab
kubectl get agent &lt;switch&gt; -n fab -o yaml

# 4. BGP Health
kubectl get agent &lt;switch&gt; -n fab -o jsonpath=&#39;{.status.state.bgpNeighbors}&#39; | jq

# 5. Grafana Dashboards
# - Fabric Dashboard (BGP, VPC health)
# - Interfaces Dashboard (interface state, errors)
# - Platform Dashboard (hardware health)
# - Logs Dashboard (error logs)

# 6. Controller Logs
kubectl logs -n fab deployment/fabric-controller-manager --tail=200 | grep -i error
</code></pre>
<p><strong>Diagnostic Collection:</strong></p>
<pre><code class=""language-bash""># Use automated script
./diagnostic-collect.sh

# OR manual collection
OUTDIR=&quot;hedgehog-diagnostics-$(date +%Y%m%d-%H%M%S)&quot;
mkdir -p ${OUTDIR}

kubectl get vpc,vpcattachment -A -o yaml &gt; ${OUTDIR}/crds-vpc.yaml
kubectl get events --all-namespaces --sort-by=&#39;.lastTimestamp&#39; &gt; ${OUTDIR}/events.log
kubectl logs -n fab deployment/fabric-controller-manager --tail=2000 &gt; ${OUTDIR}/controller.log
kubectl get agent &lt;switch&gt; -n fab -o yaml &gt; ${OUTDIR}/agent-&lt;switch&gt;.yaml

tar czf ${OUTDIR}.tar.gz ${OUTDIR}
</code></pre>
<hr>
<p><strong>Course 3 Complete!</strong> You&#39;ve mastered Hedgehog fabric observability and diagnostic collection. You can now:</p>
<ul>
<li>Monitor fabric health proactively (Grafana dashboards)</li>
<li>Troubleshoot issues systematically (kubectl + Grafana)</li>
<li>Collect comprehensive diagnostics (6-step checklist)</li>
<li>Make confident escalation decisions (self-resolve vs escalate)</li>
<li>Write effective support tickets (clear, specific, with evidence)</li>
</ul>
<p><strong>Your Next Steps:</strong></p>
<ul>
<li>Apply this workflow in your Hedgehog fabric operations</li>
<li>Customize the diagnostic script for your environment</li>
<li>Build confidence in escalation decisions</li>
<li>Partner with support as a strategic resource for reliability</li>
</ul>
<p><strong>You are now equipped with the complete observability and escalation toolkit for Hedgehog fabric operations.</strong></p>
",304,15,,,"hedgehog,fabric,kubernetes,diagnostics,troubleshooting,support,escalation"
198591074457,Rollback & Recovery,fabric-operations-rollback-recovery,[object Object],"Master GitOps rollback workflows, safe resource deletion order, Kubernetes finalizers, and partial failure recovery using Git revert and ArgoCD.","<h2>Introduction</h2>
<p>In Module 4.1, you diagnosed issues using systematic troubleshooting methodology. You formed hypotheses, collected evidence, and identified root causes.</p>
<p>But sometimes, diagnosis reveals that <strong>the change itself was the problem.</strong></p>
<h3>When You Need to Undo</h3>
<p>Consider these scenarios:</p>
<p><strong>Scenario 1: VLAN Conflict</strong></p>
<ul>
<li>You updated VPC <code>webapp-vpc</code> frontend subnet to use VLAN 1050</li>
<li>This caused a VLAN conflict with another VPC</li>
<li>Result: Connectivity broken for 20 web servers</li>
</ul>
<p><strong>Scenario 2: Peering Breaks Isolation</strong></p>
<ul>
<li>You created a VPCPeering between <code>production</code> and <code>development</code> VPCs</li>
<li>Security policy requires strict isolation</li>
<li>Result: Unexpected cross-environment traffic</li>
</ul>
<p><strong>Scenario 3: DHCP Misconfiguration</strong></p>
<ul>
<li>You modified DHCP settings, reducing the IP range</li>
<li>Existing servers lost their leases</li>
<li>Result: Servers cannot obtain IP addresses</li>
</ul>
<p>In these cases, you need to <strong>rollback the change</strong>—quickly, safely, and completely.</p>
<h3>Why GitOps Rollback?</h3>
<p>Beginners might panic and manually delete resources with kubectl. This causes:</p>
<ul>
<li><strong>No audit trail</strong> - Who changed what? When? Why?</li>
<li><strong>Configuration drift</strong> - Git no longer matches cluster state</li>
<li><strong>Incomplete rollback</strong> - Easy to miss dependent resources</li>
<li><strong>No reproducibility</strong> - Can&#39;t repeat the rollback if needed</li>
</ul>
<p>Experts use <strong>GitOps rollback workflows</strong> that provide:</p>
<ul>
<li><strong>Audit trail</strong> - Git commit history shows exactly what changed and why</li>
<li><strong>Reproducibility</strong> - Rollback procedure can be repeated if needed</li>
<li><strong>Safety</strong> - ArgoCD ensures consistent state between Git and cluster</li>
<li><strong>Testability</strong> - Can review rollback diff before applying</li>
</ul>
<h3>What You&#39;ll Learn</h3>
<p><strong>GitOps Rollback Workflow:</strong></p>
<ul>
<li>Using <code>git revert</code> to create rollback commits (preserves history)</li>
<li>Triggering ArgoCD sync to apply rollback</li>
<li>Verifying recovery in kubectl and Grafana</li>
<li>Understanding when rollback is appropriate</li>
</ul>
<p><strong>Safe Deletion Order:</strong></p>
<ul>
<li>Why order matters (dependencies and finalizers)</li>
<li>Correct sequence: VPCAttachment → VPCPeering → VPC</li>
<li>Avoiding stuck resources during deletion</li>
<li>Verifying cleanup completed successfully</li>
</ul>
<p><strong>Kubernetes Finalizers:</strong></p>
<ul>
<li>What finalizers are and why they exist</li>
<li>How finalizers protect against incomplete deletion</li>
<li>Diagnosing stuck resources (resources in &quot;Terminating&quot; state)</li>
<li>When manual finalizer removal is appropriate (last resort)</li>
</ul>
<p><strong>Partial Failure Recovery:</strong></p>
<ul>
<li>Handling stuck resources (Agent disconnected, reconciliation timeout)</li>
<li>ArgoCD sync failures (invalid YAML, dependency missing)</li>
<li>Reconciliation timeouts (controller resource constraints)</li>
<li>Emergency procedures (when to escalate vs. self-resolve)</li>
</ul>
<h3>Module Scenario</h3>
<p>You&#39;ll perform a rollback of a VPC configuration change that caused VLAN conflicts:</p>
<ul>
<li>Identify and revert the problematic Git commit</li>
<li>Trigger ArgoCD sync to apply the rollback</li>
<li>Verify connectivity restored with kubectl and Grafana</li>
<li>(Bonus) Practice safe VPCAttachment deletion</li>
</ul>
<p>By the end of this module, you&#39;ll have the skills to safely undo changes and recover from failures using GitOps best practices.</p>
<hr>
<h2>Learning Objectives</h2>
<p>By the end of this module, you will be able to:</p>
<ol>
<li><strong>Execute safe rollback procedures</strong> - Use Git revert and ArgoCD sync to undo configuration changes</li>
<li><strong>Understand Kubernetes finalizers</strong> - Explain how finalizers protect against incomplete deletion</li>
<li><strong>Follow safe deletion order</strong> - Delete resources in correct dependency order (VPCAttachment → VPC → namespace resources)</li>
<li><strong>Recover from partial failures</strong> - Handle stuck resources, finalizer issues, and reconciliation timeouts</li>
<li><strong>Identify escalation triggers</strong> - Recognize when rollback/recovery requires support intervention</li>
</ol>
<hr>
<h2>Prerequisites</h2>
<p>Before starting this module, you should have:</p>
<p><strong>Completed Modules:</strong></p>
<ul>
<li>Module 4.1: Diagnosing Fabric Issues (hypothesis-driven troubleshooting)</li>
<li>Module 1.2: GitOps Workflow (Git operations, ArgoCD sync)</li>
<li>Course 2: Provisioning &amp; Connectivity (VPC lifecycle experience)</li>
</ul>
<p><strong>Understanding:</strong></p>
<ul>
<li>Basic Git operations (commit, push, diff)</li>
<li>kubectl resource management</li>
<li>VPC and VPCAttachment dependencies</li>
<li>How finalizers work in Kubernetes</li>
</ul>
<p><strong>Environment:</strong></p>
<ul>
<li>kubectl configured and authenticated</li>
<li>Gitea access (<a href=""http://localhost:3001"">http://localhost:3001</a>)</li>
<li>ArgoCD access (<a href=""http://localhost:8080"">http://localhost:8080</a>)</li>
<li>Grafana access (<a href=""http://localhost:3000"">http://localhost:3000</a>)</li>
</ul>
<hr>
<h2>Scenario</h2>
<p><strong>Incident Background:</strong></p>
<p>Earlier today, you updated VPC <code>webapp-vpc</code> to change the <code>frontend</code> subnet VLAN from 1010 to 1050. The change was committed to Git and synced via ArgoCD successfully.</p>
<p><strong>Problem Discovered:</strong></p>
<p>VLAN 1050 is already in use by another VPC (<code>customer-app-vpc</code>). The change caused a VLAN conflict, breaking connectivity for 20 web servers in <code>webapp-vpc</code>.</p>
<p><strong>Your Task:</strong></p>
<p>Perform a GitOps rollback to restore connectivity:</p>
<ol>
<li>Revert the problematic Git commit (VLAN 1010 → 1050)</li>
<li>Sync the rollback through ArgoCD</li>
<li>Verify connectivity restored</li>
<li>(Bonus) Practice safe resource deletion procedures</li>
</ol>
<p><strong>Success Criteria:</strong></p>
<ul>
<li>Git history shows revert commit (audit trail preserved)</li>
<li>ArgoCD synced successfully</li>
<li><code>webapp-vpc</code> frontend subnet using VLAN 1010 again</li>
<li>Connectivity verified in kubectl and Grafana</li>
<li>No configuration drift (Git matches cluster)</li>
</ul>
<hr>
<h2>Core Concepts &amp; Deep Dive</h2>
<h3>Concept 1: GitOps Rollback Workflow</h3>
<h4>Why GitOps Rollback?</h4>
<p>GitOps rollback uses Git as the source of truth, providing:</p>
<ul>
<li><strong>Audit trail</strong> - Git history tracks all changes and rollbacks</li>
<li><strong>Reproducibility</strong> - Same procedure every time</li>
<li><strong>Safety</strong> - ArgoCD validates before applying</li>
<li><strong>No drift</strong> - Git and cluster stay synchronized</li>
</ul>
<h4>GitOps Rollback Procedure (5 Steps)</h4>
<h5>Step 1: Identify Problematic Commit</h5>
<pre><code class=""language-bash""># View recent Git history
cd /path/to/hedgehog-config
git log --oneline --graph --decorate -10

# Example output:
# a1b2c3d (HEAD -&gt; main) Update webapp-vpc VLAN to 1050
</code></pre>
<p><strong>Identify commit to revert:</strong> Use commit message, timestamp, and <code>git show &lt;commit-hash&gt;</code> to find the problematic change.</p>
<h5>Step 2: Revert the Commit (Git)</h5>
<pre><code class=""language-bash""># Revert the specific commit (creates NEW commit that undoes changes)
git revert a1b2c3d

# Edit commit message to add context (why rollback needed)
# Review diff with: git show HEAD
# Push revert commit
git push origin main
</code></pre>
<h5>Step 3: Trigger ArgoCD Sync</h5>
<p><strong>Option A: Wait for auto-sync</strong> (3 minutes) or <strong>Option B: Manual sync via UI or CLI:</strong></p>
<pre><code class=""language-bash""># ArgoCD CLI
argocd app sync hedgehog-config
argocd app wait hedgehog-config --health
</code></pre>
<h5>Step 4: Verify Rollback</h5>
<pre><code class=""language-bash""># Verify VLAN reverted
kubectl get vpc webapp-vpc -o jsonpath=&#39;{.spec.subnets.frontend.vlan}&#39;
# Expected: 1010

# Check events
kubectl get events --field-selector involvedObject.name=webapp-vpc --sort-by=&#39;.lastTimestamp&#39;

# Verify Agent CRD (switch configuration)
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq
# Expected: VLAN 1010 in vlans list
</code></pre>
<h5>Step 5: Validate Connectivity</h5>
<p><strong>Check Grafana:</strong> Verify VLAN 1010, no VLAN conflicts, traffic flowing on interfaces.</p>
<p><strong>Test connectivity (if available):</strong></p>
<pre><code class=""language-bash""># Ping gateway and other servers
ping -c 4 10.10.1.1
# Expected: 0% packet loss
</code></pre>
<hr>
<h3>Concept 2: Safe Deletion Order</h3>
<h4>Why Order Matters</h4>
<p>Deleting resources in wrong order causes stuck resources, orphaned configurations, and partial failures. <strong>Rule:</strong> Delete from leaves to roots (children before parents).</p>
<p><strong>Safe deletion order:</strong></p>
<pre><code>VPCAttachment → VPCPeering → ExternalPeering → VPC → Namespaces
</code></pre>
<p>If resource A references resource B in its spec, delete A before B.</p>
<h4>Example: Delete VPC and Attachments</h4>
<p><strong>✅ Correct Order:</strong></p>
<pre><code class=""language-bash""># Step 1: Delete all VPCAttachments
kubectl delete vpcattachment vpc-prod-server-01 vpc-prod-server-02 vpc-prod-server-03

# Step 2: Delete VPCPeerings (if any)
kubectl delete vpcpeering vpc-prod--vpc-staging

# Step 3: Delete VPC
kubectl delete vpc vpc-prod
# Should complete in &lt;30 seconds if dependencies cleared
</code></pre>
<p><strong>Verification:</strong> Check DHCPSubnets auto-deleted, VLAN removed from Agent CRD, VPC count decreased in Grafana.</p>
<hr>
<h3>Concept 3: Kubernetes Finalizers</h3>
<h4>What Are Finalizers?</h4>
<p>Finalizers are hooks that prevent resource deletion until cleanup completes. They ensure dependencies are deleted first, switch configurations are cleaned up, and no orphaned resources remain.</p>
<h4>Finalizer Workflow</h4>
<ol>
<li>User deletes resource → Kubernetes sets <code>deletionTimestamp</code></li>
<li>Resource enters &quot;Terminating&quot; state</li>
<li>Controller performs cleanup (remove switch VLANs, delete DHCPSubnets, etc.)</li>
<li>Controller removes finalizer</li>
<li>Kubernetes completes deletion</li>
</ol>
<h4>Stuck Resources (Finalizer Not Removed)</h4>
<p><strong>Common causes:</strong></p>
<ol>
<li><strong>VPCAttachments still exist</strong> - Delete VPCAttachments first</li>
<li><strong>Agent disconnected</strong> - Wait for reconnection</li>
<li><strong>Controller error</strong> - Check logs, restart if needed</li>
</ol>
<h4>Recovery: Remove Finalizer Manually (Last Resort)</h4>
<p><strong>⚠️ WARNING:</strong> Only do this after 30+ minutes, with dependencies deleted, and controller logs checked.</p>
<pre><code class=""language-bash""># Remove finalizer to force deletion
kubectl patch vpc vpc-prod --type=&#39;json&#39; -p=&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;
</code></pre>
<p><strong>Caution:</strong> May leave orphaned switch configurations. Verify cleanup in Agent CRD after removal.</p>
<hr>
<h3>Concept 4: Partial Failure Recovery</h3>
<h4>Scenario 1: Stuck VPCAttachment (Agent Disconnected)</h4>
<p><strong>Recovery Options:</strong></p>
<p><strong>Option A: Wait for Agent Reconnection (Preferred)</strong></p>
<ul>
<li>Controller removes finalizer automatically when agent returns</li>
<li>Safe, no orphaned configurations</li>
</ul>
<p><strong>Option B: Force Delete (Emergency, after 30+ min)</strong></p>
<pre><code class=""language-bash"">kubectl patch vpcattachment vpc-prod-server-01 --type=&#39;json&#39; -p=&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;
</code></pre>
<p>⚠️ May leave orphaned switch VLAN. Check Agent CRD after reconnection.</p>
<h4>Scenario 2: ArgoCD Sync Failure</h4>
<p><strong>Common causes:</strong> Invalid YAML (syntax error, wrong field name), missing namespace, RBAC issue.</p>
<p><strong>Recovery:</strong></p>
<pre><code class=""language-bash""># Step 1: Validate YAML locally
kubectl apply --dry-run=client -f vpcs/webapp-vpc.yaml

# Step 2: Fix YAML in Git, commit, and push
git add vpcs/webapp-vpc.yaml
git commit -m &quot;Fix YAML syntax in rollback commit&quot;
git push origin main

# Step 3: Retry sync
argocd app sync hedgehog-config
</code></pre>
<h4>Scenario 3: Reconciliation Timeout</h4>
<p><strong>Common causes:</strong> Controller resource constraints, slow Agent response, large reconciliation queue.</p>
<p><strong>Recovery:</strong></p>
<p><strong>Option A: Wait</strong> (10-15 min, usually resolves)</p>
<p><strong>Option B: Restart Controller</strong> (if hung)</p>
<pre><code class=""language-bash"">kubectl rollout restart deployment/fabric-controller-manager -n fab
</code></pre>
<p><strong>Option C: Escalate</strong> (if &gt;30 min or repeated failures)</p>
<hr>
<h3>Concept 5: Emergency Procedures</h3>
<h4>When to Escalate vs. Self-Resolve</h4>
<p><strong>Self-Resolve:</strong> Stuck resource due to missed dependency deletion, YAML syntax error, reconciliation timeout &lt;15 min, agent disconnected.</p>
<p><strong>Escalate:</strong> Controller repeatedly crashing, finalizer removal doesn&#39;t complete deletion, partial failure with inconsistent state, stuck resource &gt;30 min after manual intervention.</p>
<hr>
<h2>Hands-On Lab</h2>
<h3>Lab Overview</h3>
<p><strong>Title:</strong> Rollback a VPC Configuration Change</p>
<p><strong>Scenario:</strong></p>
<p>Earlier today, you updated VPC <code>webapp-vpc</code> to use VLAN 1050 for the <code>frontend</code> subnet. This caused a VLAN conflict with another VPC, breaking connectivity for 20 web servers.</p>
<p><strong>Your task:</strong></p>
<ol>
<li>Identify and revert the problematic commit in Git</li>
<li>Trigger ArgoCD sync to apply the rollback</li>
<li>Verify connectivity restored</li>
<li>(Bonus) Practice safe VPCAttachment deletion</li>
</ol>
<p><strong>Environment:</strong></p>
<ul>
<li><strong>Gitea:</strong> <a href=""http://localhost:3001"">http://localhost:3001</a> (username: <code>student</code>, password: <code>hedgehog123</code>)</li>
<li><strong>ArgoCD:</strong> <a href=""http://localhost:8080"">http://localhost:8080</a> (username: <code>admin</code>, password: <code>qV7hX0NMroAUhwoZ</code>)</li>
<li><strong>kubectl:</strong> Already configured</li>
<li><strong>Grafana:</strong> <a href=""http://localhost:3000"">http://localhost:3000</a></li>
</ul>
<p><strong>Git Repository:</strong> <code>student/hedgehog-config</code></p>
<hr>
<h3>Task 1: Identify and Revert Problematic Commit</h3>
<p><strong>Estimated Time:</strong> 3 minutes</p>
<p><strong>Objective:</strong> Use Git to rollback the VLAN change that caused the conflict.</p>
<h4>Steps</h4>
<pre><code class=""language-bash""># Navigate to repository
cd /path/to/hedgehog-config

# View recent history
git log --oneline -5

# Revert the commit (e.g., a1b2c3d)
git revert a1b2c3d

# Add context to commit message explaining why rollback needed
# Save and exit editor

# Push to remote
git push origin main
</code></pre>
<p><strong>Success Criteria:</strong> Revert commit exists, VLAN changed 1050 → 1010, changes pushed to main.</p>
<hr>
<h3>Task 2: Trigger ArgoCD Sync and Verify</h3>
<p><strong>Estimated Time:</strong> 2 minutes</p>
<p><strong>Objective:</strong> Apply rollback to cluster via ArgoCD and verify successful sync.</p>
<h4>Steps</h4>
<pre><code class=""language-bash""># Trigger sync (or wait 3 min for auto-sync)
argocd app sync hedgehog-config
argocd app wait hedgehog-config --health

# Verify VLAN reverted
kubectl get vpc webapp-vpc -o jsonpath=&#39;{.spec.subnets.frontend.vlan}&#39;
# Expected: 1010

# Check events
kubectl get events --field-selector involvedObject.name=webapp-vpc --sort-by=&#39;.lastTimestamp&#39;

# Verify Agent CRD shows VLAN 1010
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq
</code></pre>
<p><strong>Success Criteria:</strong> ArgoCD synced, kubectl shows VLAN 1010, no Warning events, Agent CRD updated.</p>
<hr>
<h3>Task 3: Validate Connectivity Restored</h3>
<p><strong>Estimated Time:</strong> 1 minute</p>
<p><strong>Objective:</strong> Confirm servers can communicate after rollback.</p>
<h4>Steps</h4>
<p><strong>Verify in Grafana:</strong></p>
<ul>
<li>Navigate to Fabric Dashboard: <code>webapp-vpc</code> shows VLAN 1010, no conflicts</li>
<li>Interface Dashboard: VLAN 1010 configured, traffic flowing</li>
</ul>
<p><strong>Test connectivity (optional):</strong></p>
<pre><code class=""language-bash"">ping -c 4 10.0.10.1  # Gateway
# Expected: 0% packet loss
</code></pre>
<p><strong>Success Criteria:</strong> Grafana confirms VLAN 1010, no conflicts, traffic flowing.</p>
<hr>
<h3>Task 4 (Bonus): Practice Safe Deletion Order</h3>
<p><strong>Estimated Time:</strong> 1-2 minutes</p>
<p><strong>Objective:</strong> Safely delete a test VPCAttachment.</p>
<p><strong>Scenario:</strong> Delete <code>webapp-vpc-server-03</code>.</p>
<h4>Steps</h4>
<pre><code class=""language-bash""># Delete VPCAttachment
kubectl delete vpcattachment webapp-vpc-server-03

# Verify deletion completes (&lt;30 seconds)
kubectl get vpcattachment webapp-vpc-server-03
# Expected: Error: not found

# Verify VLAN removed from Agent CRD
kubectl get agent leaf-03 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet8}&#39; | jq

# Verify VPC still exists
kubectl get vpc webapp-vpc
</code></pre>
<p><strong>Success Criteria:</strong> VPCAttachment deleted, finalizer removed automatically, VLAN removed from switch, VPC intact.</p>
<hr>
<h3>Lab Summary</h3>
<p><strong>What You Accomplished:</strong></p>
<p>You successfully performed a production-style rollback and recovery:</p>
<ol>
<li>✅ Reverted problematic Git commit using <code>git revert</code></li>
<li>✅ Synced rollback to cluster via ArgoCD</li>
<li>✅ Verified connectivity restored with kubectl and Grafana</li>
<li>✅ (Bonus) Practiced safe VPCAttachment deletion</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>GitOps rollback is safe and auditable</strong> - Git history tracks all changes and reversions</li>
<li><strong>ArgoCD ensures consistency</strong> - Cluster state always matches Git</li>
<li><strong>Deletion order prevents stuck resources</strong> - VPCAttachment before VPC</li>
<li><strong>Finalizers protect against incomplete cleanup</strong> - Ensure switch configs cleaned up</li>
<li><strong>Manual finalizer removal is last resort</strong> - Only when stuck &gt;30 minutes and confirmed safe</li>
</ol>
<p><strong>Recovery Mindset:</strong></p>
<ul>
<li>Plan rollback before making changes (know how to undo)</li>
<li>Test rollback in non-prod first (if possible)</li>
<li>Verify after rollback (don&#39;t assume it worked)</li>
<li>Document recovery procedures (for team knowledge)</li>
</ul>
<hr>
<h2>Troubleshooting</h2>
<h3>Common Lab Challenges</h3>
<h4>Challenge: &quot;Git revert created conflicts&quot;</h4>
<p><strong>Symptom:</strong> Git revert fails with merge conflicts.</p>
<p><strong>Cause:</strong> The file has been modified since the commit you&#39;re reverting.</p>
<p><strong>Solution:</strong></p>
<pre><code class=""language-bash""># Git will show conflict markers in the file
# Edit the file to resolve conflicts

nano vpcs/webapp-vpc.yaml

# Look for conflict markers:
# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
# current version
# =======
# reverted version
# &gt;&gt;&gt;&gt;&gt;&gt;&gt; parent of a1b2c3d

# Manually resolve by keeping the correct VLAN value

# Stage resolved file
git add vpcs/webapp-vpc.yaml

# Complete the revert
git revert --continue

# Push to remote
git push origin main
</code></pre>
<h4>Challenge: &quot;ArgoCD shows OutOfSync but sync fails&quot;</h4>
<p><strong>Symptom:</strong> ArgoCD detects change but sync operation fails with error.</p>
<p><strong>Cause:</strong> YAML syntax error, missing namespace, or RBAC issue.</p>
<p><strong>Solution:</strong></p>
<pre><code class=""language-bash""># Validate YAML locally
kubectl apply --dry-run=client -f vpcs/webapp-vpc.yaml

# If invalid, check ArgoCD logs for specific error
kubectl logs -n argocd deployment/argocd-application-controller | grep hedgehog-config

# Fix YAML in Git based on error message
# Commit and push fix
# Retry ArgoCD sync
</code></pre>
<h4>Challenge: &quot;VPCAttachment stuck in Terminating state&quot;</h4>
<p><strong>Symptom:</strong> VPCAttachment not deleting after several minutes.</p>
<p><strong>Cause:</strong> Agent disconnected, controller can&#39;t clean up switch configuration.</p>
<p><strong>Solution:</strong></p>
<pre><code class=""language-bash""># Check agent status
kubectl get agent leaf-03 -n fab

# If agent not Ready, wait for reconnection

# If agent Ready but VPCAttachment still stuck, check controller logs
kubectl logs -n fab deployment/fabric-controller-manager | grep webapp-vpc-server-03

# If stuck &gt;30 minutes, manually remove finalizer (last resort)
kubectl patch vpcattachment webapp-vpc-server-03 --type=&#39;json&#39; -p=&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;
</code></pre>
<h4>Challenge: &quot;Connectivity still broken after rollback&quot;</h4>
<p><strong>Symptom:</strong> kubectl shows VLAN 1010, but servers still can&#39;t communicate.</p>
<p><strong>Diagnosis:</strong></p>
<pre><code class=""language-bash""># Check Agent CRD to confirm switch configuration
kubectl get agent leaf-01 -n fab -o jsonpath=&#39;{.status.state.interfaces.Ethernet5}&#39; | jq

# Check if VLAN actually updated on switch (not just in VPC CRD)

# Check server interface configuration
# (SSH to server or use kubectl exec)
ip addr show enp2s1.1010

# Check for other issues (use Module 4.1 troubleshooting methodology)
</code></pre>
<p><strong>Common causes:</strong></p>
<ul>
<li>Agent not updated switch configuration yet (wait 1-2 minutes)</li>
<li>Server interface still configured for VLAN 1050</li>
<li>Different root cause (VLAN wasn&#39;t the issue)</li>
</ul>
<hr>
<h2>Resources</h2>
<h3>Reference Documentation</h3>
<p><strong>GitOps Workflow:</strong></p>
<ul>
<li>Module 1.2: GitOps with Hedgehog Fabric (Git operations, ArgoCD sync)</li>
<li>WORKFLOWS.md (lines 768-861): Workflow 7 - Cleanup and Rollback</li>
</ul>
<p><strong>CRD Reference:</strong></p>
<ul>
<li>CRD_REFERENCE.md: VPC, VPCAttachment finalizers and status fields</li>
<li>Understanding resource dependencies and deletion order</li>
</ul>
<p><strong>Related Modules:</strong></p>
<ul>
<li>Module 4.1: Diagnosing Fabric Issues (hypothesis-driven troubleshooting)</li>
<li>Module 4.3: Coordinating with Support (when to escalate)</li>
</ul>
<h3>Quick Reference: Rollback Commands</h3>
<p><strong>Git Rollback:</strong></p>
<pre><code class=""language-bash""># View commit history
git log --oneline --graph -10

# Revert specific commit
git revert &lt;commit-hash&gt;

# View revert diff
git show HEAD

# Push revert
git push origin main
</code></pre>
<p><strong>ArgoCD Sync:</strong></p>
<pre><code class=""language-bash""># Trigger sync
argocd app sync hedgehog-config

# Wait for completion
argocd app wait hedgehog-config --health

# Check sync status
argocd app get hedgehog-config

# View sync history
argocd app history hedgehog-config
</code></pre>
<p><strong>Safe Deletion:</strong></p>
<pre><code class=""language-bash""># Step 1: Delete VPCAttachments
kubectl delete vpcattachment &lt;name&gt;

# Step 2: Delete VPCPeerings
kubectl delete vpcpeering &lt;name&gt;

# Step 3: Delete VPC
kubectl delete vpc &lt;name&gt;

# Verify deletion
kubectl get vpc &lt;name&gt;
# Expected: Error: not found
</code></pre>
<p><strong>Finalizer Troubleshooting:</strong></p>
<pre><code class=""language-bash""># Check finalizers
kubectl get vpc &lt;name&gt; -o jsonpath=&#39;{.metadata.finalizers}&#39;

# Check deletionTimestamp
kubectl get vpc &lt;name&gt; -o jsonpath=&#39;{.metadata.deletionTimestamp}&#39;

# Remove finalizer (last resort)
kubectl patch vpc &lt;name&gt; --type=&#39;json&#39; -p=&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;
</code></pre>
<h3>Escalation Checklist</h3>
<p><strong>Before escalating, ensure you have:</strong></p>
<ul>
<li>✅ Attempted rollback using Git revert</li>
<li>✅ Verified Git commit pushed successfully</li>
<li>✅ Triggered ArgoCD sync</li>
<li>✅ Checked kubectl events for errors</li>
<li>✅ Checked controller logs for reconciliation errors</li>
<li>✅ Waited &gt;30 minutes for stuck resources</li>
<li>✅ Verified Agent connectivity</li>
<li>✅ Documented all steps attempted</li>
</ul>
<p><strong>Escalate when:</strong></p>
<ul>
<li>Controller repeatedly crashes during rollback</li>
<li>Finalizer removal doesn&#39;t complete deletion (after manual removal)</li>
<li>Partial failure leaves fabric in inconsistent state</li>
<li>Rollback causes new errors (configuration corruption)</li>
<li>Stuck resource &gt;30 min after manual finalizer removal</li>
</ul>
<p>Reference Module 4.3 for escalation procedures and creating effective support tickets.</p>
<hr>
<h2>Assessment</h2>
<h3>Question 1: GitOps Rollback</h3>
<p><strong>Scenario:</strong> You need to rollback a VPC configuration change. The problematic commit is <code>a1b2c3d</code>. What is the CORRECT command sequence?</p>
<p><strong>A:</strong></p>
<pre><code class=""language-bash"">git reset --hard HEAD~1
git push --force
</code></pre>
<p><strong>B:</strong></p>
<pre><code class=""language-bash"">git revert a1b2c3d
git push origin main
</code></pre>
<p><strong>C:</strong></p>
<pre><code class=""language-bash"">kubectl delete vpc vpc-prod
kubectl apply -f vpc-prod-backup.yaml
</code></pre>
<p><strong>D:</strong></p>
<pre><code class=""language-bash"">git checkout a1b2c3d~1
git push origin main
</code></pre>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B</p>
<p><code>git revert</code> creates a NEW commit that undoes changes, preserving Git history (audit trail), safe for shared branches, and ArgoCD compatible.</p>
<p><strong>Why others are wrong:</strong></p>
<ul>
<li><strong>A)</strong> Rewrites history, breaks collaborators&#39; clones, loses audit trail</li>
<li><strong>C)</strong> Bypasses GitOps, creates drift (Git ≠ cluster), no audit trail</li>
<li><strong>D)</strong> Creates detached HEAD, cannot push to branch</li>
</ul>
<p><strong>GitOps Principle:</strong> Always make changes via Git commits. Never rewrite shared branch history.</p>
</details>

<hr>
<h3>Question 2: Safe Deletion Order</h3>
<p><strong>Scenario:</strong> You need to delete VPC <code>prod-vpc</code>. It has 3 VPCAttachments and 1 VPCPeering. What is the CORRECT deletion order?</p>
<ul>
<li>A) Delete VPC → Delete VPCAttachments → Delete VPCPeering</li>
<li>B) Delete VPCAttachments → Delete VPCPeering → Delete VPC</li>
<li>C) Delete VPCPeering → Delete VPC → Delete VPCAttachments</li>
<li>D) Delete all resources simultaneously with <code>kubectl delete vpc,vpcattachment,vpcpeering --all</code></li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Delete VPCAttachments → Delete VPCPeering → Delete VPC</p>
<p><strong>Correct order (leaves to roots):</strong></p>
<ol>
<li>VPCAttachments depend on VPC - delete first</li>
<li>VPCPeering depends on both VPCs - delete before VPC</li>
<li>VPC - delete last after dependencies cleared</li>
</ol>
<p><strong>Why others are wrong:</strong></p>
<ul>
<li><strong>A)</strong> VPC stuck in &quot;Terminating&quot; while VPCAttachments exist</li>
<li><strong>C)</strong> VPC still stuck (VPCAttachments not deleted)</li>
<li><strong>D)</strong> Race condition, unpredictable results, orphaned configs</li>
</ul>
<p><strong>Rule:</strong> Delete children before parents. Leaves before branches.</p>
</details>

<hr>
<h3>Question 3: Finalizers</h3>
<p><strong>Scenario:</strong> You deleted VPC <code>test-vpc</code> 15 minutes ago, but it&#39;s still showing &quot;Terminating&quot; status. You check and find finalizers still present. What should you do FIRST?</p>
<ul>
<li>A) Immediately remove finalizers with <code>kubectl patch</code></li>
<li>B) Check if VPCAttachments still exist and delete them</li>
<li>C) Restart the fabric controller</li>
<li>D) Escalate to support immediately</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Check if VPCAttachments still exist and delete them</p>
<p><strong>Why B is first:</strong></p>
<ul>
<li>Most common cause (80% of stuck finalizers)</li>
<li>Safe to check (read-only)</li>
<li>Easy to fix (delete dependencies)</li>
<li>Controller removes finalizer automatically once cleared</li>
</ul>
<p><strong>Why others are wrong:</strong></p>
<ul>
<li><strong>A)</strong> Premature, dangerous (orphans switch configs), skips diagnosis</li>
<li><strong>C)</strong> Doesn&#39;t address root cause, disrupts operations unnecessarily</li>
<li><strong>D)</strong> Premature escalation, common self-resolvable issue</li>
</ul>
<p><strong>Troubleshooting order:</strong> Check dependencies → Check controller logs → Check agents → Manual removal (last resort after 30+ min)</p>
</details>

<hr>
<h3>Question 4: Partial Failure Recovery</h3>
<p><strong>Scenario:</strong> ArgoCD sync failed with error &quot;Failed to sync application: invalid YAML&quot;. Your rollback commit is in Git, but cluster state hasn&#39;t changed. What is the NEXT step?</p>
<ul>
<li>A) Delete the Git commit and start over</li>
<li>B) Validate YAML locally with <code>kubectl apply --dry-run</code>, fix errors, and commit fix</li>
<li>C) Force ArgoCD to sync with <code>--force</code> flag</li>
<li>D) Manually apply YAML with kubectl (bypass ArgoCD)</li>
</ul>
<details>
<summary>Answer & Explanation</summary>

<p><strong>Answer:</strong> B) Validate YAML locally with <code>kubectl apply --dry-run</code>, fix errors, and commit fix</p>
<p><strong>Why B is correct:</strong></p>
<ul>
<li>Fixes root cause (invalid YAML in Git)</li>
<li>Preserves GitOps (all changes tracked)</li>
<li>Maintains audit trail</li>
<li>Reproducible across environments</li>
</ul>
<p><strong>Why others are wrong:</strong></p>
<ul>
<li><strong>A)</strong> Loses audit trail, doesn&#39;t fix YAML, complicates troubleshooting</li>
<li><strong>C)</strong> <code>--force</code> doesn&#39;t fix invalid YAML syntax</li>
<li><strong>D)</strong> Bypasses GitOps, creates drift, ArgoCD will overwrite</li>
</ul>
<p><strong>GitOps Principle:</strong> Always fix issues in Git first, then sync. Never bypass Git with manual kubectl changes.</p>
</details>

<hr>
<h2>Conclusion</h2>
<p>You&#39;ve completed Module 4.2: Rollback &amp; Recovery!</p>
<h3>What You Learned</h3>
<p><strong>GitOps Rollback Workflow:</strong></p>
<ul>
<li>Using <code>git revert</code> to create safe rollback commits</li>
<li>Triggering ArgoCD sync to apply rollbacks</li>
<li>Verifying rollback success with kubectl and Grafana</li>
<li>Understanding when rollback is appropriate vs. fixing forward</li>
</ul>
<p><strong>Safe Deletion Order:</strong></p>
<ul>
<li>Why order matters (dependencies and finalizers)</li>
<li>Correct sequence: VPCAttachment → VPCPeering → VPC</li>
<li>Avoiding stuck resources during deletion</li>
<li>Verifying cleanup completed successfully</li>
</ul>
<p><strong>Kubernetes Finalizers:</strong></p>
<ul>
<li>What finalizers are and why they protect resources</li>
<li>How finalizers ensure complete cleanup</li>
<li>Diagnosing stuck resources (Terminating state)</li>
<li>When manual finalizer removal is appropriate (last resort)</li>
</ul>
<p><strong>Partial Failure Recovery:</strong></p>
<ul>
<li>Handling stuck resources (Agent disconnected, reconciliation timeout)</li>
<li>Recovering from ArgoCD sync failures</li>
<li>Emergency procedures and escalation criteria</li>
</ul>
<h3>Key Takeaways</h3>
<ol>
<li><p><strong>GitOps rollback is safe and auditable</strong> - Git history provides complete audit trail of changes and rollbacks</p>
</li>
<li><p><strong>Deletion order prevents stuck resources</strong> - Always delete children before parents (VPCAttachment before VPC)</p>
</li>
<li><p><strong>Finalizers are safety mechanisms</strong> - They prevent incomplete deletion and ensure proper cleanup</p>
</li>
<li><p><strong>Fix issues in Git, not kubectl</strong> - Maintain GitOps principles even during recovery</p>
</li>
<li><p><strong>Manual intervention is last resort</strong> - Exhaust troubleshooting before manual finalizer removal or force delete</p>
</li>
</ol>
<h3>Recovery Mindset</h3>
<p>As you continue operating Hedgehog fabrics:</p>
<ul>
<li><strong>Plan rollback before changes</strong> - Know how to undo before making changes</li>
<li><strong>Test in non-prod first</strong> - Validate rollback procedures in safe environments</li>
<li><strong>Verify after rollback</strong> - Don&#39;t assume rollback worked, confirm with kubectl and Grafana</li>
<li><strong>Document recovery procedures</strong> - Share knowledge with team</li>
<li><strong>Follow safe deletion order</strong> - Prevent stuck resources by deleting dependencies first</li>
</ul>
<h3>Course 4 Progress</h3>
<p><strong>Completed:</strong></p>
<ul>
<li>✅ Module 4.1: Diagnosing Fabric Issues</li>
<li>✅ Module 4.2: Rollback &amp; Recovery</li>
</ul>
<p><strong>Up Next:</strong></p>
<ul>
<li>Module 4.3: Coordinating with Support (effective tickets, working with engineers)</li>
<li>Module 4.4: Post-Incident Review (documentation, prevention, knowledge sharing)</li>
</ul>
<p><strong>Overall Pathway Progress:</strong> 14/16 modules complete (87.5%)</p>
<hr>
<p><strong>You&#39;re now equipped to safely rollback changes and recover from failures using GitOps best practices. See you in Module 4.3!</strong></p>
",402,15,,,"hedgehog,fabric,rollback,recovery,gitops,kubernetes,finalizers"